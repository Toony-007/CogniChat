# ü§ñ Prompt para Crear M√≥dulo de An√°lisis Cualitativo Paso a Paso

## üìã Objetivo

Este documento proporciona un **prompt detallado y estructurado** para recrear el m√≥dulo de an√°lisis cualitativo de forma **modular, independiente y coherente**. Cada subm√≥dulo se puede crear por separado sin afectar a los dem√°s.

---

## üéØ Principios de Dise√±o

### Reglas Fundamentales

1. **Separaci√≥n de Responsabilidades**: Cada clase/m√≥dulo tiene una √∫nica responsabilidad
2. **Independencia**: Los subm√≥dulos no deben depender directamente entre s√≠
3. **Coherencia**: Todos los subm√≥dulos siguen las mismas convenciones
4. **Extensibilidad**: F√°cil agregar nuevas funcionalidades
5. **Mantenibilidad**: C√≥digo limpio, documentado y organizado

### Arquitectura en UN SOLO ARCHIVO

El m√≥dulo completo est√° contenido en: `modules/qualitative_analysis.py`

**Organizaci√≥n interna del archivo (6,099 l√≠neas)**:

```
qualitative_analysis.py
‚îÇ
‚îú‚îÄ SECCI√ìN 1: IMPORTS Y CONFIGURACI√ìN (L√≠neas 1-188)
‚îÇ  ‚îî‚îÄ Todas las importaciones y configuraci√≥n global
‚îÇ
‚îú‚îÄ SECCI√ìN 2: ENUMS Y DATACLASSES (L√≠neas 189-247)
‚îÇ  ‚îú‚îÄ AnalysisType (enum)
‚îÇ  ‚îú‚îÄ VisualizationType (enum)
‚îÇ  ‚îú‚îÄ AnalysisConfig (dataclass)
‚îÇ  ‚îú‚îÄ ConceptData (dataclass)
‚îÇ  ‚îî‚îÄ AnalysisResult (dataclass)
‚îÇ
‚îú‚îÄ SECCI√ìN 3: CLASES BASE (L√≠neas 248-271)
‚îÇ  ‚îî‚îÄ BaseAnalyzer (abstract class)
‚îÇ
‚îú‚îÄ SECCI√ìN 4: GESTI√ìN DE CACHE (L√≠neas 272-324)
‚îÇ  ‚îî‚îÄ CacheManager (class)
‚îÇ
‚îú‚îÄ SECCI√ìN 5: PREPROCESAMIENTO (L√≠neas 325-394)
‚îÇ  ‚îî‚îÄ TextPreprocessor (class)
‚îÇ
‚îú‚îÄ SECCI√ìN 6: EXTRACCI√ìN DE CONCEPTOS (L√≠neas 395-591)
‚îÇ  ‚îî‚îÄ ConceptExtractor (class)
‚îÇ
‚îú‚îÄ SECCI√ìN 7: AN√ÅLISIS DE TEMAS (L√≠neas 592-807)
‚îÇ  ‚îî‚îÄ ThemeAnalyzer (class)
‚îÇ
‚îú‚îÄ SECCI√ìN 8: AN√ÅLISIS DE SENTIMIENTOS (L√≠neas 808-1023)
‚îÇ  ‚îî‚îÄ SentimentAnalyzer (class)
‚îÇ
‚îú‚îÄ SECCI√ìN 9: CLASE PRINCIPAL (L√≠neas 1024-6008)
‚îÇ  ‚îú‚îÄ AdvancedQualitativeAnalyzer (class principal)
‚îÇ  ‚îú‚îÄ M√©todos de validaci√≥n y utilidades
‚îÇ  ‚îú‚îÄ M√©todos de compatibilidad
‚îÇ  ‚îú‚îÄ M√©todos de an√°lisis principales
‚îÇ  ‚îú‚îÄ Res√∫menes autom√°ticos
‚îÇ  ‚îú‚îÄ An√°lisis paralelo
‚îÇ  ‚îú‚îÄ Configuraci√≥n y optimizaci√≥n
‚îÇ  ‚îú‚îÄ Mapas conceptuales
‚îÇ  ‚îú‚îÄ Mapas mentales
‚îÇ  ‚îú‚îÄ Triangulaci√≥n
‚îÇ  ‚îú‚îÄ Clustering
‚îÇ  ‚îî‚îÄ Nubes de palabras
‚îÇ
‚îî‚îÄ SECCI√ìN 10: FUNCIONES DE RENDERIZADO (L√≠neas 4478-6099)
   ‚îú‚îÄ render_advanced_dashboard()
   ‚îú‚îÄ render_advanced_themes()
   ‚îú‚îÄ render_clustering_analysis()
   ‚îú‚îÄ render_sentiment_analysis()
   ‚îú‚îÄ render_word_cloud()
   ‚îú‚îÄ render_interactive_concept_map()
   ‚îú‚îÄ render_interactive_mind_map()
   ‚îú‚îÄ render_automatic_summary()
   ‚îú‚îÄ render_triangulation_analysis()
   ‚îî‚îÄ render() - Funci√≥n principal
```

**Ventajas de esta arquitectura de un solo archivo**:
‚úÖ Todo el c√≥digo est√° en un solo lugar
‚úÖ Secciones claramente separadas con comentarios
‚úÖ F√°cil de navegar con Ctrl+G + n√∫mero de l√≠nea
‚úÖ Coherencia garantizada en un solo archivo
‚úÖ Sin problemas de imports entre m√≥dulos

---

## üìù Prompts para Crear Cada Secci√≥n en el Archivo √önico

### PASO 1: Crear Secci√≥n de Configuraci√≥n (L√≠neas 145-247)

```markdown
# PROMPT PARA IA:

En el archivo `modules/qualitative_analysis.py`, crea la SECCI√ìN 2 (l√≠neas 145-247) con las siguientes especificaciones:

## Requisitos:
1. Define enumeraciones para tipos de an√°lisis y visualizaci√≥n
2. Crea dataclasses para configuraci√≥n, datos de conceptos y resultados
3. Implementa validaci√≥n de configuraci√≥n
4. Proporciona valores por defecto sensatos

## Estructura esperada:

```python
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Any
from datetime import datetime

class AnalysisType(Enum):
    """Define todos los tipos de an√°lisis disponibles"""
    # Agregar tipos...

class VisualizationType(Enum):
    """Define tipos de visualizaci√≥n"""
    # Agregar tipos...

@dataclass
class AnalysisConfig:
    """Configuraci√≥n centralizada para an√°lisis"""
    # Par√°metros generales
    min_frequency: int = 2
    max_concepts: int = 50
    similarity_threshold: float = 0.6
    
    # Optimizaci√≥n
    enable_cache: bool = True
    parallel_processing: bool = True
    max_workers: int = 4
    
    # Validaci√≥n
    def validate(self) -> bool:
        """Validar que la configuraci√≥n sea correcta"""
        if self.min_frequency < 1:
            raise ValueError("min_frequency debe ser >= 1")
        if self.max_concepts < 1:
            raise ValueError("max_concepts debe ser >= 1")
        return True

@dataclass
class ConceptData:
    """Estructura para almacenar un concepto extra√≠do"""
    concept: str
    score: float
    frequency: int
    context: List[str] = field(default_factory=list)
    related_concepts: List[str] = field(default_factory=list)
    sentiment: Optional[float] = None
    category: Optional[str] = None
    
    def to_dict(self) -> Dict:
        """Convertir a diccionario"""
        return {
            'concept': self.concept,
            'score': self.score,
            'frequency': self.frequency,
            'context': self.context,
            'related_concepts': self.related_concepts,
            'sentiment': self.sentiment,
            'category': self.category
        }

@dataclass
class AnalysisResult:
    """Resultado estandarizado de cualquier an√°lisis"""
    analysis_type: AnalysisType
    data: Dict[str, Any]
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)
    processing_time: float = 0.0
    
    def to_dict(self) -> Dict:
        """Convertir a diccionario"""
        return {
            'analysis_type': self.analysis_type.value,
            'data': self.data,
            'metadata': self.metadata,
            'timestamp': self.timestamp.isoformat(),
            'processing_time': self.processing_time
        }
```

## Criterios de aceptaci√≥n:
- [ ] Todas las enumeraciones est√°n completas
- [ ] Dataclasses tienen m√©todos to_dict()
- [ ] Validaci√≥n implementada y probada
- [ ] Sin dependencias externas m√°s all√° de stdlib
```

---

### PASO 2: Crear Secci√≥n de Cache (L√≠neas 272-324)

```markdown
# PROMPT PARA IA:

En el archivo `modules/qualitative_analysis.py`, crea la SECCI√ìN 4 (l√≠neas 272-324) con un sistema de cache thread-safe y eficiente.

## Requisitos:
1. Implementar cache LRU (Least Recently Used)
2. Thread-safe con threading.Lock
3. M√©tricas de rendimiento (hit ratio, tama√±o)
4. Auto-eviction cuando alcanza tama√±o m√°ximo
5. Serializaci√≥n opcional para persistencia

## Estructura esperada:

```python
import threading
from typing import Any, Optional, Dict
from datetime import datetime
import json
from pathlib import Path

class CacheManager:
    """Gestor de cache LRU thread-safe"""
    
    def __init__(self, max_size: int = 1000, persistent: bool = False, 
                 cache_file: Optional[Path] = None):
        self.cache: Dict[str, Any] = {}
        self.max_size = max_size
        self.access_times: Dict[str, datetime] = {}
        self.lock = threading.Lock()
        self.persistent = persistent
        self.cache_file = cache_file
        
        # Estad√≠sticas
        self._hits = 0
        self._misses = 0
        
        # Cargar cache persistente si existe
        if persistent and cache_file and cache_file.exists():
            self._load_cache()
    
    def get(self, key: str) -> Optional[Any]:
        """Obtener elemento del cache"""
        with self.lock:
            if key in self.cache:
                self.access_times[key] = datetime.now()
                self._hits += 1
                return self.cache[key]
            self._misses += 1
            return None
    
    def set(self, key: str, value: Any) -> None:
        """Guardar elemento en cache"""
        with self.lock:
            # Eviction si es necesario
            if len(self.cache) >= self.max_size:
                self._evict_oldest()
            
            self.cache[key] = value
            self.access_times[key] = datetime.now()
            
            # Persistir si est√° habilitado
            if self.persistent:
                self._save_cache()
    
    def _evict_oldest(self) -> None:
        """Eliminar el elemento menos usado recientemente"""
        if not self.access_times:
            return
        
        oldest_key = min(
            self.access_times.keys(),
            key=lambda k: self.access_times[k]
        )
        
        del self.cache[oldest_key]
        del self.access_times[oldest_key]
    
    def clear(self) -> None:
        """Limpiar todo el cache"""
        with self.lock:
            self.cache.clear()
            self.access_times.clear()
            self._hits = 0
            self._misses = 0
    
    def get_stats(self) -> Dict[str, Any]:
        """Obtener estad√≠sticas del cache"""
        with self.lock:
            hit_ratio = self._hits / (self._hits + self._misses) if (self._hits + self._misses) > 0 else 0.0
            
            return {
                'size': len(self.cache),
                'max_size': self.max_size,
                'hits': self._hits,
                'misses': self._misses,
                'hit_ratio': hit_ratio,
                'utilization': len(self.cache) / self.max_size
            }
    
    def _save_cache(self) -> None:
        """Guardar cache en disco"""
        if not self.cache_file:
            return
        
        try:
            with open(self.cache_file, 'w') as f:
                json.dump(self.cache, f)
        except Exception as e:
            print(f"Error guardando cache: {e}")
    
    def _load_cache(self) -> None:
        """Cargar cache desde disco"""
        if not self.cache_file:
            return
        
        try:
            with open(self.cache_file, 'r') as f:
                self.cache = json.load(f)
        except Exception as e:
            print(f"Error cargando cache: {e}")

# Factory para diferentes tipos de cache
class CacheFactory:
    """Factory para crear diferentes tipos de cache"""
    
    @staticmethod
    def create_cache(cache_type: str = 'memory', **kwargs) -> CacheManager:
        """Crear cache seg√∫n tipo"""
        if cache_type == 'memory':
            return CacheManager(**kwargs)
        elif cache_type == 'persistent':
            return CacheManager(persistent=True, **kwargs)
        elif cache_type == 'redis':
            # Implementar RedisCacheManager
            pass
        else:
            raise ValueError(f"Tipo de cache no soportado: {cache_type}")
```

## Criterios de aceptaci√≥n:
- [ ] Thread-safe verificado
- [ ] LRU eviction funciona correctamente
- [ ] Estad√≠sticas precisas
- [ ] Persistencia opcional funciona
- [ ] Tests unitarios incluidos
```

---

### PASO 3: Crear Secci√≥n de Clases Base (L√≠neas 248-271)

```markdown
# PROMPT PARA IA:

En el archivo `modules/qualitative_analysis.py`, crea la SECCI√ìN 3 (l√≠neas 248-271) con clases base y interfaces abstractas.

## Requisitos:
1. Definir clase abstracta BaseAnalyzer
2. Implementar m√©todos comunes de validaci√≥n
3. Proveer interfaz consistente para todos los analizadores
4. Incluir logging y manejo de errores

## Estructura esperada:

```python
from abc import ABC, abstractmethod
from typing import List, Dict, Optional
import logging
from .config import AnalysisConfig, AnalysisResult

logger = logging.getLogger(__name__)

class BaseAnalyzer(ABC):
    """Clase base abstracta para todos los analizadores"""
    
    def __init__(self, config: AnalysisConfig):
        """
        Args:
            config: Configuraci√≥n del an√°lisis
        """
        self.config = config
        self.cache = {}
        self.logger = logger
        
        # Validar configuraci√≥n
        if not self.config.validate():
            raise ValueError("Configuraci√≥n inv√°lida")
    
    @abstractmethod
    def analyze(self, chunks: List[Dict]) -> AnalysisResult:
        """
        M√©todo abstracto que debe implementar cada analizador
        
        Args:
            chunks: Lista de chunks de documentos
            
        Returns:
            AnalysisResult con los datos del an√°lisis
        """
        pass
    
    def _validate_input(self, chunks: List[Dict]) -> bool:
        """
        Validar que la entrada sea v√°lida
        
        Args:
            chunks: Lista de chunks a validar
            
        Returns:
            True si la entrada es v√°lida
        """
        if not chunks or not isinstance(chunks, list):
            self.logger.warning("Chunks inv√°lidos: lista vac√≠a o tipo incorrecto")
            return False
        
        valid_chunks = sum(
            1 for chunk in chunks 
            if isinstance(chunk, dict) and chunk.get('content')
        )
        
        if valid_chunks == 0:
            self.logger.warning("No hay chunks v√°lidos con contenido")
            return False
        
        self.logger.info(f"Validados {valid_chunks} chunks de {len(chunks)} totales")
        return True
    
    def _generate_cache_key(self, chunks: List[Dict], suffix: str = "") -> str:
        """
        Generar clave √∫nica para cache
        
        Args:
            chunks: Lista de chunks
            suffix: Sufijo opcional para la clave
            
        Returns:
            Clave de cache √∫nica
        """
        import hashlib
        
        content_hash = hash(
            str(sorted([chunk.get('content', '')[:100] for chunk in chunks]))
        )
        
        return f"{self.__class__.__name__}_{content_hash}{suffix}"
    
    def _measure_processing_time(self, func):
        """Decorador para medir tiempo de procesamiento"""
        from functools import wraps
        from datetime import datetime
        
        @wraps(func)
        def wrapper(*args, **kwargs):
            start_time = datetime.now()
            result = func(*args, **kwargs)
            processing_time = (datetime.now() - start_time).total_seconds()
            
            self.logger.info(
                f"{func.__name__} completado en {processing_time:.2f}s"
            )
            
            return result
        return wrapper

class AnalyzerFactory:
    """Factory para crear analizadores seg√∫n tipo"""
    
    _analyzers = {}
    
    @classmethod
    def register(cls, analysis_type: str, analyzer_class):
        """Registrar un analizador"""
        cls._analyzers[analysis_type] = analyzer_class
    
    @classmethod
    def create(cls, analysis_type: str, config: AnalysisConfig) -> BaseAnalyzer:
        """Crear analizador seg√∫n tipo"""
        if analysis_type not in cls._analyzers:
            raise ValueError(f"Analizador no registrado: {analysis_type}")
        
        return cls._analyzers[analysis_type](config)
    
    @classmethod
    def get_available_analyzers(cls) -> List[str]:
        """Obtener lista de analizadores disponibles"""
        return list(cls._analyzers.keys())
```

## Criterios de aceptaci√≥n:
- [ ] BaseAnalyzer es completamente abstracto
- [ ] M√©todos de validaci√≥n robustos
- [ ] Factory pattern implementado
- [ ] Logging en todas las operaciones
- [ ] Sin dependencias entre analizadores
```

---

### PASO 4: Crear Secci√≥n de Preprocesamiento (L√≠neas 325-394)

```markdown
# PROMPT PARA IA:

En el archivo `modules/qualitative_analysis.py`, crea la SECCI√ìN 5 (l√≠neas 325-394) que maneje todo el preprocesamiento de texto.

## Requisitos:
1. Preprocesamiento de texto en espa√±ol
2. Gesti√≥n de stopwords con cache
3. Normalizaci√≥n y limpieza
4. Independiente de otros m√≥dulos

## Estructura esperada:

```python
import re
from typing import List, Set, Optional
import logging

try:
    import nltk
    from nltk.corpus import stopwords
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False

logger = logging.getLogger(__name__)

class TextPreprocessor:
    """Preprocesador de texto especializado para espa√±ol"""
    
    def __init__(self, custom_stopwords: Optional[Set[str]] = None):
        """
        Args:
            custom_stopwords: Stopwords adicionales espec√≠ficas del dominio
        """
        self._stopwords_cache: Optional[List[str]] = None
        self.custom_stopwords = custom_stopwords or set()
        self.logger = logger
        
        # Inicializar NLTK si est√° disponible
        self._initialize_nltk()
    
    def _initialize_nltk(self) -> None:
        """Inicializar recursos de NLTK"""
        if NLTK_AVAILABLE:
            try:
                nltk.download('stopwords', quiet=True)
                nltk.download('punkt', quiet=True)
            except Exception as e:
                self.logger.warning(f"Error descargando recursos NLTK: {e}")
    
    def get_spanish_stopwords(self) -> List[str]:
        """
        Obtener lista de stopwords en espa√±ol con cache
        
        Returns:
            Lista de stopwords
        """
        if self._stopwords_cache is not None:
            return self._stopwords_cache
        
        stopwords_set = set()
        
        # Intentar cargar de NLTK
        if NLTK_AVAILABLE:
            try:
                stopwords_set.update(stopwords.words('spanish'))
            except Exception as e:
                self.logger.warning(f"Error cargando stopwords de NLTK: {e}")
        
        # Stopwords b√°sicas como fallback
        if not stopwords_set:
            stopwords_set = {
                'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'es', 'se',
                'no', 'te', 'lo', 'le', 'da', 'su', 'por', 'son', 'con',
                'para', 'al', 'del', 'los', 'las', 'una', 'como', 'm√°s'
                # ... agregar m√°s
            }
        
        # Agregar stopwords personalizadas
        stopwords_set.update(self.custom_stopwords)
        
        # Agregar stopwords espec√≠ficas del dominio
        domain_stopwords = {
            'tambi√©n', 'puede', 'ser', 'est√°', 'est√°n', 'hacer',
            'tiene', 'tienen', 'muy', 'm√°s', 'menos', 'bien'
        }
        stopwords_set.update(domain_stopwords)
        
        # Cachear y retornar
        self._stopwords_cache = list(stopwords_set)
        return self._stopwords_cache
    
    def preprocess_text(self, text: str, 
                       remove_stopwords: bool = True,
                       min_word_length: int = 2) -> str:
        """
        Preprocesar texto para an√°lisis
        
        Args:
            text: Texto a procesar
            remove_stopwords: Si remover stopwords
            min_word_length: Longitud m√≠nima de palabras
            
        Returns:
            Texto procesado
        """
        if not text:
            return ""
        
        # 1. Normalizar a min√∫sculas
        text = text.lower()
        
        # 2. Remover caracteres especiales (mantener espacios)
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # 3. Normalizar espacios
        text = re.sub(r'\s+', ' ', text).strip()
        
        # 4. Tokenizar
        words = text.split()
        
        # 5. Filtrar palabras
        if remove_stopwords:
            stopwords = set(self.get_spanish_stopwords())
            words = [
                word for word in words 
                if word not in stopwords and len(word) >= min_word_length
            ]
        else:
            words = [word for word in words if len(word) >= min_word_length]
        
        return ' '.join(words)
    
    def tokenize_sentences(self, text: str) -> List[str]:
        """
        Dividir texto en oraciones
        
        Args:
            text: Texto a dividir
            
        Returns:
            Lista de oraciones
        """
        if NLTK_AVAILABLE:
            try:
                from nltk.tokenize import sent_tokenize
                return sent_tokenize(text, language='spanish')
            except Exception as e:
                self.logger.warning(f"Error en tokenizaci√≥n NLTK: {e}")
        
        # Fallback simple
        sentences = re.split(r'[.!?]+', text)
        return [s.strip() for s in sentences if s.strip()]
    
    def extract_ngrams(self, text: str, n: int = 2) -> List[str]:
        """
        Extraer n-gramas del texto
        
        Args:
            text: Texto a procesar
            n: Tama√±o de los n-gramas
            
        Returns:
            Lista de n-gramas
        """
        words = self.preprocess_text(text).split()
        
        if len(words) < n:
            return []
        
        ngrams = []
        for i in range(len(words) - n + 1):
            ngram = ' '.join(words[i:i+n])
            ngrams.append(ngram)
        
        return ngrams
    
    def add_custom_stopword(self, word: str) -> None:
        """Agregar stopword personalizada"""
        self.custom_stopwords.add(word.lower())
        self._stopwords_cache = None  # Invalidar cache
    
    def remove_custom_stopword(self, word: str) -> None:
        """Remover stopword personalizada"""
        self.custom_stopwords.discard(word.lower())
        self._stopwords_cache = None  # Invalidar cache
```

## Criterios de aceptaci√≥n:
- [ ] Funciona sin NLTK (fallback)
- [ ] Cache de stopwords implementado
- [ ] M√©todos de tokenizaci√≥n robustos
- [ ] Personalizaci√≥n de stopwords
- [ ] Sin dependencias de otros m√≥dulos del proyecto
```

---

### PASO 5: Crear Secci√≥n de Extracci√≥n de Conceptos (L√≠neas 395-591)

```markdown
# PROMPT PARA IA:

En el archivo `modules/qualitative_analysis.py`, crea la SECCI√ìN 6 (l√≠neas 395-591) que extraiga conceptos clave del contenido.

## Requisitos:
1. Implementar extracci√≥n con TF-IDF
2. Fallback con an√°lisis de frecuencia
3. Enriquecer conceptos con contexto
4. Encontrar conceptos relacionados
5. Independiente de otros analizadores

## Estructura esperada:

```python
from typing import List, Dict
from datetime import datetime
import numpy as np

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

# NOTA: No se necesitan imports porque todo est√° en el mismo archivo
# Las clases ya est√°n definidas en secciones anteriores:
# - BaseAnalyzer (L√≠nea 252)
# - AnalysisConfig, AnalysisResult, AnalysisType, ConceptData (L√≠neas 218-246)
# - CacheManager (L√≠nea 276)
# - TextPreprocessor (L√≠nea 329)

class ConceptExtractor(BaseAnalyzer):
    """Extractor de conceptos clave usando TF-IDF y an√°lisis de frecuencia"""
    
    def __init__(self, config: AnalysisConfig):
        super().__init__(config)
        self.preprocessor = TextPreprocessor()
        self.cache_manager = CacheManager(max_size=500)
    
    def analyze(self, chunks: List[Dict]) -> AnalysisResult:
        """
        Extraer conceptos clave del contenido
        
        Args:
            chunks: Lista de chunks de documentos
            
        Returns:
            AnalysisResult con conceptos extra√≠dos
        """
        start_time = datetime.now()
        
        # Validar entrada
        if not self._validate_input(chunks):
            return AnalysisResult(
                analysis_type=AnalysisType.CONCEPT_EXTRACTION,
                data={'concepts': []},
                metadata={'error': 'Invalid input data'}
            )
        
        # Verificar cache
        cache_key = self._generate_cache_key(chunks, f"_{self.config.min_frequency}")
        if self.config.enable_cache:
            cached_result = self.cache_manager.get(cache_key)
            if cached_result:
                self.logger.info("Usando conceptos desde cache")
                return cached_result
        
        # Procesar chunks
        concepts = self._extract_concepts(chunks)
        
        # Calcular tiempo de procesamiento
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Crear resultado
        result = AnalysisResult(
            analysis_type=AnalysisType.CONCEPT_EXTRACTION,
            data={'concepts': concepts},
            metadata={
                'total_chunks': len(chunks),
                'concepts_found': len(concepts),
                'processing_method': 'tfidf' if SKLEARN_AVAILABLE else 'frequency',
                'cache_stats': self.cache_manager.get_stats()
            },
            processing_time=processing_time
        )
        
        # Guardar en cache
        if self.config.enable_cache:
            self.cache_manager.set(cache_key, result)
        
        return result
    
    def _extract_concepts(self, chunks: List[Dict]) -> List[ConceptData]:
        """Extraer conceptos usando el mejor m√©todo disponible"""
        # Preparar textos
        texts = self._prepare_texts(chunks)
        
        if not texts:
            return []
        
        # Estrategia principal: TF-IDF
        if SKLEARN_AVAILABLE and len(texts) >= 2:
            concepts = self._extract_with_tfidf(texts)
            if concepts:
                return self._enrich_concepts(concepts, chunks)
        
        # Fallback: Frecuencia
        concepts = self._extract_with_frequency(texts)
        return self._enrich_concepts(concepts, chunks)
    
    def _prepare_texts(self, chunks: List[Dict]) -> List[str]:
        """Preparar textos para an√°lisis"""
        texts = []
        for chunk in chunks:
            content = chunk.get('content', '').strip()
            if content:
                processed = self.preprocessor.preprocess_text(content)
                if processed and len(processed.split()) > 5:
                    texts.append(processed)
        return texts
    
    def _extract_with_tfidf(self, texts: List[str]) -> List[ConceptData]:
        """Extraer conceptos usando TF-IDF"""
        try:
            vectorizer = TfidfVectorizer(
                max_features=self.config.max_concepts * 2,
                stop_words=self.preprocessor.get_spanish_stopwords(),
                ngram_range=(1, 3),
                min_df=max(1, len(texts) // 10),
                max_df=0.9
            )
            
            tfidf_matrix = vectorizer.fit_transform(texts)
            feature_names = vectorizer.get_feature_names_out()
            mean_scores = tfidf_matrix.mean(axis=0).A1
            
            concepts = []
            for i, score in enumerate(mean_scores):
                if score > 0:
                    concept = ConceptData(
                        concept=feature_names[i],
                        score=float(score),
                        frequency=int(score * len(texts) * 10)
                    )
                    concepts.append(concept)
            
            return sorted(concepts, key=lambda x: x.score, reverse=True)[:self.config.max_concepts]
            
        except Exception as e:
            self.logger.warning(f"Error en TF-IDF: {e}")
            return []
    
    def _extract_with_frequency(self, texts: List[str]) -> List[ConceptData]:
        """Extraer conceptos usando an√°lisis de frecuencia"""
        from collections import Counter
        
        all_text = " ".join(texts)
        words = all_text.split()
        word_freq = Counter(words)
        
        total_words = len(words)
        concepts = []
        
        for word, freq in word_freq.most_common(self.config.max_concepts * 2):
            if (len(word) > 3 and 
                freq >= self.config.min_frequency and
                word.isalpha()):
                
                concept = ConceptData(
                    concept=word,
                    score=freq / total_words,
                    frequency=freq
                )
                concepts.append(concept)
        
        return concepts[:self.config.max_concepts]
    
    def _enrich_concepts(self, concepts: List[ConceptData], chunks: List[Dict]) -> List[ConceptData]:
        """Enriquecer conceptos con contexto y relaciones"""
        for concept in concepts:
            # Agregar contexto
            concept.context = self._find_context(concept.concept, chunks)
            
            # Agregar conceptos relacionados
            concept.related_concepts = self._find_related(concept.concept, concepts)
        
        return concepts
    
    def _find_context(self, concept: str, chunks: List[Dict], max_contexts: int = 3) -> List[str]:
        """Encontrar contexto donde aparece el concepto"""
        contexts = []
        concept_lower = concept.lower()
        
        for chunk in chunks:
            content = chunk.get('content', '')
            if concept_lower in content.lower():
                sentences = self.preprocessor.tokenize_sentences(content)
                for sentence in sentences:
                    if concept_lower in sentence.lower() and len(sentence.strip()) > 20:
                        contexts.append(sentence.strip())
                        if len(contexts) >= max_contexts:
                            break
                if len(contexts) >= max_contexts:
                    break
        
        return contexts
    
    def _find_related(self, concept: str, all_concepts: List[ConceptData], max_related: int = 5) -> List[str]:
        """Encontrar conceptos relacionados"""
        related = []
        concept_words = set(concept.lower().split())
        
        for other_concept in all_concepts:
            if other_concept.concept == concept:
                continue
            
            other_words = set(other_concept.concept.lower().split())
            
            # Similitud por palabras compartidas
            if concept_words & other_words:
                related.append(other_concept.concept)
                if len(related) >= max_related:
                    break
        
        return related

# NOTA: En el archivo √∫nico, las clases se usan directamente
# La clase AdvancedQualitativeAnalyzer (L√≠nea 1024) las instancia en __init__:
# self.concept_extractor = ConceptExtractor(self.config)
```

## Criterios de aceptaci√≥n:
- [ ] Funciona con y sin scikit-learn
- [ ] Cache implementado y funcional
- [ ] Enriquecimiento de conceptos completo
- [ ] No depende de otros analizadores
- [ ] Registrado en AnalyzerFactory
```

---

### PASO 6: Crear Secci√≥n de An√°lisis de Temas (L√≠neas 592-807)

```markdown
# PROMPT PARA IA:

En el archivo `modules/qualitative_analysis.py`, crea la SECCI√ìN 7 (l√≠neas 592-807) que analice temas del contenido.

## Requisitos:
1. Implementar LDA para modelado de temas
2. Fallback con clustering de palabras clave
3. Calcular coherencia de temas
4. Independiente de otros analizadores

## Estructura esperada:

```python
from typing import List, Dict
from datetime import datetime
import numpy as np
from collections import Counter

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.decomposition import LatentDirichletAllocation
    from sklearn.cluster import KMeans
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

from ..core.base import BaseAnalyzer
from ..core.config import AnalysisConfig, AnalysisResult, AnalysisType
from ..core.cache import CacheManager
from ..preprocessing.text_processor import TextPreprocessor

class ThemeAnalyzer(BaseAnalyzer):
    """Analizador de temas usando LDA y clustering"""
    
    def __init__(self, config: AnalysisConfig):
        super().__init__(config)
        self.preprocessor = TextPreprocessor()
        self.cache_manager = CacheManager(max_size=200)
    
    def analyze(self, chunks: List[Dict]) -> AnalysisResult:
        """
        Analizar temas del contenido
        
        Args:
            chunks: Lista de chunks de documentos
            
        Returns:
            AnalysisResult con temas identificados
        """
        start_time = datetime.now()
        
        # Validar entrada
        if not self._validate_input(chunks):
            return AnalysisResult(
                analysis_type=AnalysisType.THEME_ANALYSIS,
                data={'themes': []},
                metadata={'error': 'Invalid input data'}
            )
        
        # Verificar cache
        cache_key = self._generate_cache_key(chunks, f"_{self.config.max_concepts}")
        if self.config.enable_cache:
            cached_result = self.cache_manager.get(cache_key)
            if cached_result:
                self.logger.info("Usando temas desde cache")
                return cached_result
        
        # Extraer temas
        themes = self._extract_themes(chunks)
        
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Crear resultado
        result = AnalysisResult(
            analysis_type=AnalysisType.THEME_ANALYSIS,
            data={'themes': themes},
            metadata={
                'total_chunks': len(chunks),
                'themes_found': len(themes),
                'processing_method': 'lda' if SKLEARN_AVAILABLE else 'clustering'
            },
            processing_time=processing_time
        )
        
        # Guardar en cache
        if self.config.enable_cache:
            self.cache_manager.set(cache_key, result)
        
        return result
    
    def _extract_themes(self, chunks: List[Dict]) -> List[Dict]:
        """Extraer temas usando el mejor m√©todo disponible"""
        # Preparar textos
        texts = []
        for chunk in chunks:
            content = chunk.get('content', '').strip()
            if content:
                processed = self.preprocessor.preprocess_text(content)
                if processed and len(processed.split()) > 10:
                    texts.append(processed)
        
        if not texts:
            return []
        
        # Estrategia principal: LDA
        if SKLEARN_AVAILABLE and len(texts) >= 3:
            themes = self._extract_with_lda(texts)
            if themes:
                return themes
        
        # Fallback: Clustering
        return self._extract_with_clustering(texts)
    
    def _extract_with_lda(self, texts: List[str]) -> List[Dict]:
        """Extraer temas usando LDA"""
        try:
            n_topics = min(
                self.config.max_concepts,
                max(3, len(texts) // 3)
            )
            
            vectorizer = TfidfVectorizer(
                max_features=1000,
                stop_words=self.preprocessor.get_spanish_stopwords(),
                ngram_range=(1, 2),
                min_df=2,
                max_df=0.8
            )
            
            tfidf_matrix = vectorizer.fit_transform(texts)
            feature_names = vectorizer.get_feature_names_out()
            
            lda = LatentDirichletAllocation(
                n_components=n_topics,
                random_state=42,
                max_iter=100,
                learning_method='batch'
            )
            
            lda.fit(tfidf_matrix)
            
            # Extraer temas
            themes = []
            for topic_idx, topic in enumerate(lda.components_):
                top_indices = topic.argsort()[-10:][::-1]
                keywords = [feature_names[i] for i in top_indices]
                weights = [topic[i] for i in top_indices]
                
                theme = {
                    'id': topic_idx,
                    'name': f"Tema {topic_idx + 1}",
                    'keywords': keywords,
                    'weights': weights,
                    'coherence': self._calculate_coherence(keywords, texts),
                    'description': self._describe_theme(keywords)
                }
                themes.append(theme)
            
            return sorted(themes, key=lambda x: x['coherence'], reverse=True)
            
        except Exception as e:
            self.logger.warning(f"Error en LDA: {e}")
            return []
    
    def _extract_with_clustering(self, texts: List[str]) -> List[Dict]:
        """Extraer temas usando clustering"""
        from collections import Counter
        
        # Extraer palabras clave
        all_words = []
        for text in texts:
            words = text.split()
            all_words.extend([w for w in words if len(w) > 3])
        
        word_freq = Counter(all_words)
        
        # Top palabras
        keywords = [
            word for word, freq in word_freq.most_common(50)
            if freq >= 2
        ]
        
        if len(keywords) < 5:
            return []
        
        # Agrupar en temas
        n_themes = min(5, max(2, len(keywords) // 10))
        theme_size = len(keywords) // n_themes
        
        themes = []
        for i in range(n_themes):
            start = i * theme_size
            end = start + theme_size if i < n_themes - 1 else len(keywords)
            theme_keywords = keywords[start:end]
            
            theme = {
                'id': i,
                'name': f"Tema {i + 1}",
                'keywords': theme_keywords,
                'weights': [1.0] * len(theme_keywords),
                'coherence': len(theme_keywords) / len(keywords),
                'description': self._describe_theme(theme_keywords)
            }
            themes.append(theme)
        
        return themes
    
    def _calculate_coherence(self, keywords: List[str], texts: List[str]) -> float:
        """Calcular coherencia del tema"""
        if len(keywords) < 2:
            return 0.0
        
        co_occurrences = 0
        total_pairs = 0
        
        for text in texts:
            text_words = set(text.split())
            for i, word1 in enumerate(keywords):
                for word2 in keywords[i+1:]:
                    total_pairs += 1
                    if word1 in text_words and word2 in text_words:
                        co_occurrences += 1
        
        return co_occurrences / total_pairs if total_pairs > 0 else 0.0
    
    def _describe_theme(self, keywords: List[str]) -> str:
        """Generar descripci√≥n del tema"""
        if not keywords:
            return "Tema sin descripci√≥n"
        
        main_keywords = keywords[:3]
        return f"Tema relacionado con: {', '.join(main_keywords)}"

# Registrar en factory
from ..core.base import AnalyzerFactory
AnalyzerFactory.register('theme_analysis', ThemeAnalyzer)
```

## Criterios de aceptaci√≥n:
- [ ] LDA funciona correctamente
- [ ] Fallback a clustering robusto
- [ ] Coherencia calculada correctamente
- [ ] Completamente independiente
- [ ] Registrado en factory
```

---

## üéØ Orden de Creaci√≥n Recomendado (Todo en UN SOLO ARCHIVO)

### Fase 1: Estructura Base (D√≠a 1)
```
En modules/qualitative_analysis.py, crear de arriba hacia abajo:

1. L√≠neas 1-188:    Imports y configuraci√≥n global
2. L√≠neas 189-247:  Enums, dataclasses y estructuras (AnalysisType, AnalysisConfig, etc.)
3. L√≠neas 248-271:  Clases base e interfaces (BaseAnalyzer)
4. L√≠neas 272-324:  Gesti√≥n de cache (CacheManager)
5. L√≠neas 325-394:  Preprocesamiento (TextPreprocessor)
```

### Fase 2: Analizadores Especializados (D√≠a 2-3)
```
6. L√≠neas 395-591:  Extracci√≥n de conceptos (ConceptExtractor)
7. L√≠neas 592-807:  An√°lisis de temas (ThemeAnalyzer)
8. L√≠neas 808-1023: An√°lisis de sentimientos (SentimentAnalyzer)
```

### Fase 3: Clase Principal - Parte 1 (D√≠a 4-5)
```
9. L√≠neas 1024-1176:  Inicializaci√≥n y carga de datos
10. L√≠neas 1177-1424: Res√∫menes autom√°ticos
11. L√≠neas 1425-1657: M√©todos de an√°lisis principales
12. L√≠neas 1658-1883: An√°lisis paralelo y optimizaci√≥n
```

### Fase 4: Clase Principal - Parte 2 (D√≠a 6-7)
```
13. L√≠neas 1977-2292:  Mapas conceptuales interactivos
14. L√≠neas 2293-2398:  Mapas mentales
15. L√≠neas 2399-2466:  Triangulaci√≥n
16. L√≠neas 3752-3838:  Clustering y agrupaci√≥n
17. L√≠neas 3934-4026:  Nubes de palabras
```

### Fase 5: Interfaz de Usuario (D√≠a 8)
```
18. L√≠neas 4478-6009:  Funciones de renderizado (dashboard, gr√°ficos, etc.)
19. L√≠neas 6010-6099:  Funci√≥n principal render()
```

**VENTAJA**: Al estar todo en un solo archivo, puedes:
- ‚úÖ Desarrollar secci√≥n por secci√≥n de arriba hacia abajo
- ‚úÖ Probar cada secci√≥n sin crear archivos separados
- ‚úÖ Mantener coherencia f√°cilmente
- ‚úÖ No preocuparte por imports entre m√≥dulos

---

## üìú Prompt Maestro Completo

```markdown
# PROMPT MAESTRO PARA CREAR M√ìDULO DE AN√ÅLISIS CUALITATIVO (ARCHIVO √öNICO)

Eres un experto en arquitectura de software y an√°lisis de texto con NLP. Tu tarea es crear un m√≥dulo de an√°lisis cualitativo avanzado en UN SOLO ARCHIVO (`modules/qualitative_analysis.py`) siguiendo estos principios:

## CONTEXTO
El m√≥dulo ser√° parte de un sistema RAG (Retrieval-Augmented Generation) llamado CogniChat. Debe analizar contenido de documentos procesados y proporcionar insights mediante NLP y visualizaciones interactivas.

## REQUISITOS FUNCIONALES

### 1. An√°lisis de Texto
- Extracci√≥n de conceptos clave (TF-IDF, BM25)
- Modelado de temas (LDA, NMF, BERTopic)
- An√°lisis de sentimientos (VADER, TextBlob, BERT)
- Clustering de documentos (K-means, DBSCAN)
- Triangulaci√≥n multi-fuente

### 2. Visualizaciones
- Mapas conceptuales interactivos (PyVis, Graphviz)
- Mapas mentales jer√°rquicos (streamlit-agraph)
- Nubes de palabras tem√°ticas
- Dashboards con m√©tricas

### 3. Res√∫menes
- Res√∫menes con LLM (Ollama)
- Res√∫menes extractivos (TextRank, LexRank)
- Res√∫menes abstractivos (BART, T5)

## REQUISITOS NO FUNCIONALES

### Arquitectura
- ‚úÖ Modular y extensible
- ‚úÖ Separaci√≥n de responsabilidades
- ‚úÖ Independencia entre componentes
- ‚úÖ Patrones de dise√±o apropiados

### Rendimiento
- ‚úÖ Cache inteligente (LRU)
- ‚úÖ Procesamiento paralelo
- ‚úÖ Optimizaci√≥n de memoria
- ‚úÖ Lazy loading cuando sea posible

### Calidad
- ‚úÖ C√≥digo limpio y documentado
- ‚úÖ Type hints completos
- ‚úÖ Manejo robusto de errores
- ‚úÖ Logging detallado
- ‚úÖ Tests unitarios

### Compatibilidad
- ‚úÖ Python 3.8+
- ‚úÖ Funciona con/sin dependencias opcionales
- ‚úÖ Fallbacks robustos
- ‚úÖ API consistente

## ESTRUCTURA DEL ARCHIVO √öNICO

Todo el m√≥dulo est√° en un solo archivo: `modules/qualitative_analysis.py`

**Organizaci√≥n interna (19 secciones en 6,099 l√≠neas)**:

```
modules/qualitative_analysis.py
‚îÇ
‚îú‚îÄ [L√çNEAS 1-188] SECCI√ìN 1: Imports y Configuraci√≥n Global
‚îÇ  ‚îî‚îÄ Todas las importaciones y logger setup
‚îÇ
‚îú‚îÄ [L√çNEAS 189-247] SECCI√ìN 2: Enums y Dataclasses
‚îÇ  ‚îú‚îÄ AnalysisType
‚îÇ  ‚îú‚îÄ VisualizationType  
‚îÇ  ‚îú‚îÄ AnalysisConfig
‚îÇ  ‚îú‚îÄ ConceptData
‚îÇ  ‚îî‚îÄ AnalysisResult
‚îÇ
‚îú‚îÄ [L√çNEAS 248-271] SECCI√ìN 3: Clases Base
‚îÇ  ‚îî‚îÄ BaseAnalyzer (abstract)
‚îÇ
‚îú‚îÄ [L√çNEAS 272-324] SECCI√ìN 4: Cache Manager
‚îÇ  ‚îî‚îÄ CacheManager (LRU cache thread-safe)
‚îÇ
‚îú‚îÄ [L√çNEAS 325-394] SECCI√ìN 5: Preprocesamiento
‚îÇ  ‚îî‚îÄ TextPreprocessor
‚îÇ
‚îú‚îÄ [L√çNEAS 395-591] SECCI√ìN 6: Extracci√≥n de Conceptos
‚îÇ  ‚îî‚îÄ ConceptExtractor
‚îÇ
‚îú‚îÄ [L√çNEAS 592-807] SECCI√ìN 7: An√°lisis de Temas
‚îÇ  ‚îî‚îÄ ThemeAnalyzer
‚îÇ
‚îú‚îÄ [L√çNEAS 808-1023] SECCI√ìN 8: An√°lisis de Sentimientos
‚îÇ  ‚îî‚îÄ SentimentAnalyzer
‚îÇ
‚îú‚îÄ [L√çNEAS 1024-4477] SECCI√ìN 9: Clase Principal
‚îÇ  ‚îî‚îÄ AdvancedQualitativeAnalyzer
‚îÇ     ‚îú‚îÄ Validaci√≥n y utilidades
‚îÇ     ‚îú‚îÄ Compatibilidad con API legacy
‚îÇ     ‚îú‚îÄ M√©todos de an√°lisis principales
‚îÇ     ‚îú‚îÄ Secci√≥n 10: Mapas conceptuales (L√≠nea 1977)
‚îÇ     ‚îú‚îÄ Secci√≥n 11: Mapas mentales (L√≠nea 2293)
‚îÇ     ‚îú‚îÄ Secci√≥n 12: Res√∫menes (L√≠nea 1177)
‚îÇ     ‚îú‚îÄ Secci√≥n 13: Triangulaci√≥n (L√≠nea 2399)
‚îÇ     ‚îú‚îÄ Secci√≥n 14: Nubes de palabras (L√≠nea 3934)
‚îÇ     ‚îú‚îÄ Secci√≥n 9: Clustering (L√≠nea 3752)
‚îÇ     ‚îú‚îÄ Secci√≥n 16: An√°lisis paralelo (L√≠nea 1658)
‚îÇ     ‚îî‚îÄ Secci√≥n 17: Configuraci√≥n (L√≠nea 1780)
‚îÇ
‚îî‚îÄ [L√çNEAS 4478-6099] SECCI√ìN 15-19: Renderizado UI
   ‚îú‚îÄ Secci√≥n 15: Visualizaciones y gr√°ficos
   ‚îú‚îÄ render_advanced_dashboard()
   ‚îú‚îÄ render_advanced_themes()
   ‚îú‚îÄ render_clustering_analysis()
   ‚îú‚îÄ render_sentiment_analysis()
   ‚îú‚îÄ render_word_cloud()
   ‚îú‚îÄ render_interactive_concept_map()
   ‚îú‚îÄ render_interactive_mind_map()
   ‚îú‚îÄ render_automatic_summary()
   ‚îú‚îÄ render_triangulation_analysis()
   ‚îî‚îÄ Secci√≥n 19: render() - Funci√≥n principal
```

**C√≥mo navegar el archivo**:
1. Busca el separador: `# === SECCI√ìN X ===`
2. Usa Ctrl+G para ir a la l√≠nea espec√≠fica
3. Toda la funcionalidad relacionada est√° junta
4. Las secciones est√°n ordenadas l√≥gicamente

## CONVENCIONES DE C√ìDIGO

### Naming
- Clases: PascalCase (ConceptExtractor)
- M√©todos p√∫blicos: snake_case (extract_concepts)
- M√©todos privados: _snake_case (_validate_input)
- Constantes: UPPER_SNAKE_CASE (MAX_CONCEPTS)

### Documentaci√≥n
- Docstrings en Google style
- Type hints obligatorios
- Comentarios inline cuando sea necesario
- README por subm√≥dulo

### Logging
- logger.debug() para detalles de depuraci√≥n
- logger.info() para eventos importantes
- logger.warning() para situaciones recuperables
- logger.error() para errores cr√≠ticos

## PASOS DE IMPLEMENTACI√ìN (TODO EN UN ARCHIVO)

Para cada secci√≥n del archivo √∫nico, sigue este proceso:

1. **Dise√±o**
   - Define interfaz p√∫blica
   - Identifica dependencias
   - Planifica estructura interna

2. **Implementaci√≥n**
   - Crea clase/m√≥dulo seg√∫n template
   - Implementa m√©todo principal
   - Agrega m√©todos auxiliares
   - Incluye manejo de errores

3. **Optimizaci√≥n**
   - Implementa cache
   - Optimiza algoritmos
   - Agrega m√©tricas

4. **Documentaci√≥n**
   - Docstrings completos
   - Ejemplos de uso
   - Notas de implementaci√≥n

5. **Testing**
   - Tests unitarios
   - Tests de integraci√≥n
   - Verificaci√≥n de rendimiento

## EJEMPLO DE SECCI√ìN COMPLETA EN EL ARCHIVO √öNICO

Aqu√≠ est√° un ejemplo completo de c√≥mo debe verse una secci√≥n dentro de `modules/qualitative_analysis.py`:

```python
# =============================================================================
# 8. AN√ÅLISIS DE SENTIMIENTOS
# =============================================================================
# Esta secci√≥n proporciona an√°lisis de sentimientos usando m√∫ltiples algoritmos:
# - VADER (primario): An√°lisis l√©xico optimizado
# - TextBlob (secundario): An√°lisis de polaridad
# - B√°sico (fallback): Conteo de palabras positivas/negativas
#
# UBICACI√ìN EN EL ARCHIVO: L√≠neas 808-1023
#
# USO:
#     analyzer = AdvancedQualitativeAnalyzer()
#     result = analyzer.advanced_sentiment_analysis(chunks)
#     print(result['overall_stats'])
#
# MODIFICAR AQU√ç PARA:
# - Cambiar umbrales de sentimiento
# - Agregar nuevos algoritmos (BERT, RoBERTa)
# - Mejorar an√°lisis de emociones
# - Ajustar clasificaci√≥n de sentimientos
# ============================================================================="

from typing import List, Dict, Optional
from datetime import datetime
import logging

try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False

try:
    from nltk.sentiment import SentimentIntensityAnalyzer
    VADER_AVAILABLE = True
except ImportError:
    VADER_AVAILABLE = False

from ..core.base import BaseAnalyzer
from ..core.config import AnalysisConfig, AnalysisResult, AnalysisType
from ..core.cache import CacheManager

logger = logging.getLogger(__name__)

class SentimentAnalyzer(BaseAnalyzer):
    """
    Analizador de sentimientos multi-algoritmo
    
    Attributes:
        config: Configuraci√≥n del an√°lisis
        cache_manager: Gestor de cache
        
    Example:
        >>> analyzer = SentimentAnalyzer(config)
        >>> result = analyzer.analyze(chunks)
        >>> sentiments = result.data['sentiments']
    """
    
    def __init__(self, config: AnalysisConfig):
        """
        Inicializar analizador de sentimientos
        
        Args:
            config: Configuraci√≥n del an√°lisis
        """
        super().__init__(config)
        self.cache_manager = CacheManager(max_size=300)
        
        # Inicializar analizadores
        self._vader = None
        if VADER_AVAILABLE:
            try:
                self._vader = SentimentIntensityAnalyzer()
            except Exception as e:
                logger.warning(f"Error inicializando VADER: {e}")
    
    def analyze(self, chunks: List[Dict]) -> AnalysisResult:
        """
        Analizar sentimientos del contenido
        
        Args:
            chunks: Lista de chunks con contenido
            
        Returns:
            AnalysisResult con an√°lisis de sentimientos
            
        Raises:
            ValueError: Si los chunks son inv√°lidos
        """
        start_time = datetime.now()
        
        # Validar
        if not self._validate_input(chunks):
            raise ValueError("Chunks inv√°lidos")
        
        # Cache
        cache_key = self._generate_cache_key(chunks)
        if self.config.enable_cache:
            cached = self.cache_manager.get(cache_key)
            if cached:
                logger.info("Usando sentimientos desde cache")
                return cached
        
        # Analizar
        sentiments = self._analyze_sentiments(chunks)
        
        # Resultado
        result = AnalysisResult(
            analysis_type=AnalysisType.SENTIMENT_ANALYSIS,
            data={'sentiments': sentiments},
            metadata={
                'method': self._get_method_used(),
                'total_analyzed': len(sentiments['by_chunk'])
            },
            processing_time=(datetime.now() - start_time).total_seconds()
        )
        
        # Cache
        if self.config.enable_cache:
            self.cache_manager.set(cache_key, result)
        
        return result
    
    def _analyze_sentiments(self, chunks: List[Dict]) -> Dict:
        """An√°lisis interno de sentimientos"""
        if self._vader:
            return self._analyze_with_vader(chunks)
        elif TEXTBLOB_AVAILABLE:
            return self._analyze_with_textblob(chunks)
        else:
            return self._analyze_basic(chunks)
    
    def _analyze_with_vader(self, chunks: List[Dict]) -> Dict:
        """An√°lisis con VADER"""
        # Implementaci√≥n...
        pass
    
    def _analyze_with_textblob(self, chunks: List[Dict]) -> Dict:
        """An√°lisis con TextBlob"""
        # Implementaci√≥n...
        pass
    
    def _analyze_basic(self, chunks: List[Dict]) -> Dict:
        """An√°lisis b√°sico de fallback"""
        # Implementaci√≥n...
        pass
    
    def _get_method_used(self) -> str:
        """Retornar m√©todo usado"""
        if self._vader:
            return "VADER"
        elif TEXTBLOB_AVAILABLE:
            return "TextBlob"
        else:
            return "Basic"

# Registrar en factory
from ..core.base import AnalyzerFactory
AnalyzerFactory.register('sentiment_analysis', SentimentAnalyzer)
```

## CHECKLIST DE CALIDAD

Para cada subm√≥dulo creado, verificar:

- [ ] Sigue la estructura definida
- [ ] Documentaci√≥n completa
- [ ] Type hints en todos los m√©todos
- [ ] Manejo de errores robusto
- [ ] Logging apropiado
- [ ] Cache implementado
- [ ] Tests unitarios incluidos
- [ ] Sin dependencias circulares
- [ ] API p√∫blica clara
- [ ] Ejemplos de uso incluidos

## INTEGRACI√ìN FINAL EN EL ARCHIVO √öNICO

Todo est√° integrado en `modules/qualitative_analysis.py`. La clase principal es:

```python
class AdvancedQualitativeAnalyzer:
    """
    Clase principal que orquesta todos los an√°lisis
    
    Esta clase coordina todas las secciones del m√≥dulo y proporciona
    una interfaz unificada para el an√°lisis cualitativo.
    
    Ubicaci√≥n: L√≠neas 1024-4477
    """
    
    def __init__(self, config: Optional[AnalysisConfig] = None):
        """Inicializar analizador con configuraci√≥n"""
        self.config = config or AnalysisConfig()
        
        # Componentes especializados (clases definidas arriba en el archivo)
        self.preprocessor = TextPreprocessor()          # L√≠nea 325
        self.cache_manager = CacheManager()             # L√≠nea 272
        self.concept_extractor = ConceptExtractor(self.config)   # L√≠nea 395
        self.theme_analyzer = ThemeAnalyzer(self.config)         # L√≠nea 592
        self.sentiment_analyzer = SentimentAnalyzer(self.config) # L√≠nea 808
    
    # M√âTODOS P√öBLICOS PRINCIPALES:
    
    def extract_key_concepts(self, chunks) ‚Üí List[Dict]
        """L√≠nea 1429 - Extracci√≥n de conceptos"""
    
    def extract_advanced_themes(self, chunks, n_topics) ‚Üí Dict
        """L√≠nea 1463 - An√°lisis de temas"""
    
    def advanced_sentiment_analysis(self, chunks) ‚Üí Dict
        """L√≠nea 1497 - An√°lisis de sentimientos"""
    
    def perform_clustering(self, chunks, n_clusters) ‚Üí Dict
        """L√≠nea 3756 - Clustering de documentos"""
    
    def perform_triangulation_analysis(self, chunks) ‚Üí Dict
        """L√≠nea 2403 - Validaci√≥n multi-fuente"""
    
    def create_interactive_concept_map(self, chunks, layout_type) ‚Üí str
        """L√≠nea 2001 - Mapas conceptuales"""
    
    def create_interactive_mind_map(self, chunks, node_spacing) ‚Üí Dict
        """L√≠nea 2297 - Mapas mentales"""
    
    def generate_intelligent_summary(self, chunks, summary_type) ‚Üí Dict
        """L√≠nea 1241 - Resumen con LLM"""
    
    def generate_word_cloud(self, chunks, source_filter) ‚Üí str
        """L√≠nea 3938 - Nube de palabras"""
    
    def perform_parallel_analysis(self, chunks, analysis_types) ‚Üí Dict
        """L√≠nea 1662 - An√°lisis paralelo"""
    
    def optimize_performance(self) ‚Üí Dict
        """L√≠nea 1834 - Optimizaci√≥n autom√°tica"""
```

**PUNTO DE ENTRADA**:
La funci√≥n `render()` en la l√≠nea 6013 es el punto de entrada principal que Streamlit llama.

---

## üß™ Testing de Cada Subm√≥dulo

```python
# test_concept_extractor.py
import pytest
from qualitative_analysis.analyzers import ConceptExtractor
from qualitative_analysis.core import AnalysisConfig

def test_concept_extraction_with_valid_input():
    config = AnalysisConfig(max_concepts=10)
    extractor = ConceptExtractor(config)
    
    chunks = [
        {'content': 'Este es un documento sobre inteligencia artificial'},
        {'content': 'La inteligencia artificial es importante'}
    ]
    
    result = extractor.analyze(chunks)
    
    assert result.data['concepts']
    assert len(result.data['concepts']) <= 10
    assert result.processing_time > 0

def test_concept_extraction_with_empty_input():
    config = AnalysisConfig()
    extractor = ConceptExtractor(config)
    
    result = extractor.analyze([])
    
    assert not result.data['concepts']
    assert 'error' in result.metadata
```

---

## üöÄ Beneficios de Esta Arquitectura

### 1. Independencia
- Cada subm√≥dulo se puede desarrollar por separado
- Cambios en un m√≥dulo no afectan otros
- F√°cil testing aislado

### 2. Reutilizaci√≥n
- Componentes reutilizables en otros proyectos
- Factory pattern facilita extender funcionalidad
- Cache compartido entre analizadores

### 3. Mantenibilidad
- C√≥digo organizado y limpio
- F√°cil localizar bugs
- Simple agregar nuevas funcionalidades

### 4. Escalabilidad
- Procesamiento paralelo built-in
- Cache distribuido posible
- Microservicios ready

### 5. Testing
- Tests aislados por m√≥dulo
- Mock dependencies f√°cilmente
- CI/CD simplificado

---

## üìö Recursos Adicionales

### Librer√≠as Recomendadas

**An√°lisis de Texto**:
- scikit-learn: TF-IDF, LDA, clustering
- gensim: Topic modeling, word embeddings
- spaCy: NLP avanzado, NER
- transformers: BERT, GPT, T5

**Visualizaciones**:
- plotly: Gr√°ficos interactivos
- networkx: Grafos y redes
- pyvis: Redes interactivas
- graphviz: Diagramas jer√°rquicos

**Res√∫menes**:
- sumy: Res√∫menes extractivos
- transformers: Res√∫menes abstractivos
- gensim: TextRank

### Referencias

- [scikit-learn Documentation](https://scikit-learn.org/)
- [Gensim Topic Modeling](https://radimrehurek.com/gensim/)
- [NLTK Book](https://www.nltk.org/book/)
- [BERTopic Documentation](https://maartengr.github.io/BERTopic/)
- [Streamlit Components](https://streamlit.io/components)

---

## üí° Tips de Implementaci√≥n

### 1. Comienza Simple
```python
# Versi√≥n 1: Funcionalidad m√≠nima
def analyze(self, chunks):
    return basic_analysis(chunks)

# Versi√≥n 2: Agregar optimizaciones
def analyze(self, chunks):
    cached = self.cache.get(key)
    if cached:
        return cached
    
    result = basic_analysis(chunks)
    self.cache.set(key, result)
    return result

# Versi√≥n 3: Agregar an√°lisis avanzado
def analyze(self, chunks):
    if advanced_available:
        return advanced_analysis(chunks)
    return basic_analysis(chunks)
```

### 2. Siempre Ten Fallback
```python
def _extract_with_advanced_method(self):
    try:
        # M√©todo avanzado
        return advanced_extraction()
    except Exception as e:
        logger.warning(f"Fallback a m√©todo b√°sico: {e}")
        return basic_extraction()
```

### 3. Usa Decoradores para Cross-Cutting Concerns
```python
def cached(func):
    """Decorador para cache autom√°tico"""
    def wrapper(self, *args, **kwargs):
        key = f"{func.__name__}_{hash(str(args))}"
        cached = self.cache_manager.get(key)
        if cached:
            return cached
        
        result = func(self, *args, **kwargs)
        self.cache_manager.set(key, result)
        return result
    
    return wrapper

@cached
def extract_concepts(self, chunks):
    # La cache se maneja autom√°ticamente
    return self._do_extraction(chunks)
```

---

## üéì Conclusi√≥n

Este prompt te permite crear o recrear el m√≥dulo de an√°lisis cualitativo en **UN SOLO ARCHIVO**, de forma **incremental, organizada y coherente**. Cada secci√≥n es independiente pero todas trabajan juntas arm√≥nicamente dentro del mismo archivo.

**Ventajas del enfoque de archivo √∫nico**:
- ‚úÖ Todo el c√≥digo en un solo lugar
- ‚úÖ No hay problemas de imports circulares
- ‚úÖ Secciones claramente separadas con comentarios
- ‚úÖ Navegaci√≥n f√°cil con Ctrl+G + n√∫mero de l√≠nea
- ‚úÖ Debugging simplificado (un solo archivo)
- ‚úÖ Coherencia garantizada
- ‚úÖ Mantenimiento m√°s simple

**Estructura del archivo resultante**:
```
modules/qualitative_analysis.py (6,099 l√≠neas)
‚îú‚îÄ Secciones 1-8: Clases especializadas (1,023 l√≠neas)
‚îú‚îÄ Secci√≥n 9: Clase principal orquestadora (3,454 l√≠neas)
‚îî‚îÄ Secciones 15-19: Funciones de renderizado (1,622 l√≠neas)
```

Sigue los prompts secci√≥n por secci√≥n y obtendr√°s un m√≥dulo robusto, escalable y profesional en un solo archivo perfectamente organizado.

