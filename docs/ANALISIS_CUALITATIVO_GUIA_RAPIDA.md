# üöÄ Gu√≠a R√°pida del M√≥dulo de An√°lisis Cualitativo

## üìç Navegaci√≥n R√°pida del C√≥digo Actual

### Tabla de Contenidos por L√≠neas

| üéØ Funcionalidad | üìç L√≠nea | üîß Qu√© Modificar |
|------------------|----------|------------------|
| **Configuraci√≥n y Estructuras** | 145 | Agregar tipos de an√°lisis, cambiar valores por defecto |
| **Cache y Memoria** | 223 | Mejorar gesti√≥n de cache, agregar persistencia |
| **Preprocesamiento** | 276 | Cambiar stopwords, mejorar limpieza de texto |
| **Extracci√≥n de Conceptos** | 346 | Cambiar TF-IDF a BM25, ajustar par√°metros |
| **An√°lisis de Temas** | 533 | Cambiar LDA a NMF, agregar BERTopic |
| **An√°lisis de Sentimientos** | 749 | Mejorar VADER, agregar an√°lisis de emociones |
| **Mapas Conceptuales** | 1918 | Cambiar colores, layouts, agregar 3D |
| **Mapas Mentales** | 2214 | Ajustar jerarqu√≠a, mejorar visualizaci√≥n |
| **Res√∫menes Autom√°ticos** | 1118 | Cambiar prompts LLM, agregar BART |
| **Triangulaci√≥n** | 2320 | Mejorar validaci√≥n multi-fuente |
| **Clustering** | 3637 | Cambiar K-means a DBSCAN, agregar jer√°rquico |
| **Nubes de Palabras** | 3819 | Cambiar colores, tama√±os, m√°scaras |
| **Visualizaciones UI** | 4363 | Modificar dashboards, gr√°ficos |
| **Optimizaci√≥n** | 1589 | Mejorar paralelizaci√≥n, cache distribuido |
| **Configuraci√≥n** | 1711 | Agregar par√°metros, validaciones |
| **Renderizado Principal** | 5890 | Modificar tabs, agregar nuevas vistas |

---

## üé® Ejemplos de Modificaciones Comunes

### 1. Cambiar Algoritmo de Extracci√≥n de Conceptos

**Ve a:** L√≠nea 346 (Secci√≥n 6)

**Cambio:** De TF-IDF a BM25

```python
# ANTES (L√≠nea ~460)
def _extract_with_tfidf(self, texts: List[str]) -> List[ConceptData]:
    vectorizer = TfidfVectorizer(...)
    # ...

# DESPU√âS
def _extract_with_bm25(self, texts: List[str]) -> List[ConceptData]:
    """Extraer conceptos usando BM25"""
    from rank_bm25 import BM25Okapi
    
    tokenized_texts = [text.split() for text in texts]
    bm25 = BM25Okapi(tokenized_texts)
    
    # Obtener t√©rminos √∫nicos
    all_terms = set()
    for text in tokenized_texts:
        all_terms.update(text)
    
    # Rankear t√©rminos
    term_scores = {}
    for term in all_terms:
        scores = bm25.get_scores([term])
        term_scores[term] = np.mean(scores)
    
    # Crear ConceptData
    concepts = []
    for term, score in sorted(term_scores.items(), key=lambda x: x[1], reverse=True)[:50]:
        concepts.append(ConceptData(
            concept=term,
            score=score,
            frequency=sum(1 for text in tokenized_texts if term in text)
        ))
    
    return concepts

# Y en el m√©todo principal (L√≠nea ~430)
def _extract_concepts_advanced(self, chunks: List[Dict]) -> List[ConceptData]:
    texts = self._prepare_texts(chunks)
    
    # Cambiar a BM25
    if len(texts) >= 2:
        concepts = self._extract_with_bm25(texts)  # ‚Üê Cambio aqu√≠
    else:
        concepts = self._extract_with_frequency(texts)
    
    return self._enrich_concepts(concepts, chunks)
```

---

### 2. Agregar An√°lisis de Emociones

**Ve a:** L√≠nea 749 (Secci√≥n 8)

**Agregar despu√©s de** `SentimentAnalyzer`:

```python
# L√≠nea ~900 (despu√©s de SentimentAnalyzer)
class EmotionAnalyzer(BaseAnalyzer):
    """Analizador de emociones espec√≠ficas"""
    
    def __init__(self, config: AnalysisConfig):
        super().__init__(config)
        self.emotions = ['joy', 'anger', 'sadness', 'fear', 'surprise', 'love']
    
    def analyze(self, chunks: List[Dict]) -> AnalysisResult:
        """Analizar emociones en el contenido"""
        from transformers import pipeline
        
        emotion_pipeline = pipeline(
            "text-classification",
            model="j-hartmann/emotion-english-distilroberta-base",
            top_k=None
        )
        
        results = {emotion: 0 for emotion in self.emotions}
        by_chunk = []
        
        for chunk in chunks:
            content = chunk.get('content', '')[:512]
            if not content:
                continue
            
            emotions = emotion_pipeline(content)[0]
            dominant = max(emotions, key=lambda x: x['score'])
            
            results[dominant['label']] += 1
            by_chunk.append({
                'content': content[:100],
                'emotions': emotions,
                'dominant': dominant['label']
            })
        
        return AnalysisResult(
            analysis_type=AnalysisType.SENTIMENT_ANALYSIS,
            data={'emotions': results, 'by_chunk': by_chunk},
            metadata={'model': 'distilroberta-emotion'}
        )

# Registrar en factory
AnalyzerFactory.register('emotion_analysis', EmotionAnalyzer)

# Y en AdvancedQualitativeAnalyzer.__init__ (L√≠nea ~1020)
self.emotion_analyzer = EmotionAnalyzer(self.config)

# Agregar m√©todo p√∫blico (L√≠nea ~1350)
def analyze_emotions(self, chunks: List[Dict]) -> Dict:
    """Analizar emociones del contenido"""
    result = self.emotion_analyzer.analyze(chunks)
    return result.data
```

---

### 3. Mejorar Visualizaci√≥n de Mapas Conceptuales

**Ve a:** L√≠nea 1918 (Secci√≥n 10)

**Cambio:** Agregar visualizaci√≥n 3D

```python
# Agregar m√©todo nuevo (L√≠nea ~2100)
def create_3d_concept_map(self, chunks: List[Dict]) -> go.Figure:
    """Crear mapa conceptual en 3D"""
    import plotly.graph_objects as go
    import networkx as nx
    
    # Extraer conceptos
    concepts = self.extract_key_concepts(chunks)
    
    # Crear grafo
    G = nx.Graph()
    for concept in concepts[:20]:  # Limitar para claridad
        G.add_node(concept['concept'], weight=concept['score'])
    
    # Agregar aristas (conceptos relacionados)
    for i, c1 in enumerate(concepts[:20]):
        for j, c2 in enumerate(concepts[i+1:20], i+1):
            if c2['concept'] in c1.get('related_concepts', []):
                G.add_edge(c1['concept'], c2['concept'])
    
    # Layout 3D usando spring
    pos = nx.spring_layout(G, dim=3, seed=42)
    
    # Extraer coordenadas
    edge_traces = []
    for edge in G.edges():
        x0, y0, z0 = pos[edge[0]]
        x1, y1, z1 = pos[edge[1]]
        
        edge_traces.append(
            go.Scatter3d(
                x=[x0, x1, None],
                y=[y0, y1, None],
                z=[z0, z1, None],
                mode='lines',
                line=dict(color='gray', width=2),
                hoverinfo='none'
            )
        )
    
    # Nodos
    node_x, node_y, node_z = [], [], []
    node_text, node_sizes = [], []
    
    for node in G.nodes():
        x, y, z = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_z.append(z)
        node_text.append(node)
        node_sizes.append(G.nodes[node]['weight'] * 100)
    
    node_trace = go.Scatter3d(
        x=node_x, y=node_y, z=node_z,
        mode='markers+text',
        text=node_text,
        textposition='top center',
        marker=dict(
            size=node_sizes,
            color='lightblue',
            line=dict(color='darkblue', width=2)
        ),
        hovertext=node_text,
        hoverinfo='text'
    )
    
    # Crear figura
    fig = go.Figure(data=edge_traces + [node_trace])
    fig.update_layout(
        title='Mapa Conceptual 3D Interactivo',
        showlegend=False,
        hovermode='closest',
        scene=dict(
            xaxis=dict(showgrid=False, showticklabels=False, title=''),
            yaxis=dict(showgrid=False, showticklabels=False, title=''),
            zaxis=dict(showgrid=False, showticklabels=False, title='')
        ),
        margin=dict(l=0, r=0, b=0, t=40)
    )
    
    return fig

# Y agregar funci√≥n de renderizado (L√≠nea ~4800)
def render_3d_concept_map(analyzer, chunks):
    """Renderizar mapa conceptual 3D"""
    st.header("üó∫Ô∏è Mapa Conceptual 3D")
    
    if st.button("Generar Mapa 3D"):
        with st.spinner("Generando visualizaci√≥n 3D..."):
            fig = analyzer.create_3d_concept_map(chunks)
            st.plotly_chart(fig, use_container_width=True)

# Agregar tab en render() (L√≠nea ~5950)
tabs = st.tabs([
    # ... existentes ...
    "üó∫Ô∏è Mapa 3D"  # NUEVO
])

with tabs[N]:
    render_3d_concept_map(analyzer, chunks)
```

---

### 4. Implementar Resumen con BART

**Ve a:** L√≠nea 1118 (Secci√≥n 12)

**Agregar:**

```python
# L√≠nea ~1350
def generate_abstractive_summary(self, chunks: List[Dict], max_length: int = 500) -> str:
    """Generar resumen abstractivo usando BART"""
    try:
        from transformers import pipeline
        
        # Inicializar pipeline
        summarizer = pipeline(
            "summarization",
            model="facebook/bart-large-cnn"
        )
        
        # Combinar contenido
        all_text = " ".join([
            chunk.get('content', '') for chunk in chunks 
            if chunk.get('content')
        ])
        
        # Limitar longitud
        words = all_text.split()
        if len(words) > 1024:
            all_text = " ".join(words[:1024])
        
        # Generar resumen
        summary = summarizer(
            all_text,
            max_length=max_length,
            min_length=100,
            do_sample=False
        )[0]['summary_text']
        
        return f"**Resumen Abstractivo (BART)**:\n\n{summary}"
        
    except Exception as e:
        logger.error(f"Error en resumen abstractivo: {e}")
        # Fallback a resumen b√°sico
        return self.generate_rag_summary(chunks, max_length)

# Y en render_automatic_summary (L√≠nea ~4650)
summary_type = st.selectbox(
    "Tipo de Resumen:",
    options=["comprehensive", "executive", "analytical", "thematic", "abstractive"],  # ‚Üê Agregar
    format_func=lambda x: {
        "comprehensive": "üìã Comprehensivo",
        "executive": "üéØ Ejecutivo",
        "analytical": "üîç Anal√≠tico",
        "thematic": "üè∑Ô∏è Tem√°tico",
        "abstractive": "ü§ñ Abstractivo (BART)"  # ‚Üê Nuevo
    }[x]
)

# Modificar la generaci√≥n
if summary_type == "abstractive":
    summary = analyzer.generate_abstractive_summary(chunks, max_length=500)
else:
    result = analyzer.generate_intelligent_summary(chunks, summary_type)
    summary = result['summary']
```

---

## üîç B√∫squeda R√°pida de Funcionalidades

### Por Palabra Clave

| üîé Buscar | üìç Ir a L√≠nea | üéØ Encontrar√°s |
|-----------|---------------|----------------|
| `AnalysisConfig` | 145 | Configuraci√≥n central |
| `CacheManager` | 223 | Sistema de cache |
| `TextPreprocessor` | 276 | Preprocesamiento de texto |
| `ConceptExtractor` | 346 | Extracci√≥n de conceptos |
| `ThemeAnalyzer` | 533 | An√°lisis de temas |
| `SentimentAnalyzer` | 749 | An√°lisis de sentimientos |
| `create_interactive_concept_map` | 1918 | Mapas conceptuales |
| `create_interactive_mind_map` | 2214 | Mapas mentales |
| `generate_intelligent_summary` | 1200 | Res√∫menes con LLM |
| `perform_triangulation` | 2320 | Triangulaci√≥n |
| `perform_clustering` | 3637 | Clustering |
| `generate_word_cloud` | 3819 | Nubes de palabras |
| `perform_parallel_analysis` | 1589 | An√°lisis paralelo |
| `optimize_performance` | 1735 | Optimizaci√≥n autom√°tica |

---

## üõ†Ô∏è Modificaciones Comunes

### Cambiar N√∫mero de Conceptos por Defecto

**L√≠nea 145** ‚Üí `AnalysisConfig`
```python
@dataclass
class AnalysisConfig:
    max_concepts: int = 100  # Era 50
```

### Habilitar/Deshabilitar Cache

**L√≠nea 1020** ‚Üí `__init__` de `AdvancedQualitativeAnalyzer`
```python
def __init__(self, config: Optional[AnalysisConfig] = None):
    self.config = config or AnalysisConfig(
        enable_cache=False  # Deshabilitar cache
    )
```

### Ajustar Umbrales de Sentimiento

**L√≠nea 800** ‚Üí `_analyze_with_textblob_vader`
```python
# Cambiar umbrales
if polarity > 0.2:  # Era 0.1
    sentiment = 'positive'
elif polarity < -0.2:  # Era -0.1
    sentiment = 'negative'
```

### Cambiar Colores de Mapas Conceptuales

**L√≠nea 2000** ‚Üí `create_interactive_concept_map`
```python
professional_palette = {
    'main_theme': {
        'background': '#1a237e',  # Cambiar color principal
        'border': '#0d1642',
        'text': '#ffffff'
    }
}
```

### Agregar Nueva Stopword

**L√≠nea 276** ‚Üí `TextPreprocessor`
```python
def __init__(self):
    self.stopwords_cache = None
    self.custom_stopwords = {
        'palabra_nueva',  # ‚Üê Agregar aqu√≠
        'otra_palabra'
    }
```

---

## üìä Mejoras Recomendadas por Prioridad

### üî• Alta Prioridad (Hacer Primero)

1. **Implementar BM25 para Conceptos** (L√≠nea 346)
   - Mejora: 30% m√°s preciso que TF-IDF
   - Complejidad: Media
   - Tiempo: 2 horas

2. **Agregar Cache Persistente** (L√≠nea 223)
   - Mejora: Reduce 90% tiempo en an√°lisis repetidos
   - Complejidad: Baja
   - Tiempo: 1 hora

3. **Optimizar Procesamiento Paralelo** (L√≠nea 1589)
   - Mejora: 3x m√°s r√°pido en an√°lisis m√∫ltiples
   - Complejidad: Media
   - Tiempo: 3 horas

### ‚ö° Media Prioridad (Hacer Despu√©s)

4. **Implementar BERTopic** (L√≠nea 533)
   - Mejora: Temas m√°s coherentes y sem√°nticos
   - Complejidad: Alta
   - Tiempo: 5 horas

5. **Agregar Mapas 3D** (L√≠nea 1918)
   - Mejora: Mejor visualizaci√≥n de relaciones
   - Complejidad: Media
   - Tiempo: 3 horas

6. **Res√∫menes Abstractivos con BART** (L√≠nea 1118)
   - Mejora: Res√∫menes m√°s naturales
   - Complejidad: Alta
   - Tiempo: 4 horas

### üí° Baja Prioridad (Mejoras Futuras)

7. **An√°lisis de Complejidad Textual**
   - Mejora: M√©tricas adicionales
   - Complejidad: Baja
   - Tiempo: 2 horas

8. **Exportaci√≥n a Formatos M√∫ltiples**
   - Mejora: PDF, DOCX, LaTeX
   - Complejidad: Media
   - Tiempo: 4 horas

9. **Dashboard Personalizable**
   - Mejora: UX mejorada
   - Complejidad: Media
   - Tiempo: 5 horas

---

## üéØ Checklist de Verificaci√≥n

Antes de hacer cambios:
- [ ] Leer documentaci√≥n de la secci√≥n
- [ ] Identificar dependencias
- [ ] Hacer backup del c√≥digo
- [ ] Verificar tests existentes

Durante los cambios:
- [ ] Mantener firma de m√©todos p√∫blicos
- [ ] Agregar logging apropiado
- [ ] Actualizar docstrings
- [ ] Preservar fallbacks

Despu√©s de los cambios:
- [ ] Ejecutar tests
- [ ] Verificar linting (sin errores)
- [ ] Probar con datos reales
- [ ] Actualizar documentaci√≥n

---

## üêõ Debugging R√°pido

### Error: "AttributeError: 'NoneType' object has no attribute..."

**Causa**: Configuraci√≥n no inicializada correctamente

**Soluci√≥n**: Verificar en `__init__` (L√≠nea ~1020)
```python
# Asegurar que config no sea None
from config.settings import config as global_config
self.cache_path = Path(global_config.CACHE_DIR) / "rag_cache.json"
```

### Error: "KeyError: 'words'"

**Causa**: Formato inconsistente entre m√©todos

**Soluci√≥n**: Usar `.get()` con fallback (L√≠nea ~4526)
```python
keywords = topic.get('words', topic.get('keywords', []))
```

### Error: "TF-IDF no funciona"

**Causa**: Muy pocos documentos

**Soluci√≥n**: Verificar fallback (L√≠nea ~460)
```python
if SKLEARN_AVAILABLE and len(texts) >= 2:  # M√≠nimo 2 documentos
    concepts = self._extract_with_tfidf(texts)
else:
    concepts = self._extract_with_frequency(texts)  # Fallback
```

---

## üìù Plantillas de C√≥digo

### Plantilla para Nuevo Analizador

```python
from ..core.base import BaseAnalyzer
from ..core.config import AnalysisConfig, AnalysisResult, AnalysisType

class MiNuevoAnalizador(BaseAnalyzer):
    """Descripci√≥n del analizador"""
    
    def __init__(self, config: AnalysisConfig):
        super().__init__(config)
        # Inicializar componentes
    
    def analyze(self, chunks: List[Dict]) -> AnalysisResult:
        """M√©todo principal de an√°lisis"""
        # 1. Validar
        if not self._validate_input(chunks):
            return AnalysisResult(
                analysis_type=AnalysisType.TU_TIPO,
                data={},
                metadata={'error': 'Invalid input'}
            )
        
        # 2. Cache
        cache_key = self._generate_cache_key(chunks)
        cached = self.cache_manager.get(cache_key)
        if cached:
            return cached
        
        # 3. Procesar
        result_data = self._do_analysis(chunks)
        
        # 4. Crear resultado
        result = AnalysisResult(
            analysis_type=AnalysisType.TU_TIPO,
            data=result_data,
            metadata={'info': 'metadata'}
        )
        
        # 5. Guardar en cache
        self.cache_manager.set(cache_key, result)
        
        return result
    
    def _do_analysis(self, chunks):
        """L√≥gica de an√°lisis"""
        pass

# Registrar
from ..core.base import AnalyzerFactory
AnalyzerFactory.register('mi_analisis', MiNuevoAnalizador)
```

### Plantilla para Nueva Visualizaci√≥n

```python
def render_mi_visualizacion(analyzer, chunks):
    """Renderizar mi nueva visualizaci√≥n"""
    st.header("üìä Mi Visualizaci√≥n")
    
    # Configuraci√≥n
    col1, col2 = st.columns([3, 1])
    
    with col1:
        param = st.slider("Par√°metro", 1, 10, 5)
    
    with col2:
        if st.button("üîÑ Generar"):
            if 'mi_viz_cache' in st.session_state:
                del st.session_state.mi_viz_cache
    
    # Generar
    if 'mi_viz_cache' not in st.session_state:
        with st.spinner("Generando..."):
            st.session_state.mi_viz_cache = analyzer.mi_metodo(chunks, param)
    
    data = st.session_state.mi_viz_cache
    
    # Visualizar
    if data:
        st.plotly_chart(data, use_container_width=True)
    else:
        st.error("No se pudo generar la visualizaci√≥n")
```

---

## üéì Conclusi√≥n

Esta gu√≠a r√°pida te permite:

‚úÖ Navegar el c√≥digo actual f√°cilmente
‚úÖ Hacer modificaciones comunes sin riesgo
‚úÖ Agregar nuevas funcionalidades de forma modular
‚úÖ Debugging r√°pido de problemas comunes
‚úÖ Seguir mejores pr√°cticas

**Recuerda**: Siempre verifica que tus cambios no rompan funcionalidades existentes ejecutando los tests despu√©s de cada modificaci√≥n.

