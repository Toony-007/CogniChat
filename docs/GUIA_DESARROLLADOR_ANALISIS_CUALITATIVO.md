# ğŸ‘¨â€ğŸ’» GuÃ­a de Desarrollador - MÃ³dulo de AnÃ¡lisis Cualitativo

## ğŸ“‹ Tabla de Contenidos

1. [IntroducciÃ³n](#introducciÃ³n)
2. [ConfiguraciÃ³n del Entorno](#configuraciÃ³n-del-entorno)
3. [Arquitectura del CÃ³digo](#arquitectura-del-cÃ³digo)
4. [GuÃ­a de ContribuciÃ³n](#guÃ­a-de-contribuciÃ³n)
5. [Patrones de DiseÃ±o](#patrones-de-diseÃ±o)
6. [Testing](#testing)
7. [Debugging](#debugging)
8. [OptimizaciÃ³n](#optimizaciÃ³n)
9. [Extensibilidad](#extensibilidad)

---

## ğŸ¯ IntroducciÃ³n

Esta guÃ­a estÃ¡ dirigida a desarrolladores que quieren entender, modificar o extender el mÃ³dulo de anÃ¡lisis cualitativo. Cubre la arquitectura, patrones de diseÃ±o, mejores prÃ¡cticas y cÃ³mo contribuir al proyecto.

### ğŸ¯ **Objetivos de esta GuÃ­a**

- âœ… **Comprender** la arquitectura del mÃ³dulo
- âœ… **Contribuir** efectivamente al cÃ³digo
- âœ… **Extender** funcionalidades existentes
- âœ… **Debuggear** problemas eficientemente
- âœ… **Optimizar** el rendimiento
- âœ… **Mantener** la calidad del cÃ³digo

---

## ğŸ› ï¸ ConfiguraciÃ³n del Entorno

### ğŸ“¦ **Dependencias Requeridas**

```bash
# Dependencias principales
pip install streamlit
pip install scikit-learn
pip install nltk
pip install pyvis
pip install streamlit-agraph
pip install plotly
pip install pandas
pip install numpy

# Dependencias opcionales
pip install textblob
pip install vaderSentiment
pip install wordcloud
pip install python-docx
pip install reportlab
pip install pyperclip

# Dependencias de desarrollo
pip install pytest
pip install black
pip install flake8
pip install mypy
```

### ğŸ”§ **ConfiguraciÃ³n del Proyecto**

```bash
# Clonar el repositorio
git clone https://github.com/Toony-007/CogniChat.git
cd CogniChat

# Crear entorno virtual
python -m venv cognichat-env
source cognichat-env/bin/activate  # Linux/Mac
# cognichat-env\Scripts\activate  # Windows

# Instalar dependencias
pip install -r requirements.txt

# Configurar NLTK
python -c "import nltk; nltk.download('stopwords'); nltk.download('punkt')"

# Verificar instalaciÃ³n
python -c "from modules.qualitative_analysis import AdvancedQualitativeAnalyzer; print('âœ… InstalaciÃ³n exitosa')"
```

### âš™ï¸ **Variables de Entorno**

```bash
# .env
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_LLM_MODEL=deepseek-r1:7b
DEFAULT_EMBEDDING_MODEL=nomic-embed-text:latest
CHUNK_SIZE=2000
CHUNK_OVERLAP=300
MAX_RETRIEVAL_DOCS=15
SIMILARITY_THRESHOLD=0.6
MAX_RESPONSE_TOKENS=3000
ENABLE_DEBUG_MODE=true
ENABLE_CACHE=true
```

---

## ğŸ—ï¸ Arquitectura del CÃ³digo

### ğŸ“ **Estructura del MÃ³dulo**

```
modules/qualitative_analysis.py
â”œâ”€â”€ 1. IMPORTS Y CONFIGURACIÃ“N GLOBAL
â”œâ”€â”€ 2. ENUMS, DATACLASSES Y ESTRUCTURAS DE DATOS
â”œâ”€â”€ 3. CLASES BASE Y INTERFACES
â”œâ”€â”€ 4. GESTIÃ“N DE CACHE Y MEMORIA
â”œâ”€â”€ 5. PREPROCESAMIENTO DE TEXTO
â”œâ”€â”€ 6. EXTRACCIÃ“N DE CONCEPTOS
â”œâ”€â”€ 7. ANÃLISIS DE TEMAS
â”œâ”€â”€ 8. ANÃLISIS DE SENTIMIENTOS
â”œâ”€â”€ 9. CLUSTERING Y AGRUPACIÃ“N
â”œâ”€â”€ 10. MAPAS CONCEPTUALES INTERACTIVOS
â”œâ”€â”€ 11. MAPAS MENTALES
â”œâ”€â”€ 12. RESUMENES AUTOMÃTICOS
â”œâ”€â”€ 13. ANÃLISIS DE TRIANGULACIÃ“N
â”œâ”€â”€ 14. NUBES DE PALABRAS
â”œâ”€â”€ 15. VISUALIZACIONES Y GRÃFICOS
â”œâ”€â”€ 16. ANÃLISIS PARALELO Y OPTIMIZACIÃ“N
â”œâ”€â”€ 17. MÃ‰TODOS DE CONFIGURACIÃ“N
â”œâ”€â”€ 18. FUNCIONES DE RENDERIZADO (STREAMLIT)
â””â”€â”€ 19. FUNCIÃ“N PRINCIPAL DE RENDERIZADO
```

### ğŸ”§ **Componentes Principales**

#### **1. Clases Base**

```python
# Base para todos los analizadores
class BaseAnalyzer(ABC):
    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.logger = setup_logger()
    
    @abstractmethod
    def analyze(self, chunks: List[Dict]) -> AnalysisResult:
        pass
    
    def _validate_input(self, chunks: List[Dict]) -> bool:
        return bool(chunks and all('content' in chunk for chunk in chunks))
```

#### **2. GestiÃ³n de Cache**

```python
class CacheManager:
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
        self.lock = threading.Lock()
    
    def get(self, key: str) -> Optional[Any]:
        # ImplementaciÃ³n thread-safe
        pass
    
    def set(self, key: str, value: Any) -> None:
        # ImplementaciÃ³n con evicciÃ³n LRU
        pass
```

#### **3. Clase Principal**

```python
class AdvancedQualitativeAnalyzer:
    def __init__(self, config: Optional[AnalysisConfig] = None):
        self.config = config or AnalysisConfig()
        self.preprocessor = TextPreprocessor()
        self.cache_manager = CacheManager()
        self.concept_extractor = ConceptExtractor(self.config)
        self.theme_analyzer = ThemeAnalyzer(self.config)
        self.sentiment_analyzer = SentimentAnalyzer(self.config)
    
    # MÃ©todos principales de anÃ¡lisis
    def extract_key_concepts(self, chunks: List[Dict]) -> List[Dict]:
        pass
    
    def extract_advanced_themes(self, chunks: List[Dict]) -> Dict:
        pass
    
    def advanced_sentiment_analysis(self, chunks: List[Dict]) -> Dict:
        pass
```

---

## ğŸ¤ GuÃ­a de ContribuciÃ³n

### ğŸ”„ **Flujo de Trabajo**

```bash
# 1. Fork del repositorio
git clone https://github.com/TU-USUARIO/CogniChat.git
cd CogniChat

# 2. Crear rama para feature
git checkout -b feature/nueva-funcionalidad

# 3. Hacer cambios
# ... cÃ³digo ...

# 4. Ejecutar tests
pytest tests/

# 5. Verificar formato
black modules/qualitative_analysis.py
flake8 modules/qualitative_analysis.py

# 6. Commit
git add .
git commit -m "feat: agregar nueva funcionalidad"

# 7. Push
git push origin feature/nueva-funcionalidad

# 8. Crear Pull Request
```

### ğŸ“ **EstÃ¡ndares de CÃ³digo**

#### **Formato de CÃ³digo**

```python
# Usar Black para formateo
black modules/qualitative_analysis.py

# Verificar con flake8
flake8 modules/qualitative_analysis.py

# Verificar tipos con mypy
mypy modules/qualitative_analysis.py
```

#### **Convenciones de Nomenclatura**

```python
# Clases: PascalCase
class AdvancedQualitativeAnalyzer:
    pass

# MÃ©todos y variables: snake_case
def extract_key_concepts(self, chunks: List[Dict]) -> List[Dict]:
    pass

# Constantes: UPPER_SNAKE_CASE
MAX_CONCEPTS = 50
DEFAULT_CONFIG = AnalysisConfig()

# Enums: PascalCase
class AnalysisType(Enum):
    CONCEPT_EXTRACTION = "concept_extraction"
    THEME_ANALYSIS = "theme_analysis"
```

#### **DocumentaciÃ³n**

```python
def extract_key_concepts(self, chunks: List[Dict], min_freq: int = 2) -> List[Dict]:
    """
    Extrae conceptos clave de los chunks de texto.
    
    Args:
        chunks: Lista de chunks de texto con contenido y metadatos
        min_freq: Frecuencia mÃ­nima para considerar un concepto
        
    Returns:
        Lista de conceptos con score, frecuencia y contexto
        
    Raises:
        ValueError: Si chunks estÃ¡ vacÃ­o o mal formateado
        
    Example:
        >>> chunks = [{'content': 'Texto...', 'metadata': {...}}]
        >>> concepts = analyzer.extract_key_concepts(chunks)
        >>> print(f"Conceptos encontrados: {len(concepts)}")
    """
    pass
```

### ğŸ§ª **Testing**

#### **Estructura de Tests**

```python
# tests/test_qualitative_analysis.py
import pytest
from modules.qualitative_analysis import AdvancedQualitativeAnalyzer, AnalysisConfig

class TestAdvancedQualitativeAnalyzer:
    def setup_method(self):
        self.config = AnalysisConfig(min_frequency=1, max_concepts=10)
        self.analyzer = AdvancedQualitativeAnalyzer(self.config)
        self.sample_chunks = [
            {
                'content': 'La inteligencia artificial estÃ¡ transformando la educaciÃ³n.',
                'metadata': {'source_file': 'test.pdf'}
            },
            {
                'content': 'Machine learning permite personalizar el aprendizaje.',
                'metadata': {'source_file': 'test.pdf'}
            }
        ]
    
    def test_extract_key_concepts(self):
        """Test extracciÃ³n de conceptos bÃ¡sica"""
        concepts = self.analyzer.extract_key_concepts(self.sample_chunks)
        
        assert len(concepts) > 0
        assert all('concept' in concept for concept in concepts)
        assert all('score' in concept for concept in concepts)
        assert all('frequency' in concept for concept in concepts)
    
    def test_extract_key_concepts_empty_input(self):
        """Test con entrada vacÃ­a"""
        concepts = self.analyzer.extract_key_concepts([])
        assert concepts == []
    
    def test_extract_key_concepts_invalid_input(self):
        """Test con entrada invÃ¡lida"""
        with pytest.raises(ValueError):
            self.analyzer.extract_key_concepts([{'invalid': 'data'}])
    
    def test_cache_functionality(self):
        """Test funcionalidad de cache"""
        # Primera ejecuciÃ³n
        concepts1 = self.analyzer.extract_key_concepts(self.sample_chunks)
        
        # Segunda ejecuciÃ³n (debe usar cache)
        concepts2 = self.analyzer.extract_key_concepts(self.sample_chunks)
        
        assert concepts1 == concepts2
        assert len(self.analyzer.cache_manager.cache) > 0
```

#### **Ejecutar Tests**

```bash
# Ejecutar todos los tests
pytest

# Ejecutar tests especÃ­ficos
pytest tests/test_qualitative_analysis.py::TestAdvancedQualitativeAnalyzer::test_extract_key_concepts

# Ejecutar con cobertura
pytest --cov=modules.qualitative_analysis --cov-report=html

# Ejecutar tests de rendimiento
pytest tests/test_performance.py -v
```

---

## ğŸ¨ Patrones de DiseÃ±o

### ğŸ­ **PatrÃ³n Factory**

```python
class AnalyzerFactory:
    @staticmethod
    def create_analyzer(analyzer_type: str, config: AnalysisConfig) -> BaseAnalyzer:
        """Factory para crear analizadores especÃ­ficos"""
        analyzers = {
            'concept': ConceptExtractor,
            'theme': ThemeAnalyzer,
            'sentiment': SentimentAnalyzer,
            'clustering': ClusteringAnalyzer
        }
        
        if analyzer_type not in analyzers:
            raise ValueError(f"Tipo de analizador no soportado: {analyzer_type}")
        
        return analyzers[analyzer_type](config)

# Uso
config = AnalysisConfig()
concept_analyzer = AnalyzerFactory.create_analyzer('concept', config)
```

### ğŸ¯ **PatrÃ³n Strategy**

```python
class ConceptExtractionStrategy(ABC):
    @abstractmethod
    def extract(self, texts: List[str]) -> List[Dict]:
        pass

class TFIDFStrategy(ConceptExtractionStrategy):
    def extract(self, texts: List[str]) -> List[Dict]:
        # ImplementaciÃ³n TF-IDF
        pass

class FrequencyStrategy(ConceptExtractionStrategy):
    def extract(self, texts: List[str]) -> List[Dict]:
        # ImplementaciÃ³n por frecuencia
        pass

class ConceptExtractor:
    def __init__(self):
        self.strategies = {
            'tfidf': TFIDFStrategy(),
            'frequency': FrequencyStrategy()
        }
    
    def extract_concepts(self, texts: List[str], strategy: str = 'tfidf'):
        return self.strategies[strategy].extract(texts)
```

### ğŸ‘ï¸ **PatrÃ³n Observer**

```python
class AnalysisObserver(ABC):
    @abstractmethod
    def update(self, event: str, data: Any) -> None:
        pass

class MetricsObserver(AnalysisObserver):
    def update(self, event: str, data: Any) -> None:
        if event == 'analysis_completed':
            self.record_metrics(data)

class CacheObserver(AnalysisObserver):
    def update(self, event: str, data: Any) -> None:
        if event == 'analysis_completed':
            self.cache_results(data)

class AdvancedQualitativeAnalyzer:
    def __init__(self):
        self.observers = []
    
    def add_observer(self, observer: AnalysisObserver):
        self.observers.append(observer)
    
    def notify_observers(self, event: str, data: Any):
        for observer in self.observers:
            observer.update(event, data)
```

---

## ğŸ› Debugging

### ğŸ” **Herramientas de Debug**

#### **Logging Configurado**

```python
import logging

# Configurar logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/qualitative_analysis.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Uso en el cÃ³digo
def extract_key_concepts(self, chunks: List[Dict]) -> List[Dict]:
    logger.info(f"Iniciando extracciÃ³n de conceptos para {len(chunks)} chunks")
    
    try:
        # ... cÃ³digo ...
        logger.debug(f"Conceptos extraÃ­dos: {len(concepts)}")
        return concepts
    except Exception as e:
        logger.error(f"Error en extracciÃ³n de conceptos: {e}")
        raise
```

#### **Debugging con PDB**

```python
import pdb

def extract_key_concepts(self, chunks: List[Dict]) -> List[Dict]:
    # ... cÃ³digo ...
    
    # Punto de quiebre para debugging
    pdb.set_trace()
    
    # ... mÃ¡s cÃ³digo ...
```

#### **Debugging con Streamlit**

```python
def render_debug_info(self, chunks: List[Dict]):
    """Renderizar informaciÃ³n de debug en Streamlit"""
    if st.checkbox("Mostrar informaciÃ³n de debug"):
        st.subheader("ğŸ” InformaciÃ³n de Debug")
        
        # InformaciÃ³n del cache
        cache_stats = self.cache_manager.get_stats()
        st.write(f"**Cache hits:** {cache_stats['hits']}")
        st.write(f"**Cache misses:** {cache_stats['misses']}")
        
        # InformaciÃ³n de chunks
        st.write(f"**Total chunks:** {len(chunks)}")
        st.write(f"**Chunks con contenido:** {len([c for c in chunks if c.get('content')])}")
        
        # InformaciÃ³n de configuraciÃ³n
        st.write(f"**ConfiguraciÃ³n:** {self.config.__dict__}")
```

### ğŸ”§ **Herramientas de Profiling**

#### **Profiling de Rendimiento**

```python
import cProfile
import pstats

def profile_analysis():
    """Profilar el rendimiento del anÃ¡lisis"""
    analyzer = AdvancedQualitativeAnalyzer()
    chunks = load_test_chunks()
    
    # Crear profiler
    profiler = cProfile.Profile()
    profiler.enable()
    
    # Ejecutar anÃ¡lisis
    concepts = analyzer.extract_key_concepts(chunks)
    
    profiler.disable()
    
    # Mostrar estadÃ­sticas
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)  # Top 20 funciones mÃ¡s lentas
```

#### **Monitoreo de Memoria**

```python
import tracemalloc

def monitor_memory():
    """Monitorear uso de memoria"""
    tracemalloc.start()
    
    analyzer = AdvancedQualitativeAnalyzer()
    chunks = load_large_chunks()
    
    # Ejecutar anÃ¡lisis
    concepts = analyzer.extract_key_concepts(chunks)
    
    # Obtener estadÃ­sticas de memoria
    current, peak = tracemalloc.get_traced_memory()
    print(f"Memoria actual: {current / 1024 / 1024:.1f} MB")
    print(f"Memoria pico: {peak / 1024 / 1024:.1f} MB")
    
    tracemalloc.stop()
```

---

## âš¡ OptimizaciÃ³n

### ğŸš€ **Optimizaciones de Rendimiento**

#### **Cache Inteligente**

```python
class SmartCache:
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.access_times = {}
        self.access_counts = {}
        self.max_size = max_size
    
    def get(self, key: str) -> Optional[Any]:
        if key in self.cache:
            self.access_times[key] = time.time()
            self.access_counts[key] = self.access_counts.get(key, 0) + 1
            return self.cache[key]
        return None
    
    def set(self, key: str, value: Any) -> None:
        if len(self.cache) >= self.max_size:
            self._evict_least_valuable()
        
        self.cache[key] = value
        self.access_times[key] = time.time()
        self.access_counts[key] = 1
    
    def _evict_least_valuable(self):
        """Evict basado en valor (frecuencia * recencia)"""
        if not self.cache:
            return
        
        current_time = time.time()
        least_valuable_key = min(
            self.cache.keys(),
            key=lambda k: self.access_counts.get(k, 1) * (current_time - self.access_times.get(k, 0))
        )
        
        del self.cache[least_valuable_key]
        del self.access_times[least_valuable_key]
        del self.access_counts[least_valuable_key]
```

#### **Procesamiento en Lotes**

```python
class BatchProcessor:
    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size
    
    def process_in_batches(self, chunks: List[Dict], processor_func) -> List[Any]:
        """Procesar chunks en lotes para optimizar memoria"""
        results = []
        
        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i:i + self.batch_size]
            batch_results = processor_func(batch)
            results.extend(batch_results)
            
            # Liberar memoria del lote
            del batch
        
        return results
```

#### **VectorizaciÃ³n con NumPy**

```python
import numpy as np
from scipy.sparse import csr_matrix

def vectorize_texts_efficiently(texts: List[str]) -> csr_matrix:
    """VectorizaciÃ³n eficiente usando sparse matrices"""
    # Usar TfidfVectorizer con sparse=True
    vectorizer = TfidfVectorizer(
        max_features=1000,
        sparse=True,  # Usar matrices sparse
        ngram_range=(1, 2)
    )
    
    # Vectorizar
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    # Convertir a CSR para operaciones eficientes
    return tfidf_matrix.tocsr()
```

### ğŸ”§ **Optimizaciones de Memoria**

#### **Generadores para Datos Grandes**

```python
def process_chunks_generator(chunks: List[Dict], batch_size: int = 100):
    """Generador para procesar chunks grandes sin cargar todo en memoria"""
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i + batch_size]
        yield batch

def analyze_large_dataset(chunks: List[Dict]):
    """Analizar dataset grande usando generadores"""
    analyzer = AdvancedQualitativeAnalyzer()
    all_concepts = []
    
    for batch in process_chunks_generator(chunks):
        batch_concepts = analyzer.extract_key_concepts(batch)
        all_concepts.extend(batch_concepts)
        
        # Liberar memoria del batch
        del batch
    
    return all_concepts
```

#### **CompresiÃ³n de Datos**

```python
import pickle
import gzip

class CompressedCache:
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
    
    def get(self, key: str) -> Optional[Any]:
        if key in self.cache:
            compressed_data = self.cache[key]
            return pickle.loads(gzip.decompress(compressed_data))
        return None
    
    def set(self, key: str, value: Any) -> None:
        if len(self.cache) >= self.max_size:
            self._evict_oldest()
        
        compressed_data = gzip.compress(pickle.dumps(value))
        self.cache[key] = compressed_data
```

---

## ğŸ”Œ Extensibilidad

### ğŸ¯ **Crear Nuevos Analizadores**

```python
class CustomAnalyzer(BaseAnalyzer):
    """Ejemplo de analizador personalizado"""
    
    def __init__(self, config: AnalysisConfig):
        super().__init__(config)
        self.custom_param = config.custom_settings.get('custom_param', 'default')
    
    def analyze(self, chunks: List[Dict]) -> AnalysisResult:
        """Implementar anÃ¡lisis personalizado"""
        if not self._validate_input(chunks):
            return AnalysisResult(
                analysis_type=AnalysisType.CUSTOM,
                data={'error': 'Invalid input'},
                metadata={'error': 'Input validation failed'}
            )
        
        try:
            # Implementar lÃ³gica personalizada
            results = self._custom_analysis(chunks)
            
            return AnalysisResult(
                analysis_type=AnalysisType.CUSTOM,
                data={'results': results},
                metadata={'total_items': len(results)}
            )
            
        except Exception as e:
            self.logger.error(f"Error en anÃ¡lisis personalizado: {e}")
            return AnalysisResult(
                analysis_type=AnalysisType.CUSTOM,
                data={'error': str(e)},
                metadata={'error': str(e)}
            )
    
    def _custom_analysis(self, chunks: List[Dict]) -> List[Dict]:
        """LÃ³gica especÃ­fica del anÃ¡lisis personalizado"""
        # Implementar aquÃ­ la lÃ³gica especÃ­fica
        pass

# Registrar el nuevo tipo de anÃ¡lisis
AnalysisType.CUSTOM = "custom"
```

### ğŸ¨ **Crear Nuevas Visualizaciones**

```python
def render_custom_visualization(analyzer: AdvancedQualitativeAnalyzer, chunks: List[Dict]):
    """Ejemplo de visualizaciÃ³n personalizada"""
    st.header("ğŸ¨ VisualizaciÃ³n Personalizada")
    
    # Obtener datos
    concepts = analyzer.extract_key_concepts(chunks)
    
    # Crear visualizaciÃ³n personalizada
    fig = go.Figure()
    
    # Agregar datos
    concept_names = [c['concept'] for c in concepts[:10]]
    concept_scores = [c['score'] for c in concepts[:10]]
    
    fig.add_trace(go.Bar(
        x=concept_names,
        y=concept_scores,
        marker_color='lightblue'
    ))
    
    fig.update_layout(
        title="Top 10 Conceptos",
        xaxis_title="Conceptos",
        yaxis_title="Score"
    )
    
    st.plotly_chart(fig, use_container_width=True)
```

### ğŸ”Œ **Crear Plugins**

```python
class AnalysisPlugin:
    """Base para plugins de anÃ¡lisis"""
    
    def __init__(self, name: str, version: str):
        self.name = name
        self.version = version
    
    def initialize(self, analyzer: AdvancedQualitativeAnalyzer):
        """Inicializar plugin con el analizador"""
        pass
    
    def process(self, data: Any) -> Any:
        """Procesar datos"""
        pass
    
    def cleanup(self):
        """Limpiar recursos del plugin"""
        pass

class SentimentPlugin(AnalysisPlugin):
    """Plugin de anÃ¡lisis de sentimientos avanzado"""
    
    def __init__(self):
        super().__init__("Advanced Sentiment", "1.0.0")
    
    def process(self, chunks: List[Dict]) -> Dict:
        """Procesar anÃ¡lisis de sentimientos"""
        # Implementar anÃ¡lisis avanzado
        pass

# Registrar plugin
plugin_manager = PluginManager()
plugin_manager.register_plugin(SentimentPlugin())
```

---

## ğŸ“Š MÃ©tricas y Monitoreo

### ğŸ“ˆ **Sistema de MÃ©tricas**

```python
class MetricsCollector:
    def __init__(self):
        self.metrics = {
            'total_analyses': 0,
            'total_processing_time': 0.0,
            'total_concepts': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'errors': 0
        }
    
    def record_analysis(self, processing_time: float, concepts_count: int, 
                       cache_hit: bool = False, error: bool = False):
        """Registrar mÃ©tricas de anÃ¡lisis"""
        self.metrics['total_analyses'] += 1
        self.metrics['total_processing_time'] += processing_time
        self.metrics['total_concepts'] += concepts_count
        
        if cache_hit:
            self.metrics['cache_hits'] += 1
        else:
            self.metrics['cache_misses'] += 1
        
        if error:
            self.metrics['errors'] += 1
    
    def get_average_metrics(self) -> Dict[str, float]:
        """Obtener mÃ©tricas promedio"""
        if self.metrics['total_analyses'] == 0:
            return {}
        
        return {
            'avg_processing_time': self.metrics['total_processing_time'] / self.metrics['total_analyses'],
            'avg_concepts_per_analysis': self.metrics['total_concepts'] / self.metrics['total_analyses'],
            'cache_hit_rate': self.metrics['cache_hits'] / (self.metrics['cache_hits'] + self.metrics['cache_misses']),
            'error_rate': self.metrics['errors'] / self.metrics['total_analyses']
        }
```

---

## ğŸ¯ Mejores PrÃ¡cticas

### âœ… **Do's**

- âœ… **Usar type hints** para mejor documentaciÃ³n
- âœ… **Implementar logging** detallado
- âœ… **Escribir tests** para nuevas funcionalidades
- âœ… **Documentar** mÃ©todos pÃºblicos
- âœ… **Usar cache** para operaciones costosas
- âœ… **Validar entrada** en todos los mÃ©todos
- âœ… **Manejar errores** graciosamente
- âœ… **Optimizar** para rendimiento

### âŒ **Don'ts**

- âŒ **No hardcodear** valores mÃ¡gicos
- âŒ **No ignorar** errores de validaciÃ³n
- âŒ **No usar** variables globales
- âŒ **No hacer** cambios breaking sin versionado
- âŒ **No olvidar** limpiar recursos
- âŒ **No usar** memoria excesiva
- âŒ **No hacer** commits sin tests
- âŒ **No ignorar** feedback de code review

---

## ğŸ‰ ConclusiÃ³n

Esta guÃ­a proporciona las herramientas y conocimientos necesarios para contribuir efectivamente al mÃ³dulo de anÃ¡lisis cualitativo. Recuerda:

### ğŸ¯ **Principios Clave**

1. **Calidad**: CÃ³digo limpio, bien documentado y testeado
2. **Rendimiento**: OptimizaciÃ³n para velocidad y memoria
3. **Mantenibilidad**: CÃ³digo modular y extensible
4. **Usabilidad**: Interfaz intuitiva y responsive
5. **Escalabilidad**: Arquitectura que crece con el proyecto

### ğŸš€ **PrÃ³ximos Pasos**

1. **Explora** el cÃ³digo existente
2. **Identifica** Ã¡reas de mejora
3. **PropÃ³n** nuevas funcionalidades
4. **Contribuye** con cÃ³digo de calidad
5. **MantÃ©n** la documentaciÃ³n actualizada

---

**Â¡Feliz coding!** ğŸš€ğŸ‘¨â€ğŸ’»

*"Construyendo el futuro del anÃ¡lisis cualitativo, una lÃ­nea de cÃ³digo a la vez."*
