# üë®‚Äçüíª Gu√≠a de Desarrollador - M√≥dulo de An√°lisis Cualitativo

## üìã Tabla de Contenidos

1. [Introducci√≥n](#introducci√≥n)
2. [Configuraci√≥n del Entorno](#configuraci√≥n-del-entorno)
3. [Arquitectura del C√≥digo](#arquitectura-del-c√≥digo)
4. [Gu√≠a de Contribuci√≥n](#gu√≠a-de-contribuci√≥n)
5. [Patrones de Dise√±o](#patrones-de-dise√±o)
6. [Testing](#testing)
7. [Debugging](#debugging)
8. [Optimizaci√≥n](#optimizaci√≥n)
9. [Extensibilidad](#extensibilidad)

---

## üéØ Introducci√≥n

Esta gu√≠a est√° dirigida a desarrolladores que quieren entender, modificar o extender el m√≥dulo de an√°lisis cualitativo. Cubre la arquitectura, patrones de dise√±o, mejores pr√°cticas y c√≥mo contribuir al proyecto.

### üéØ **Objetivos de esta Gu√≠a**

- ‚úÖ **Comprender** la arquitectura del m√≥dulo
- ‚úÖ **Contribuir** efectivamente al c√≥digo
- ‚úÖ **Extender** funcionalidades existentes
- ‚úÖ **Debuggear** problemas eficientemente
- ‚úÖ **Optimizar** el rendimiento
- ‚úÖ **Mantener** la calidad del c√≥digo

---

## üõ†Ô∏è Configuraci√≥n del Entorno

### üì¶ **Dependencias Requeridas**

```bash
# Dependencias principales
pip install streamlit
pip install scikit-learn
pip install nltk
pip install pyvis
pip install streamlit-agraph
pip install plotly
pip install pandas
pip install numpy

# Dependencias opcionales
pip install textblob
pip install vaderSentiment
pip install wordcloud
pip install python-docx
pip install reportlab
pip install pyperclip

# Dependencias de desarrollo
pip install pytest
pip install black
pip install flake8
pip install mypy
```

### üîß **Configuraci√≥n del Proyecto**

```bash
# Clonar el repositorio
git clone https://github.com/Toony-007/CogniChat.git
cd CogniChat

# Crear entorno virtual
python -m venv cognichat-env
source cognichat-env/bin/activate  # Linux/Mac
# cognichat-env\Scripts\activate  # Windows

# Instalar dependencias
pip install -r requirements.txt

# Configurar NLTK
python -c "import nltk; nltk.download('stopwords'); nltk.download('punkt')"

# Verificar instalaci√≥n
python -c "from modules.qualitative_analysis import AdvancedQualitativeAnalyzer; print('‚úÖ Instalaci√≥n exitosa')"
```

### ‚öôÔ∏è **Variables de Entorno**

```bash
# .env
OLLAMA_BASE_URL=http://localhost:11434
DEFAULT_LLM_MODEL=deepseek-r1:7b
DEFAULT_EMBEDDING_MODEL=nomic-embed-text:latest
CHUNK_SIZE=2000
CHUNK_OVERLAP=300
MAX_RETRIEVAL_DOCS=15
SIMILARITY_THRESHOLD=0.6
MAX_RESPONSE_TOKENS=3000
ENABLE_DEBUG_MODE=true
ENABLE_CACHE=true
```

---

## üèóÔ∏è Arquitectura del C√≥digo

### üìÅ **Estructura del M√≥dulo**

```
modules/qualitative_analysis.py
‚îú‚îÄ‚îÄ 1. IMPORTS Y CONFIGURACI√ìN GLOBAL
‚îú‚îÄ‚îÄ 2. ENUMS, DATACLASSES Y ESTRUCTURAS DE DATOS
‚îú‚îÄ‚îÄ 3. CLASES BASE Y INTERFACES
‚îú‚îÄ‚îÄ 4. GESTI√ìN DE CACHE Y MEMORIA
‚îú‚îÄ‚îÄ 5. PREPROCESAMIENTO DE TEXTO
‚îú‚îÄ‚îÄ 6. EXTRACCI√ìN DE CONCEPTOS
‚îú‚îÄ‚îÄ 7. AN√ÅLISIS DE TEMAS
‚îú‚îÄ‚îÄ 8. AN√ÅLISIS DE SENTIMIENTOS
‚îú‚îÄ‚îÄ 9. CLUSTERING Y AGRUPACI√ìN
‚îú‚îÄ‚îÄ 10. MAPAS CONCEPTUALES INTERACTIVOS
‚îú‚îÄ‚îÄ 11. MAPAS MENTALES
‚îú‚îÄ‚îÄ 12. RESUMENES AUTOM√ÅTICOS
‚îú‚îÄ‚îÄ 13. AN√ÅLISIS DE TRIANGULACI√ìN
‚îú‚îÄ‚îÄ 14. NUBES DE PALABRAS
‚îú‚îÄ‚îÄ 15. VISUALIZACIONES Y GR√ÅFICOS
‚îú‚îÄ‚îÄ 16. AN√ÅLISIS PARALELO Y OPTIMIZACI√ìN
‚îú‚îÄ‚îÄ 17. M√âTODOS DE CONFIGURACI√ìN
‚îú‚îÄ‚îÄ 18. FUNCIONES DE RENDERIZADO (STREAMLIT)
‚îî‚îÄ‚îÄ 19. FUNCI√ìN PRINCIPAL DE RENDERIZADO
```

### üîß **Componentes Principales**

#### **1. Clases Base**

```python
# Base para todos los analizadores
class BaseAnalyzer(ABC):
    def __init__(self, config: AnalysisConfig):
        self.config = config
        self.logger = setup_logger()
    
    @abstractmethod
    def analyze(self, chunks: List[Dict]) -> AnalysisResult:
        pass
    
    def _validate_input(self, chunks: List[Dict]) -> bool:
        return bool(chunks and all('content' in chunk for chunk in chunks))
```

#### **2. Gesti√≥n de Cache**

```python
class CacheManager:
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.access_times = {}
        self.max_size = max_size
        self.lock = threading.Lock()
    
    def get(self, key: str) -> Optional[Any]:
        # Implementaci√≥n thread-safe
        pass
    
    def set(self, key: str, value: Any) -> None:
        # Implementaci√≥n con evicci√≥n LRU
        pass
```

#### **3. Clase Principal**

```python
class AdvancedQualitativeAnalyzer:
    def __init__(self, config: Optional[AnalysisConfig] = None):
        self.config = config or AnalysisConfig()
        self.preprocessor = TextPreprocessor()
        self.cache_manager = CacheManager()
        self.concept_extractor = ConceptExtractor(self.config)
        self.theme_analyzer = ThemeAnalyzer(self.config)
        self.sentiment_analyzer = SentimentAnalyzer(self.config)
    
    # M√©todos principales de an√°lisis
    def extract_key_concepts(self, chunks: List[Dict]) -> List[Dict]:
        pass
    
    def extract_advanced_themes(self, chunks: List[Dict]) -> Dict:
        pass
    
    def advanced_sentiment_analysis(self, chunks: List[Dict]) -> Dict:
        pass
```

---

## ü§ù Gu√≠a de Contribuci√≥n

### üîÑ **Flujo de Trabajo**

```bash
# 1. Fork del repositorio
git clone https://github.com/TU-USUARIO/CogniChat.git
cd CogniChat

# 2. Crear rama para feature
git checkout -b feature/nueva-funcionalidad

# 3. Hacer cambios
# ... c√≥digo ...

# 4. Ejecutar tests
pytest tests/

# 5. Verificar formato
black modules/qualitative_analysis.py
flake8 modules/qualitative_analysis.py

# 6. Commit
git add .
git commit -m "feat: agregar nueva funcionalidad"

# 7. Push
git push origin feature/nueva-funcionalidad

# 8. Crear Pull Request
```

### üìù **Est√°ndares de C√≥digo**

#### **Formato de C√≥digo**

```python
# Usar Black para formateo
black modules/qualitative_analysis.py

# Verificar con flake8
flake8 modules/qualitative_analysis.py

# Verificar tipos con mypy
mypy modules/qualitative_analysis.py
```

#### **Convenciones de Nomenclatura**

```python
# Clases: PascalCase
class AdvancedQualitativeAnalyzer:
    pass

# M√©todos y variables: snake_case
def extract_key_concepts(self, chunks: List[Dict]) -> List[Dict]:
    pass

# Constantes: UPPER_SNAKE_CASE
MAX_CONCEPTS = 50
DEFAULT_CONFIG = AnalysisConfig()

# Enums: PascalCase
class AnalysisType(Enum):
    CONCEPT_EXTRACTION = "concept_extraction"
    THEME_ANALYSIS = "theme_analysis"
```

#### **Documentaci√≥n**

```python
def extract_key_concepts(self, chunks: List[Dict], min_freq: int = 2) -> List[Dict]:
    """
    Extrae conceptos clave de los chunks de texto.
    
    Args:
        chunks: Lista de chunks de texto con contenido y metadatos
        min_freq: Frecuencia m√≠nima para considerar un concepto
        
    Returns:
        Lista de conceptos con score, frecuencia y contexto
        
    Raises:
        ValueError: Si chunks est√° vac√≠o o mal formateado
        
    Example:
        >>> chunks = [{'content': 'Texto...', 'metadata': {...}}]
        >>> concepts = analyzer.extract_key_concepts(chunks)
        >>> print(f"Conceptos encontrados: {len(concepts)}")
    """
    pass
```

### üß™ **Testing**

#### **Estructura de Tests**

```python
# tests/test_qualitative_analysis.py
import pytest
from modules.qualitative_analysis import AdvancedQualitativeAnalyzer, AnalysisConfig

class TestAdvancedQualitativeAnalyzer:
    def setup_method(self):
        self.config = AnalysisConfig(min_frequency=1, max_concepts=10)
        self.analyzer = AdvancedQualitativeAnalyzer(self.config)
        self.sample_chunks = [
            {
                'content': 'La inteligencia artificial est√° transformando la educaci√≥n.',
                'metadata': {'source_file': 'test.pdf'}
            },
            {
                'content': 'Machine learning permite personalizar el aprendizaje.',
                'metadata': {'source_file': 'test.pdf'}
            }
        ]
    
    def test_extract_key_concepts(self):
        """Test extracci√≥n de conceptos b√°sica"""
        concepts = self.analyzer.extract_key_concepts(self.sample_chunks)
        
        assert len(concepts) > 0
        assert all('concept' in concept for concept in concepts)
        assert all('score' in concept for concept in concepts)
        assert all('frequency' in concept for concept in concepts)
    
    def test_extract_key_concepts_empty_input(self):
        """Test con entrada vac√≠a"""
        concepts = self.analyzer.extract_key_concepts([])
        assert concepts == []
    
    def test_extract_key_concepts_invalid_input(self):
        """Test con entrada inv√°lida"""
        with pytest.raises(ValueError):
            self.analyzer.extract_key_concepts([{'invalid': 'data'}])
    
    def test_cache_functionality(self):
        """Test funcionalidad de cache"""
        # Primera ejecuci√≥n
        concepts1 = self.analyzer.extract_key_concepts(self.sample_chunks)
        
        # Segunda ejecuci√≥n (debe usar cache)
        concepts2 = self.analyzer.extract_key_concepts(self.sample_chunks)
        
        assert concepts1 == concepts2
        assert len(self.analyzer.cache_manager.cache) > 0
```

#### **Ejecutar Tests**

```bash
# Ejecutar todos los tests
pytest

# Ejecutar tests espec√≠ficos
pytest tests/test_qualitative_analysis.py::TestAdvancedQualitativeAnalyzer::test_extract_key_concepts

# Ejecutar con cobertura
pytest --cov=modules.qualitative_analysis --cov-report=html

# Ejecutar tests de rendimiento
pytest tests/test_performance.py -v
```

---

## üé® Patrones de Dise√±o

### üè≠ **Patr√≥n Factory**

```python
class AnalyzerFactory:
    @staticmethod
    def create_analyzer(analyzer_type: str, config: AnalysisConfig) -> BaseAnalyzer:
        """Factory para crear analizadores espec√≠ficos"""
        analyzers = {
            'concept': ConceptExtractor,
            'theme': ThemeAnalyzer,
            'sentiment': SentimentAnalyzer,
            'clustering': ClusteringAnalyzer
        }
        
        if analyzer_type not in analyzers:
            raise ValueError(f"Tipo de analizador no soportado: {analyzer_type}")
        
        return analyzers[analyzer_type](config)

# Uso
config = AnalysisConfig()
concept_analyzer = AnalyzerFactory.create_analyzer('concept', config)
```

### üéØ **Patr√≥n Strategy**

```python
class ConceptExtractionStrategy(ABC):
    @abstractmethod
    def extract(self, texts: List[str]) -> List[Dict]:
        pass

class TFIDFStrategy(ConceptExtractionStrategy):
    def extract(self, texts: List[str]) -> List[Dict]:
        # Implementaci√≥n TF-IDF
        pass

class FrequencyStrategy(ConceptExtractionStrategy):
    def extract(self, texts: List[str]) -> List[Dict]:
        # Implementaci√≥n por frecuencia
        pass

class ConceptExtractor:
    def __init__(self):
        self.strategies = {
            'tfidf': TFIDFStrategy(),
            'frequency': FrequencyStrategy()
        }
    
    def extract_concepts(self, texts: List[str], strategy: str = 'tfidf'):
        return self.strategies[strategy].extract(texts)
```

### üëÅÔ∏è **Patr√≥n Observer**

```python
class AnalysisObserver(ABC):
    @abstractmethod
    def update(self, event: str, data: Any) -> None:
        pass

class MetricsObserver(AnalysisObserver):
    def update(self, event: str, data: Any) -> None:
        if event == 'analysis_completed':
            self.record_metrics(data)

class CacheObserver(AnalysisObserver):
    def update(self, event: str, data: Any) -> None:
        if event == 'analysis_completed':
            self.cache_results(data)

class AdvancedQualitativeAnalyzer:
    def __init__(self):
        self.observers = []
    
    def add_observer(self, observer: AnalysisObserver):
        self.observers.append(observer)
    
    def notify_observers(self, event: str, data: Any):
        for observer in self.observers:
            observer.update(event, data)
```

---

## üêõ Debugging

### üîç **Herramientas de Debug**

#### **Logging Configurado**

```python
import logging

# Configurar logging
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/qualitative_analysis.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# Uso en el c√≥digo
def extract_key_concepts(self, chunks: List[Dict]) -> List[Dict]:
    logger.info(f"Iniciando extracci√≥n de conceptos para {len(chunks)} chunks")
    
    try:
        # ... c√≥digo ...
        logger.debug(f"Conceptos extra√≠dos: {len(concepts)}")
        return concepts
    except Exception as e:
        logger.error(f"Error en extracci√≥n de conceptos: {e}")
        raise
```

#### **Debugging con PDB**

```python
import pdb

def extract_key_concepts(self, chunks: List[Dict]) -> List[Dict]:
    # ... c√≥digo ...
    
    # Punto de quiebre para debugging
    pdb.set_trace()
    
    # ... m√°s c√≥digo ...
```

#### **Debugging con Streamlit**

```python
def render_debug_info(self, chunks: List[Dict]):
    """Renderizar informaci√≥n de debug en Streamlit"""
    if st.checkbox("Mostrar informaci√≥n de debug"):
        st.subheader("üîç Informaci√≥n de Debug")
        
        # Informaci√≥n del cache
        cache_stats = self.cache_manager.get_stats()
        st.write(f"**Cache hits:** {cache_stats['hits']}")
        st.write(f"**Cache misses:** {cache_stats['misses']}")
        
        # Informaci√≥n de chunks
        st.write(f"**Total chunks:** {len(chunks)}")
        st.write(f"**Chunks con contenido:** {len([c for c in chunks if c.get('content')])}")
        
        # Informaci√≥n de configuraci√≥n
        st.write(f"**Configuraci√≥n:** {self.config.__dict__}")
```

### üîß **Herramientas de Profiling**

#### **Profiling de Rendimiento**

```python
import cProfile
import pstats

def profile_analysis():
    """Profilar el rendimiento del an√°lisis"""
    analyzer = AdvancedQualitativeAnalyzer()
    chunks = load_test_chunks()
    
    # Crear profiler
    profiler = cProfile.Profile()
    profiler.enable()
    
    # Ejecutar an√°lisis
    concepts = analyzer.extract_key_concepts(chunks)
    
    profiler.disable()
    
    # Mostrar estad√≠sticas
    stats = pstats.Stats(profiler)
    stats.sort_stats('cumulative')
    stats.print_stats(20)  # Top 20 funciones m√°s lentas
```

#### **Monitoreo de Memoria**

```python
import tracemalloc

def monitor_memory():
    """Monitorear uso de memoria"""
    tracemalloc.start()
    
    analyzer = AdvancedQualitativeAnalyzer()
    chunks = load_large_chunks()
    
    # Ejecutar an√°lisis
    concepts = analyzer.extract_key_concepts(chunks)
    
    # Obtener estad√≠sticas de memoria
    current, peak = tracemalloc.get_traced_memory()
    print(f"Memoria actual: {current / 1024 / 1024:.1f} MB")
    print(f"Memoria pico: {peak / 1024 / 1024:.1f} MB")
    
    tracemalloc.stop()
```

---

## ‚ö° Optimizaci√≥n

### üöÄ **Optimizaciones de Rendimiento**

#### **Cache Inteligente**

```python
class SmartCache:
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.access_times = {}
        self.access_counts = {}
        self.max_size = max_size
    
    def get(self, key: str) -> Optional[Any]:
        if key in self.cache:
            self.access_times[key] = time.time()
            self.access_counts[key] = self.access_counts.get(key, 0) + 1
            return self.cache[key]
        return None
    
    def set(self, key: str, value: Any) -> None:
        if len(self.cache) >= self.max_size:
            self._evict_least_valuable()
        
        self.cache[key] = value
        self.access_times[key] = time.time()
        self.access_counts[key] = 1
    
    def _evict_least_valuable(self):
        """Evict basado en valor (frecuencia * recencia)"""
        if not self.cache:
            return
        
        current_time = time.time()
        least_valuable_key = min(
            self.cache.keys(),
            key=lambda k: self.access_counts.get(k, 1) * (current_time - self.access_times.get(k, 0))
        )
        
        del self.cache[least_valuable_key]
        del self.access_times[least_valuable_key]
        del self.access_counts[least_valuable_key]
```

#### **Procesamiento en Lotes**

```python
class BatchProcessor:
    def __init__(self, batch_size: int = 100):
        self.batch_size = batch_size
    
    def process_in_batches(self, chunks: List[Dict], processor_func) -> List[Any]:
        """Procesar chunks en lotes para optimizar memoria"""
        results = []
        
        for i in range(0, len(chunks), self.batch_size):
            batch = chunks[i:i + self.batch_size]
            batch_results = processor_func(batch)
            results.extend(batch_results)
            
            # Liberar memoria del lote
            del batch
        
        return results
```

#### **Vectorizaci√≥n con NumPy**

```python
import numpy as np
from scipy.sparse import csr_matrix

def vectorize_texts_efficiently(texts: List[str]) -> csr_matrix:
    """Vectorizaci√≥n eficiente usando sparse matrices"""
    # Usar TfidfVectorizer con sparse=True
    vectorizer = TfidfVectorizer(
        max_features=1000,
        sparse=True,  # Usar matrices sparse
        ngram_range=(1, 2)
    )
    
    # Vectorizar
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    # Convertir a CSR para operaciones eficientes
    return tfidf_matrix.tocsr()
```

### üîß **Optimizaciones de Memoria**

#### **Generadores para Datos Grandes**

```python
def process_chunks_generator(chunks: List[Dict], batch_size: int = 100):
    """Generador para procesar chunks grandes sin cargar todo en memoria"""
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i + batch_size]
        yield batch

def analyze_large_dataset(chunks: List[Dict]):
    """Analizar dataset grande usando generadores"""
    analyzer = AdvancedQualitativeAnalyzer()
    all_concepts = []
    
    for batch in process_chunks_generator(chunks):
        batch_concepts = analyzer.extract_key_concepts(batch)
        all_concepts.extend(batch_concepts)
        
        # Liberar memoria del batch
        del batch
    
    return all_concepts
```

#### **Compresi√≥n de Datos**

```python
import pickle
import gzip

class CompressedCache:
    def __init__(self, max_size: int = 1000):
        self.cache = {}
        self.max_size = max_size
    
    def get(self, key: str) -> Optional[Any]:
        if key in self.cache:
            compressed_data = self.cache[key]
            return pickle.loads(gzip.decompress(compressed_data))
        return None
    
    def set(self, key: str, value: Any) -> None:
        if len(self.cache) >= self.max_size:
            self._evict_oldest()
        
        compressed_data = gzip.compress(pickle.dumps(value))
        self.cache[key] = compressed_data
```

---

## üîå Extensibilidad

### üéØ **Crear Nuevos Analizadores**

```python
class CustomAnalyzer(BaseAnalyzer):
    """Ejemplo de analizador personalizado"""
    
    def __init__(self, config: AnalysisConfig):
        super().__init__(config)
        self.custom_param = config.custom_settings.get('custom_param', 'default')
    
    def analyze(self, chunks: List[Dict]) -> AnalysisResult:
        """Implementar an√°lisis personalizado"""
        if not self._validate_input(chunks):
            return AnalysisResult(
                analysis_type=AnalysisType.CUSTOM,
                data={'error': 'Invalid input'},
                metadata={'error': 'Input validation failed'}
            )
        
        try:
            # Implementar l√≥gica personalizada
            results = self._custom_analysis(chunks)
            
            return AnalysisResult(
                analysis_type=AnalysisType.CUSTOM,
                data={'results': results},
                metadata={'total_items': len(results)}
            )
            
        except Exception as e:
            self.logger.error(f"Error en an√°lisis personalizado: {e}")
            return AnalysisResult(
                analysis_type=AnalysisType.CUSTOM,
                data={'error': str(e)},
                metadata={'error': str(e)}
            )
    
    def _custom_analysis(self, chunks: List[Dict]) -> List[Dict]:
        """L√≥gica espec√≠fica del an√°lisis personalizado"""
        # Implementar aqu√≠ la l√≥gica espec√≠fica
        pass

# Registrar el nuevo tipo de an√°lisis
AnalysisType.CUSTOM = "custom"
```

### üé® **Crear Nuevas Visualizaciones**

```python
def render_custom_visualization(analyzer: AdvancedQualitativeAnalyzer, chunks: List[Dict]):
    """Ejemplo de visualizaci√≥n personalizada"""
    st.header("üé® Visualizaci√≥n Personalizada")
    
    # Obtener datos
    concepts = analyzer.extract_key_concepts(chunks)
    
    # Crear visualizaci√≥n personalizada
    fig = go.Figure()
    
    # Agregar datos
    concept_names = [c['concept'] for c in concepts[:10]]
    concept_scores = [c['score'] for c in concepts[:10]]
    
    fig.add_trace(go.Bar(
        x=concept_names,
        y=concept_scores,
        marker_color='lightblue'
    ))
    
    fig.update_layout(
        title="Top 10 Conceptos",
        xaxis_title="Conceptos",
        yaxis_title="Score"
    )
    
    st.plotly_chart(fig, use_container_width=True)
```

### üîå **Crear Plugins**

```python
class AnalysisPlugin:
    """Base para plugins de an√°lisis"""
    
    def __init__(self, name: str, version: str):
        self.name = name
        self.version = version
    
    def initialize(self, analyzer: AdvancedQualitativeAnalyzer):
        """Inicializar plugin con el analizador"""
        pass
    
    def process(self, data: Any) -> Any:
        """Procesar datos"""
        pass
    
    def cleanup(self):
        """Limpiar recursos del plugin"""
        pass

class SentimentPlugin(AnalysisPlugin):
    """Plugin de an√°lisis de sentimientos avanzado"""
    
    def __init__(self):
        super().__init__("Advanced Sentiment", "1.0.0")
    
    def process(self, chunks: List[Dict]) -> Dict:
        """Procesar an√°lisis de sentimientos"""
        # Implementar an√°lisis avanzado
        pass

# Registrar plugin
plugin_manager = PluginManager()
plugin_manager.register_plugin(SentimentPlugin())
```

---

## üìä M√©tricas y Monitoreo

### üìà **Sistema de M√©tricas**

```python
class MetricsCollector:
    def __init__(self):
        self.metrics = {
            'total_analyses': 0,
            'total_processing_time': 0.0,
            'total_concepts': 0,
            'cache_hits': 0,
            'cache_misses': 0,
            'errors': 0
        }
    
    def record_analysis(self, processing_time: float, concepts_count: int, 
                       cache_hit: bool = False, error: bool = False):
        """Registrar m√©tricas de an√°lisis"""
        self.metrics['total_analyses'] += 1
        self.metrics['total_processing_time'] += processing_time
        self.metrics['total_concepts'] += concepts_count
        
        if cache_hit:
            self.metrics['cache_hits'] += 1
        else:
            self.metrics['cache_misses'] += 1
        
        if error:
            self.metrics['errors'] += 1
    
    def get_average_metrics(self) -> Dict[str, float]:
        """Obtener m√©tricas promedio"""
        if self.metrics['total_analyses'] == 0:
            return {}
        
        return {
            'avg_processing_time': self.metrics['total_processing_time'] / self.metrics['total_analyses'],
            'avg_concepts_per_analysis': self.metrics['total_concepts'] / self.metrics['total_analyses'],
            'cache_hit_rate': self.metrics['cache_hits'] / (self.metrics['cache_hits'] + self.metrics['cache_misses']),
            'error_rate': self.metrics['errors'] / self.metrics['total_analyses']
        }
```

---

## üéØ Mejores Pr√°cticas

### ‚úÖ **Do's**

- ‚úÖ **Usar type hints** para mejor documentaci√≥n
- ‚úÖ **Implementar logging** detallado
- ‚úÖ **Escribir tests** para nuevas funcionalidades
- ‚úÖ **Documentar** m√©todos p√∫blicos
- ‚úÖ **Usar cache** para operaciones costosas
- ‚úÖ **Validar entrada** en todos los m√©todos
- ‚úÖ **Manejar errores** graciosamente
- ‚úÖ **Optimizar** para rendimiento

### ‚ùå **Don'ts**

- ‚ùå **No hardcodear** valores m√°gicos
- ‚ùå **No ignorar** errores de validaci√≥n
- ‚ùå **No usar** variables globales
- ‚ùå **No hacer** cambios breaking sin versionado
- ‚ùå **No olvidar** limpiar recursos
- ‚ùå **No usar** memoria excesiva
- ‚ùå **No hacer** commits sin tests
- ‚ùå **No ignorar** feedback de code review

---

## üéâ Conclusi√≥n

Esta gu√≠a proporciona las herramientas y conocimientos necesarios para contribuir efectivamente al m√≥dulo de an√°lisis cualitativo. Recuerda:

### üéØ **Principios Clave**

1. **Calidad**: C√≥digo limpio, bien documentado y testeado
2. **Rendimiento**: Optimizaci√≥n para velocidad y memoria
3. **Mantenibilidad**: C√≥digo modular y extensible
4. **Usabilidad**: Interfaz intuitiva y responsive
5. **Escalabilidad**: Arquitectura que crece con el proyecto

### üöÄ **Pr√≥ximos Pasos**

1. **Explora** el c√≥digo existente
2. **Identifica** √°reas de mejora
3. **Prop√≥n** nuevas funcionalidades
4. **Contribuye** con c√≥digo de calidad
5. **Mant√©n** la documentaci√≥n actualizada

---

**¬°Feliz coding!** üöÄüë®‚Äçüíª

*"Construyendo el futuro del an√°lisis cualitativo, una l√≠nea de c√≥digo a la vez."*
