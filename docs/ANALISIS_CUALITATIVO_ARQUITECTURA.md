# üìö Arquitectura del M√≥dulo de An√°lisis Cualitativo

## üìã √çndice

1. [Visi√≥n General](#visi√≥n-general)
2. [Arquitectura del Sistema](#arquitectura-del-sistema)
3. [Componentes Principales](#componentes-principales)
4. [Flujo de Datos](#flujo-de-datos)
5. [An√°lisis Detallado por Secci√≥n](#an√°lisis-detallado-por-secci√≥n)
6. [Mejoras Potenciales](#mejoras-potenciales)
7. [Gu√≠a de Modificaci√≥n](#gu√≠a-de-modificaci√≥n)

---

## üéØ Visi√≥n General

### Prop√≥sito del M√≥dulo

El m√≥dulo de **An√°lisis Cualitativo Avanzado** (`modules/qualitative_analysis.py`) es el componente m√°s complejo del sistema CogniChat. Proporciona an√°lisis profundo de contenido RAG utilizando t√©cnicas de NLP, visualizaciones interactivas y algoritmos de inteligencia artificial.

### Caracter√≠sticas Principales

- üîç **Extracci√≥n de Conceptos**: Identificaci√≥n de conceptos clave usando TF-IDF
- üéØ **An√°lisis de Temas**: Modelado de temas con LDA (Latent Dirichlet Allocation)
- üòä **An√°lisis de Sentimientos**: Evaluaci√≥n emocional con VADER y TextBlob
- üó∫Ô∏è **Mapas Conceptuales**: Visualizaciones interactivas con PyVis
- üß† **Mapas Mentales**: Representaciones jer√°rquicas con streamlit-agraph
- üìù **Res√∫menes Autom√°ticos**: Generaci√≥n con LLM y m√©todos extractivos
- üî∫ **Triangulaci√≥n**: Validaci√≥n multi-fuente de conceptos
- ‚òÅÔ∏è **Nubes de Palabras**: Visualizaciones de frecuencia
- üîç **Clustering**: Agrupaci√≥n de documentos con K-means y DBSCAN
- ‚ö° **Procesamiento Paralelo**: An√°lisis concurrente optimizado

### Estad√≠sticas del C√≥digo

- **Total de l√≠neas**: ~6,100
- **Secciones organizadas**: 19
- **Clases principales**: 6
- **M√©todos p√∫blicos**: ~25
- **M√©todos privados**: ~80
- **Funciones de renderizado**: 12

---

## üèóÔ∏è Arquitectura del Sistema

### Diagrama de Arquitectura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  M√ìDULO DE AN√ÅLISIS CUALITATIVO                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ                    ‚îÇ                    ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ DATOS   ‚îÇ         ‚îÇ AN√ÅLISIS ‚îÇ         ‚îÇ VISUAL. ‚îÇ
    ‚îÇ & CACHE ‚îÇ         ‚îÇ & PROC.  ‚îÇ         ‚îÇ & UI    ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                    ‚îÇ                    ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ              COMPONENTES ESPECIALIZADOS            ‚îÇ
    ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îÇ ‚Ä¢ TextPreprocessor    ‚Ä¢ ConceptExtractor          ‚îÇ
    ‚îÇ ‚Ä¢ ThemeAnalyzer       ‚Ä¢ SentimentAnalyzer         ‚îÇ
    ‚îÇ ‚Ä¢ CacheManager        ‚Ä¢ BaseAnalyzer              ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Patr√≥n de Dise√±o Utilizado

El m√≥dulo implementa varios patrones de dise√±o:

1. **Strategy Pattern**: Diferentes estrategias de an√°lisis (TF-IDF, LDA, VADER)
2. **Factory Pattern**: Creaci√≥n de analizadores especializados
3. **Singleton Pattern**: CacheManager para gesti√≥n de memoria
4. **Template Method Pattern**: BaseAnalyzer con m√©todo `analyze()` abstracto
5. **Observer Pattern**: Sistema de m√©tricas y monitoreo

---

## üß© Componentes Principales

### 1. Estructuras de Datos (L√≠neas 145-260)

#### AnalysisType (Enum)
Define los tipos de an√°lisis disponibles:
```python
class AnalysisType(Enum):
    CONCEPT_EXTRACTION = "concept_extraction"
    THEME_ANALYSIS = "theme_analysis"
    SENTIMENT_ANALYSIS = "sentiment_analysis"
    CLUSTERING = "clustering"
    CONCEPT_MAP = "concept_map"
    MIND_MAP = "mind_map"
    SUMMARY = "summary"
    TRIANGULATION = "triangulation"
```

#### AnalysisConfig (Dataclass)
Configuraci√≥n centralizada:
```python
@dataclass
class AnalysisConfig:
    min_frequency: int = 2
    max_concepts: int = 50
    similarity_threshold: float = 0.6
    chunk_size: int = 2000
    enable_cache: bool = True
    parallel_processing: bool = True
    max_workers: int = 4
```

#### ConceptData (Dataclass)
Estructura para conceptos extra√≠dos:
```python
@dataclass
class ConceptData:
    concept: str
    score: float
    frequency: int
    context: List[str]
    related_concepts: List[str]
    sentiment: Optional[float]
    category: Optional[str]
```

### 2. Clases Base (L√≠neas 260-350)

#### BaseAnalyzer (ABC)
Clase abstracta base para todos los analizadores:
```python
class BaseAnalyzer(ABC):
    def __init__(self, config: AnalysisConfig)
    
    @abstractmethod
    def analyze(self, chunks: List[Dict]) -> AnalysisResult
    
    def _validate_input(self, chunks: List[Dict]) -> bool
```

**Prop√≥sito**: Garantizar interfaz consistente entre todos los analizadores.

#### CacheManager
Gestor de cache con LRU eviction:
```python
class CacheManager:
    def get(self, key: str) -> Optional[Any]
    def set(self, key: str, value: Any) -> None
    def clear(self) -> None
    def _evict_oldest(self) -> None
    def get_stats(self) -> Dict[str, Any]
```

**Caracter√≠sticas**:
- Thread-safe con `threading.Lock()`
- Eviction autom√°tico de elementos antiguos
- Estad√≠sticas de uso en tiempo real

### 3. Preprocesamiento (L√≠neas 350-395)

#### TextPreprocessor
Preprocesamiento especializado de texto:
```python
class TextPreprocessor:
    def get_spanish_stopwords(self) -> List[str]
    def preprocess_text(self, text: str) -> str
```

**Funcionalidades**:
- Stopwords en espa√±ol con NLTK
- Normalizaci√≥n de texto
- Eliminaci√≥n de caracteres especiales
- Cache de stopwords

### 4. Analizadores Especializados (L√≠neas 395-900)

#### ConceptExtractor
Extracci√≥n de conceptos clave:
```python
class ConceptExtractor(BaseAnalyzer):
    def analyze(self, chunks) ‚Üí AnalysisResult
    def _extract_with_tfidf(self, texts) ‚Üí List[ConceptData]
    def _extract_with_frequency(self, texts) ‚Üí List[ConceptData]
    def _enrich_concepts_with_context(self, concepts, chunks)
```

**Algoritmos Usados**:
- **TF-IDF** (primario): Identifica t√©rminos importantes
- **Frecuencia** (fallback): Conteo simple de palabras

#### ThemeAnalyzer
An√°lisis de temas principal:
```python
class ThemeAnalyzer(BaseAnalyzer):
    def analyze(self, chunks) ‚Üí AnalysisResult
    def _extract_themes_with_lda(self, texts) ‚Üí List[Dict]
    def _extract_themes_with_clustering(self, texts) ‚Üí List[Dict]
```

**Algoritmos Usados**:
- **LDA** (Latent Dirichlet Allocation): Modelado de temas
- **K-means clustering**: Agrupaci√≥n de palabras clave

#### SentimentAnalyzer
An√°lisis de sentimientos:
```python
class SentimentAnalyzer(BaseAnalyzer):
    def analyze(self, chunks) ‚Üí AnalysisResult
    def _analyze_with_textblob_vader(self, chunks) ‚Üí Dict
    def _analyze_basic_sentiment(self, chunks) ‚Üí Dict
```

**Algoritmos Usados**:
- **VADER** (primario): An√°lisis de sentimientos espec√≠fico para espa√±ol
- **TextBlob** (secundario): An√°lisis de polaridad y subjetividad
- **Conteo de palabras** (fallback): M√©todo b√°sico

### 5. Clase Principal (L√≠neas 900-3700)

#### AdvancedQualitativeAnalyzer
Orquestador principal que coordina todos los an√°lisis:

**M√©todos de An√°lisis**:
```python
# Extracci√≥n de conceptos
extract_key_concepts(chunks, min_freq) ‚Üí List[Dict]

# An√°lisis de temas
extract_advanced_themes(chunks, n_topics) ‚Üí Dict

# An√°lisis de sentimientos
advanced_sentiment_analysis(chunks) ‚Üí Dict

# Clustering
perform_clustering(chunks, n_clusters) ‚Üí Dict

# Triangulaci√≥n
perform_triangulation_analysis(chunks) ‚Üí Dict

# Mapas conceptuales
create_interactive_concept_map(chunks, layout_type) ‚Üí Optional[str]

# Mapas mentales
create_interactive_mind_map(chunks, node_spacing) ‚Üí Optional[Dict]

# Res√∫menes
generate_intelligent_summary(chunks, summary_type) ‚Üí Dict
generate_rag_summary(chunks, max_length) ‚Üí str
generate_basic_summary(chunks, max_sentences) ‚Üí str

# Nubes de palabras
generate_word_cloud(chunks, source_filter) ‚Üí Optional[str]
```

**M√©todos de Optimizaci√≥n**:
```python
# An√°lisis paralelo
perform_parallel_analysis(chunks, analysis_types) ‚Üí Dict[str, AnalysisResult]

# Optimizaci√≥n de rendimiento
optimize_performance() ‚Üí Dict[str, Any]
get_performance_metrics() ‚Üí Dict[str, Any]

# Gesti√≥n de cache
clear_cache() ‚Üí None
get_cache_stats() ‚Üí Dict
```

---

## üîÑ Flujo de Datos

### Flujo General de An√°lisis

```
1. ENTRADA DE DATOS
   ‚îú‚îÄ Chunks del sistema RAG
   ‚îî‚îÄ Configuraci√≥n de an√°lisis

2. VALIDACI√ìN
   ‚îú‚îÄ Verificar chunks v√°lidos
   ‚îî‚îÄ Verificar configuraci√≥n

3. PREPROCESAMIENTO
   ‚îú‚îÄ Limpiar texto
   ‚îú‚îÄ Remover stopwords
   ‚îî‚îÄ Normalizar

4. AN√ÅLISIS
   ‚îú‚îÄ Extracci√≥n de conceptos (TF-IDF)
   ‚îú‚îÄ An√°lisis de temas (LDA)
   ‚îú‚îÄ An√°lisis de sentimientos (VADER)
   ‚îî‚îÄ Clustering (K-means)

5. ENRIQUECIMIENTO
   ‚îú‚îÄ Agregar contexto
   ‚îú‚îÄ Encontrar relaciones
   ‚îî‚îÄ Calcular m√©tricas

6. VISUALIZACI√ìN
   ‚îú‚îÄ Generar mapas conceptuales
   ‚îú‚îÄ Crear gr√°ficos interactivos
   ‚îî‚îÄ Preparar dashboards

7. SALIDA
   ‚îú‚îÄ AnalysisResult estructurado
   ‚îî‚îÄ Formato legacy compatible
```

### Flujo de Cache

```
CONSULTA
   ‚îú‚îÄ Generar cache_key (hash de contenido)
   ‚îú‚îÄ Verificar en CacheManager
   ‚îÇ  ‚îú‚îÄ HIT ‚Üí Retornar resultado cacheado
   ‚îÇ  ‚îî‚îÄ MISS ‚Üí Procesar y guardar
   ‚îî‚îÄ Actualizar access_times
```

---

## üìä An√°lisis Detallado por Secci√≥n

### Secci√≥n 6: EXTRACCI√ìN DE CONCEPTOS (L√≠nea 395)

#### Algoritmos Implementados

**1. TF-IDF (Term Frequency-Inverse Document Frequency)**
```python
Ventajas:
‚úÖ Identifica t√©rminos importantes espec√≠ficos del corpus
‚úÖ Reduce peso de palabras comunes
‚úÖ Funciona bien con documentos t√©cnicos

Limitaciones:
‚ùå No captura relaciones sem√°nticas
‚ùå Sensible al tama√±o del corpus
‚ùå Requiere m√∫ltiples documentos

Par√°metros configurables:
- max_features: N√∫mero m√°ximo de t√©rminos (default: 200)
- ngram_range: Rango de n-gramas (default: 1-3)
- min_df: Frecuencia m√≠nima de documento
- max_df: Frecuencia m√°xima de documento
```

**2. An√°lisis de Frecuencia (Fallback)**
```python
Ventajas:
‚úÖ Simple y r√°pido
‚úÖ No requiere m√∫ltiples documentos
‚úÖ F√°cil de entender

Limitaciones:
‚ùå No considera importancia relativa
‚ùå Puede sobrevalorar palabras comunes
‚ùå No detecta conceptos complejos

Mejoras sugeridas:
‚Üí Implementar BM25 para mejor ranking
‚Üí Usar Mutual Information para co-ocurrencias
‚Üí Agregar detecci√≥n de entidades nombradas (NER)
```

#### Mejoras Potenciales

**Algoritmos Tradicionales a Agregar**:

1. **BM25 (Best Matching 25)**
```python
from rank_bm25 import BM25Okapi

def _extract_with_bm25(self, texts: List[str]) -> List[ConceptData]:
    """Extraer conceptos usando BM25"""
    tokenized_texts = [text.split() for text in texts]
    bm25 = BM25Okapi(tokenized_texts)
    
    # Obtener palabras √∫nicas
    all_words = set()
    for text in tokenized_texts:
        all_words.update(text)
    
    # Ranking de palabras
    concepts = []
    for word in all_words:
        scores = bm25.get_scores([word])
        avg_score = np.mean(scores)
        if avg_score > threshold:
            concepts.append(ConceptData(
                concept=word,
                score=avg_score,
                frequency=sum(1 for text in tokenized_texts if word in text)
            ))
    
    return sorted(concepts, key=lambda x: x.score, reverse=True)
```

2. **PMI (Pointwise Mutual Information)**
```python
def _extract_collocations_with_pmi(self, texts: List[str]) -> List[ConceptData]:
    """Extraer colocaciones importantes usando PMI"""
    from nltk.collocations import BigramAssocMeasures, BigramCollocationFinder
    
    finder = BigramCollocationFinder.from_documents(
        [text.split() for text in texts]
    )
    
    bigram_measures = BigramAssocMeasures()
    scored_bigrams = finder.score_ngrams(bigram_measures.pmi)
    
    concepts = []
    for (word1, word2), score in scored_bigrams[:50]:
        concept = ConceptData(
            concept=f"{word1} {word2}",
            score=score,
            frequency=finder.ngram_fd[(word1, word2)]
        )
        concepts.append(concept)
    
    return concepts
```

**Algoritmos con IA a Agregar**:

1. **BERT para Extracci√≥n de Conceptos**
```python
from transformers import pipeline

def _extract_with_bert_ner(self, texts: List[str]) -> List[ConceptData]:
    """Extraer entidades nombradas usando BERT"""
    ner_pipeline = pipeline(
        "ner",
        model="dccuchile/bert-base-spanish-wwm-uncased",
        aggregation_strategy="simple"
    )
    
    concepts = []
    for text in texts:
        entities = ner_pipeline(text)
        for entity in entities:
            concept = ConceptData(
                concept=entity['word'],
                score=entity['score'],
                frequency=1,
                category=entity['entity_group']
            )
            concepts.append(concept)
    
    return concepts
```

2. **Embeddings Sem√°nticos con Sentence-BERT**
```python
from sentence_transformers import SentenceTransformer

def _extract_with_semantic_clustering(self, texts: List[str]) -> List[ConceptData]:
    """Extraer conceptos usando embeddings sem√°nticos"""
    model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    # Generar embeddings
    embeddings = model.encode(texts)
    
    # Clustering sem√°ntico
    from sklearn.cluster import DBSCAN
    clustering = DBSCAN(eps=0.5, min_samples=2, metric='cosine')
    labels = clustering.fit_predict(embeddings)
    
    # Extraer conceptos representativos de cada cluster
    concepts = []
    for label in set(labels):
        if label == -1:  # Ruido
            continue
        
        cluster_texts = [texts[i] for i, l in enumerate(labels) if l == label]
        # Extraer concepto representativo
        representative = cluster_texts[0][:50]
        
        concept = ConceptData(
            concept=representative,
            score=len(cluster_texts) / len(texts),
            frequency=len(cluster_texts),
            category=f"cluster_{label}"
        )
        concepts.append(concept)
    
    return concepts
```

---

### Secci√≥n 7: AN√ÅLISIS DE TEMAS (L√≠nea 533)

#### Algoritmos Implementados

**1. LDA (Latent Dirichlet Allocation)**
```python
Ventajas:
‚úÖ Descubre temas latentes autom√°ticamente
‚úÖ Asigna documentos a m√∫ltiples temas
‚úÖ Interpretable y bien establecido

Limitaciones:
‚ùå Requiere especificar n√∫mero de temas
‚ùå Sensible a par√°metros de inicializaci√≥n
‚ùå Computacionalmente costoso

Par√°metros clave:
- n_components: N√∫mero de temas
- max_iter: Iteraciones m√°ximas (default: 100)
- learning_method: 'batch' o 'online'
- random_state: Semilla para reproducibilidad
```

**2. Clustering de Palabras Clave (Fallback)**
```python
Ventajas:
‚úÖ R√°pido y eficiente
‚úÖ No requiere muchos par√°metros
‚úÖ Funciona con corpus peque√±os

Limitaciones:
‚ùå Menos sofisticado que LDA
‚ùå Puede no capturar relaciones sutiles
‚ùå Depende de frecuencia de palabras
```

#### Mejoras Potenciales

**Algoritmos Tradicionales a Agregar**:

1. **NMF (Non-Negative Matrix Factorization)**
```python
from sklearn.decomposition import NMF

def _extract_themes_with_nmf(self, texts: List[str], n_topics: int) -> List[Dict]:
    """Extraer temas usando NMF (m√°s interpretable que LDA)"""
    vectorizer = TfidfVectorizer(
        max_features=1000,
        stop_words=self.preprocessor.get_spanish_stopwords()
    )
    
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    nmf = NMF(
        n_components=n_topics,
        random_state=42,
        init='nndsvda',  # Inicializaci√≥n mejorada
        solver='mu',      # Multiplicative Update
        max_iter=400
    )
    
    W = nmf.fit_transform(tfidf_matrix)  # Matriz documento-tema
    H = nmf.components_                   # Matriz tema-palabra
    
    feature_names = vectorizer.get_feature_names_out()
    
    themes = []
    for topic_idx, topic in enumerate(H):
        top_indices = topic.argsort()[-10:][::-1]
        keywords = [feature_names[i] for i in top_indices]
        weights = [topic[i] for i in top_indices]
        
        theme = {
            'id': topic_idx,
            'name': f"Tema {topic_idx + 1}",
            'keywords': keywords,
            'weights': weights,
            'coherence': self._calculate_topic_coherence(keywords, texts),
            'description': self._generate_theme_description(keywords)
        }
        themes.append(theme)
    
    return themes
```

2. **LSA (Latent Semantic Analysis)**
```python
from sklearn.decomposition import TruncatedSVD

def _extract_themes_with_lsa(self, texts: List[str], n_topics: int) -> List[Dict]:
    """Extraer temas usando LSA/LSI"""
    vectorizer = TfidfVectorizer(
        max_features=1000,
        stop_words=self.preprocessor.get_spanish_stopwords()
    )
    
    tfidf_matrix = vectorizer.fit_transform(texts)
    
    svd = TruncatedSVD(n_components=n_topics, random_state=42)
    svd.fit(tfidf_matrix)
    
    feature_names = vectorizer.get_feature_names_out()
    
    themes = []
    for topic_idx, topic in enumerate(svd.components_):
        top_indices = topic.argsort()[-10:][::-1]
        keywords = [feature_names[i] for i in top_indices]
        
        theme = {
            'id': topic_idx,
            'name': f"Tema {topic_idx + 1}",
            'keywords': keywords,
            'weights': topic[top_indices].tolist(),
            'variance_explained': svd.explained_variance_ratio_[topic_idx]
        }
        themes.append(theme)
    
    return themes
```

**Algoritmos con IA a Agregar**:

1. **BERTopic**
```python
from bertopic import BERTopic
from sentence_transformers import SentenceTransformer

def _extract_themes_with_bertopic(self, texts: List[str]) -> List[Dict]:
    """Extraer temas usando BERTopic (estado del arte)"""
    # Modelo de embeddings
    embedding_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
    
    # Configurar BERTopic
    topic_model = BERTopic(
        embedding_model=embedding_model,
        language='spanish',
        calculate_probabilities=True,
        verbose=False
    )
    
    # Extraer temas
    topics, probs = topic_model.fit_transform(texts)
    
    # Convertir a formato esperado
    theme_list = []
    for topic_id in set(topics):
        if topic_id == -1:  # Outliers
            continue
        
        topic_words = topic_model.get_topic(topic_id)
        if topic_words:
            keywords = [word for word, score in topic_words]
            weights = [score for word, score in topic_words]
            
            theme = {
                'id': topic_id,
                'name': topic_model.get_topic_info()[topic_id]['Name'],
                'keywords': keywords,
                'weights': weights,
                'documents_count': sum(1 for t in topics if t == topic_id),
                'representative_docs': topic_model.get_representative_docs(topic_id)
            }
            theme_list.append(theme)
    
    return theme_list
```

2. **Top2Vec**
```python
from top2vec import Top2Vec

def _extract_themes_with_top2vec(self, texts: List[str]) -> List[Dict]:
    """Extraer temas usando Top2Vec"""
    model = Top2Vec(
        documents=texts,
        embedding_model='universal-sentence-encoder-multilingual',
        speed='learn',
        workers=4
    )
    
    topic_words, word_scores, topic_nums = model.get_topics()
    
    themes = []
    for i in range(len(topic_nums)):
        theme = {
            'id': i,
            'name': f"Tema {i + 1}",
            'keywords': topic_words[i][:10].tolist(),
            'weights': word_scores[i][:10].tolist(),
            'size': model.topic_sizes[i],
            'documents': model.get_documents_topics(topic_nums=[i])
        }
        themes.append(theme)
    
    return themes
```

---

### Secci√≥n 8: AN√ÅLISIS DE SENTIMIENTOS (L√≠nea 749)

#### Algoritmos Implementados

**1. VADER (Valence Aware Dictionary and sEntiment Reasoner)**
```python
Ventajas:
‚úÖ Optimizado para redes sociales y texto informal
‚úÖ Maneja intensificadores y negaciones
‚úÖ R√°pido y eficiente

Limitaciones:
‚ùå Basado en l√©xico (no contextual)
‚ùå Puede no capturar iron√≠a
‚ùå Dise√±ado originalmente para ingl√©s

Salida:
{
    'neg': 0.0,      # Probabilidad negativa
    'neu': 0.5,      # Probabilidad neutral
    'pos': 0.5,      # Probabilidad positiva
    'compound': 0.5  # Score compuesto [-1, 1]
}
```

**2. TextBlob**
```python
Ventajas:
‚úÖ Simple de usar
‚úÖ Proporciona polaridad y subjetividad
‚úÖ Multiidioma

Limitaciones:
‚ùå Menos preciso que VADER
‚ùå No maneja bien contexto
‚ùå Basado en diccionario

Salida:
sentiment.polarity: float [-1.0, 1.0]
sentiment.subjectivity: float [0.0, 1.0]
```

#### Mejoras Potenciales

**Algoritmos Tradicionales a Agregar**:

1. **An√°lisis de Aspectos (ABSA)**
```python
def _aspect_based_sentiment_analysis(self, chunks: List[Dict]) -> Dict:
    """An√°lisis de sentimientos basado en aspectos"""
    results = {
        'aspects': {},
        'overall': {'positive': 0, 'negative': 0, 'neutral': 0}
    }
    
    # Definir aspectos a analizar
    aspects = {
        'calidad': ['calidad', 'excelente', 'bueno', 'malo', 'terrible'],
        'servicio': ['servicio', 'atenci√≥n', 'ayuda', 'soporte'],
        'precio': ['precio', 'costo', 'barato', 'caro', 'econ√≥mico']
    }
    
    for chunk in chunks:
        content = chunk.get('content', '').lower()
        
        for aspect_name, keywords in aspects.items():
            if any(keyword in content for keyword in keywords):
                # Analizar sentimiento del aspecto
                blob = TextBlob(content)
                sentiment = blob.sentiment.polarity
                
                if aspect_name not in results['aspects']:
                    results['aspects'][aspect_name] = {
                        'positive': 0, 'negative': 0, 'neutral': 0, 'scores': []
                    }
                
                if sentiment > 0.1:
                    results['aspects'][aspect_name]['positive'] += 1
                elif sentiment < -0.1:
                    results['aspects'][aspect_name]['negative'] += 1
                else:
                    results['aspects'][aspect_name]['neutral'] += 1
                
                results['aspects'][aspect_name]['scores'].append(sentiment)
    
    return results
```

**Algoritmos con IA a Agregar**:

1. **BERT para An√°lisis de Sentimientos**
```python
from transformers import pipeline

def _analyze_sentiment_with_bert(self, chunks: List[Dict]) -> Dict:
    """An√°lisis de sentimientos usando BERT en espa√±ol"""
    sentiment_pipeline = pipeline(
        "sentiment-analysis",
        model="nlptown/bert-base-multilingual-uncased-sentiment",
        device=-1  # CPU
    )
    
    results = {
        'by_chunk': [],
        'overall': {'positive': 0, 'negative': 0, 'neutral': 0},
        'confidence_avg': 0.0
    }
    
    confidences = []
    
    for chunk in chunks:
        content = chunk.get('content', '').strip()
        if not content or len(content) < 10:
            continue
        
        # Limitar longitud para BERT
        content = content[:512]
        
        try:
            result = sentiment_pipeline(content)[0]
            label = result['label']
            score = result['score']
            
            # Mapear estrellas a sentimiento
            if label in ['4 stars', '5 stars']:
                sentiment = 'positive'
            elif label in ['1 star', '2 stars']:
                sentiment = 'negative'
            else:
                sentiment = 'neutral'
            
            results['by_chunk'].append({
                'content': content[:100],
                'sentiment': sentiment,
                'confidence': score,
                'label': label
            })
            
            results['overall'][sentiment] += 1
            confidences.append(score)
            
        except Exception as e:
            logger.warning(f"Error en BERT sentiment: {e}")
    
    results['confidence_avg'] = np.mean(confidences) if confidences else 0.0
    
    return results
```

2. **An√°lisis de Emociones con IA**
```python
def _analyze_emotions_with_ai(self, chunks: List[Dict]) -> Dict:
    """An√°lisis de emociones espec√≠ficas usando IA"""
    # Usar modelo espec√≠fico de emociones
    from transformers import pipeline
    
    emotion_pipeline = pipeline(
        "text-classification",
        model="j-hartmann/emotion-english-distilroberta-base",
        top_k=None
    )
    
    emotions_result = {
        'joy': 0, 'anger': 0, 'sadness': 0,
        'fear': 0, 'surprise': 0, 'disgust': 0,
        'love': 0, 'by_chunk': []
    }
    
    for chunk in chunks:
        content = chunk.get('content', '')[:512]
        
        if not content:
            continue
        
        try:
            emotions = emotion_pipeline(content)[0]
            
            # Encontrar emoci√≥n dominante
            dominant = max(emotions, key=lambda x: x['score'])
            emotions_result[dominant['label']] += 1
            
            chunk_emotions = {
                'content': content[:100],
                'emotions': emotions,
                'dominant': dominant['label']
            }
            emotions_result['by_chunk'].append(chunk_emotions)
            
        except Exception as e:
            logger.warning(f"Error en an√°lisis de emociones: {e}")
    
    return emotions_result
```

---

### Secci√≥n 10: MAPAS CONCEPTUALES (L√≠nea 1977)

#### Bibliotecas Utilizadas

**PyVis**: Visualizaci√≥n interactiva basada en vis.js
```python
Ventajas:
‚úÖ Altamente interactivo
‚úÖ M√∫ltiples layouts disponibles
‚úÖ Personalizaci√≥n completa de nodos y aristas

Limitaciones:
‚ùå Puede ser lento con muchos nodos
‚ùå Requiere HTML/JavaScript
‚ùå No se integra nativamente con Streamlit
```

#### Mejoras Potenciales

**Visualizaciones Alternativas**:

1. **Graphviz para Jerarqu√≠as Est√°ticas**
```python
import graphviz

def _create_hierarchical_concept_map(self, chunks: List[Dict]) -> str:
    """Crear mapa conceptual jer√°rquico con Graphviz"""
    dot = graphviz.Digraph(comment='Mapa Conceptual')
    dot.attr(rankdir='TB')  # Top to Bottom
    dot.attr('node', shape='box', style='rounded,filled', fillcolor='lightblue')
    
    # Analizar jerarqu√≠a
    hierarchy = self._analyze_concept_hierarchy(chunks)
    
    # Nodo principal
    dot.node('main', hierarchy['main_theme']['name'], fillcolor='lightcoral')
    
    # Conceptos principales
    for i, concept in enumerate(hierarchy['main_concepts']):
        node_id = f"concept_{i}"
        dot.node(node_id, concept['name'], fillcolor='lightgreen')
        dot.edge('main', node_id, label='define')
        
        # Sub-conceptos
        for j, sub in enumerate(concept['sub_concepts'][:3]):
            sub_id = f"sub_{i}_{j}"
            dot.node(sub_id, sub['name'], fillcolor='lightyellow')
            dot.edge(node_id, sub_id, label=sub['relation'])
    
    return dot.pipe(format='svg').decode('utf-8')
```

2. **Plotly para Mapas Interactivos 3D**
```python
def _create_3d_concept_network(self, chunks: List[Dict]) -> go.Figure:
    """Crear red de conceptos en 3D con Plotly"""
    # Extraer conceptos
    concepts = self.extract_key_concepts(chunks)
    
    # Crear grafo
    G = nx.Graph()
    for concept in concepts:
        G.add_node(concept['concept'], weight=concept['score'])
    
    # Agregar aristas (relaciones)
    for i, c1 in enumerate(concepts):
        for c2 in concepts[i+1:]:
            similarity = self._calculate_concept_similarity(
                c1['concept'], c2['concept']
            )
            if similarity > 0.3:
                G.add_edge(c1['concept'], c2['concept'], weight=similarity)
    
    # Layout 3D
    pos = nx.spring_layout(G, dim=3, seed=42)
    
    # Preparar trazas
    edge_trace = []
    for edge in G.edges():
        x0, y0, z0 = pos[edge[0]]
        x1, y1, z1 = pos[edge[1]]
        edge_trace.append(
            go.Scatter3d(
                x=[x0, x1, None],
                y=[y0, y1, None],
                z=[z0, z1, None],
                mode='lines',
                line=dict(color='gray', width=2),
                hoverinfo='none'
            )
        )
    
    # Nodos
    node_x, node_y, node_z = [], [], []
    node_text = []
    node_sizes = []
    
    for node in G.nodes():
        x, y, z = pos[node]
        node_x.append(x)
        node_y.append(y)
        node_z.append(z)
        node_text.append(node)
        node_sizes.append(G.nodes[node]['weight'] * 50)
    
    node_trace = go.Scatter3d(
        x=node_x, y=node_y, z=node_z,
        mode='markers+text',
        text=node_text,
        textposition='top center',
        marker=dict(
            size=node_sizes,
            color='lightblue',
            line=dict(color='darkblue', width=2)
        )
    )
    
    # Crear figura
    fig = go.Figure(data=edge_trace + [node_trace])
    fig.update_layout(
        title='Mapa Conceptual 3D',
        showlegend=False,
        hovermode='closest',
        scene=dict(
            xaxis=dict(showgrid=False, showticklabels=False),
            yaxis=dict(showgrid=False, showticklabels=False),
            zaxis=dict(showgrid=False, showticklabels=False)
        )
    )
    
    return fig
```

---

### Secci√≥n 12: RESUMENES AUTOM√ÅTICOS (L√≠nea 1177)

#### Estrategias Implementadas

**1. Resumen con LLM (IA)**
```python
Tipos disponibles:
- comprehensive: Resumen completo y detallado
- executive: Resumen ejecutivo para decisiones
- analytical: An√°lisis profundo acad√©mico
- thematic: Organizado por temas

Ventajas:
‚úÖ Alta calidad y coherencia
‚úÖ Comprensi√≥n contextual profunda
‚úÖ Personalizable con prompts

Limitaciones:
‚ùå Requiere modelo LLM (Ollama)
‚ùå M√°s lento que m√©todos extractivos
‚ùå Puede ser costoso computacionalmente
```

**2. Resumen Extractivo B√°sico**
```python
Algoritmo:
1. Dividir en oraciones
2. Calcular frecuencia de palabras importantes
3. Puntuar oraciones por relevancia
4. Seleccionar top N oraciones
5. Ordenar por aparici√≥n original

Ventajas:
‚úÖ Muy r√°pido
‚úÖ No requiere IA
‚úÖ Preserva texto original

Limitaciones:
‚ùå No genera texto nuevo
‚ùå Puede ser incoherente
‚ùå Limitado por oraciones existentes
```

#### Mejoras Potenciales

**Algoritmos Tradicionales a Agregar**:

1. **TextRank (PageRank para texto)**
```python
def _summarize_with_textrank(self, chunks: List[Dict], num_sentences: int = 5) -> str:
    """Generar resumen usando TextRank"""
    from gensim.summarization import summarize
    from gensim.summarization.textcleaner import split_sentences
    
    # Combinar todo el texto
    all_text = " ".join([chunk.get('content', '') for chunk in chunks])
    
    try:
        # Usar TextRank de gensim
        summary = summarize(
            all_text,
            ratio=0.2,  # 20% del texto original
            word_count=500,  # M√°ximo 500 palabras
            split=True  # Retornar lista de oraciones
        )
        
        return ". ".join(summary[:num_sentences]) + "."
        
    except Exception as e:
        logger.warning(f"Error en TextRank: {e}")
        return self.generate_basic_summary(chunks, num_sentences)
```

2. **LexRank**
```python
from lexrank import LexRank
from lexrank.mappings.stopwords import STOPWORDS

def _summarize_with_lexrank(self, chunks: List[Dict], num_sentences: int = 5) -> str:
    """Generar resumen usando LexRank"""
    # Preparar documentos
    documents = [chunk.get('content', '') for chunk in chunks if chunk.get('content')]
    
    # Configurar LexRank
    lxr = LexRank(documents, stopwords=STOPWORDS['es'])
    
    # Obtener resumen
    summary_sentences = lxr.get_summary(
        documents,
        summary_size=num_sentences,
        threshold=0.1
    )
    
    return ". ".join(summary_sentences) + "."
```

**Algoritmos con IA a Agregar**:

1. **BART para Resumen Abstractivo**
```python
from transformers import pipeline

def _summarize_with_bart(self, chunks: List[Dict], max_length: int = 500) -> str:
    """Resumen abstractivo usando BART"""
    summarizer = pipeline(
        "summarization",
        model="facebook/bart-large-cnn"
    )
    
    # Combinar contenido
    all_text = " ".join([chunk.get('content', '') for chunk in chunks])
    
    # Dividir en secciones si es muy largo
    max_input_length = 1024
    if len(all_text.split()) > max_input_length:
        # Resumir por secciones y luego combinar
        sections = self._split_into_sections(all_text, max_input_length)
        summaries = []
        
        for section in sections:
            summary = summarizer(
                section,
                max_length=150,
                min_length=50,
                do_sample=False
            )[0]['summary_text']
            summaries.append(summary)
        
        # Resumir los res√∫menes
        combined = " ".join(summaries)
        final_summary = summarizer(
            combined,
            max_length=max_length,
            min_length=100,
            do_sample=False
        )[0]['summary_text']
        
        return final_summary
    else:
        summary = summarizer(
            all_text,
            max_length=max_length,
            min_length=100,
            do_sample=False
        )[0]['summary_text']
        
        return summary
```

2. **T5 para Resumen Multiling√ºe**
```python
from transformers import T5ForConditionalGeneration, T5Tokenizer

def _summarize_with_t5(self, chunks: List[Dict]) -> str:
    """Resumen usando T5 multiling√ºe"""
    model_name = "google/mt5-base"
    tokenizer = T5Tokenizer.from_pretrained(model_name)
    model = T5ForConditionalGeneration.from_pretrained(model_name)
    
    # Preparar texto
    all_text = " ".join([chunk.get('content', '') for chunk in chunks])
    
    # Tokenizar
    inputs = tokenizer.encode(
        "summarize: " + all_text,
        return_tensors="pt",
        max_length=512,
        truncation=True
    )
    
    # Generar resumen
    summary_ids = model.generate(
        inputs,
        max_length=200,
        min_length=50,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True
    )
    
    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    
    return summary
```

---

## üîÑ Mejoras Sugeridas por Componente

### Cache y Rendimiento

**1. Implementar Redis para Cache Distribuido**
```python
import redis

class RedisCache Manager(CacheManager):
    """Cache distribuido con Redis"""
    
    def __init__(self, redis_host='localhost', redis_port=6379):
        self.redis_client = redis.Redis(
            host=redis_host,
            port=redis_port,
            decode_responses=True
        )
    
    def get(self, key: str) -> Optional[Any]:
        data = self.redis_client.get(key)
        if data:
            return json.loads(data)
        return None
    
    def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        self.redis_client.setex(
            key,
            ttl,
            json.dumps(value)
        )
```

**2. Procesamiento As√≠ncrono con AsyncIO**
```python
import asyncio
from typing import List, Dict

async def _async_process_chunks(self, chunks: List[Dict]) -> List[Dict]:
    """Procesar chunks de forma as√≠ncrona"""
    async def process_single_chunk(chunk):
        content = chunk.get('content', '')
        processed = await self._async_preprocess(content)
        return processed
    
    tasks = [process_single_chunk(chunk) for chunk in chunks]
    results = await asyncio.gather(*tasks)
    
    return results
```

### An√°lisis Avanzados

**1. An√°lisis de Coherencia de Documentos**
```python
def analyze_document_coherence(self, chunks: List[Dict]) -> Dict:
    """Analizar coherencia entre secciones del documento"""
    from sklearn.metrics.pairwise import cosine_similarity
    
    # Generar embeddings para cada chunk
    embeddings = []
    for chunk in chunks:
        vector = self._get_chunk_embedding(chunk)
        embeddings.append(vector)
    
    embeddings_matrix = np.array(embeddings)
    
    # Calcular similitud entre chunks consecutivos
    coherence_scores = []
    for i in range(len(embeddings) - 1):
        similarity = cosine_similarity(
            [embeddings[i]],
            [embeddings[i + 1]]
        )[0][0]
        coherence_scores.append(similarity)
    
    return {
        'overall_coherence': np.mean(coherence_scores),
        'min_coherence': np.min(coherence_scores),
        'max_coherence': np.max(coherence_scores),
        'coherence_by_section': coherence_scores
    }
```

**2. An√°lisis de Complejidad Textual**
```python
def analyze_text_complexity(self, chunks: List[Dict]) -> Dict:
    """Analizar complejidad del texto"""
    import textstat
    
    complexity_results = {
        'readability': {},
        'by_chunk': [],
        'overall': {}
    }
    
    all_scores = {
        'flesch_reading_ease': [],
        'gunning_fog': [],
        'automated_readability_index': [],
        'avg_sentence_length': [],
        'avg_word_length': []
    }
    
    for chunk in chunks:
        content = chunk.get('content', '')
        if not content:
            continue
        
        chunk_scores = {
            'flesch_reading_ease': textstat.flesch_reading_ease(content),
            'gunning_fog': textstat.gunning_fog(content),
            'automated_readability_index': textstat.automated_readability_index(content),
            'avg_sentence_length': textstat.avg_sentence_length(content),
            'avg_word_length': textstat.avg_character_per_word(content)
        }
        
        for key, value in chunk_scores.items():
            all_scores[key].append(value)
        
        complexity_results['by_chunk'].append(chunk_scores)
    
    # Calcular promedios
    for key, values in all_scores.items():
        if values:
            complexity_results['overall'][key] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'min': np.min(values),
                'max': np.max(values)
            }
    
    return complexity_results
```

---

## üìñ Gu√≠a de Modificaci√≥n

### C√≥mo Agregar un Nuevo Tipo de An√°lisis

**Paso 1: Agregar al Enum** (L√≠nea 145)
```python
class AnalysisType(Enum):
    # ... existentes ...
    COMPLEXITY_ANALYSIS = "complexity_analysis"  # NUEVO
```

**Paso 2: Crear Analizador Especializado**
```python
# Agregar despu√©s de SentimentAnalyzer (L√≠nea ~900)
class ComplexityAnalyzer(BaseAnalyzer):
    """Analizador de complejidad textual"""
    
    def __init__(self, config: AnalysisConfig):
        super().__init__(config)
        self.preprocessor = TextPreprocessor()
    
    def analyze(self, chunks: List[Dict]) -> AnalysisResult:
        """Analizar complejidad del texto"""
        # Tu implementaci√≥n aqu√≠
        pass
```

**Paso 3: Integrar en Clase Principal**
```python
# En __init__ de AdvancedQualitativeAnalyzer
self.complexity_analyzer = ComplexityAnalyzer(self.config)

# Agregar m√©todo p√∫blico
def analyze_complexity(self, chunks: List[Dict]) -> Dict:
    result = self.complexity_analyzer.analyze(chunks)
    return result.data
```

**Paso 4: Agregar Funci√≥n de Renderizado**
```python
# En secci√≥n 15 (L√≠nea 4363)
def render_complexity_analysis(analyzer: AdvancedQualitativeAnalyzer, chunks: List[Dict]):
    """Renderizar an√°lisis de complejidad"""
    st.header("üìä An√°lisis de Complejidad")
    
    # Realizar an√°lisis
    complexity = analyzer.analyze_complexity(chunks)
    
    # Visualizar resultados
    # ... tu c√≥digo de visualizaci√≥n
```

**Paso 5: Integrar en render() Principal**
```python
# En funci√≥n render() (L√≠nea 5890)
tabs = st.tabs([
    # ... existentes ...
    "üìä Complejidad"  # NUEVA
])

with tabs[N]:  # N = posici√≥n de la nueva tab
    render_complexity_analysis(analyzer, chunks)
```

### C√≥mo Mejorar un Algoritmo Existente

**Ejemplo: Mejorar Extracci√≥n de Conceptos**

1. **Ve a la L√≠nea 395** (Secci√≥n 6: EXTRACCI√ìN DE CONCEPTOS)
2. **Localiza** `ConceptExtractor._extract_with_tfidf()`
3. **Modifica** los par√°metros o agrega nuevo m√©todo:

```python
def _extract_with_tfidf(self, texts: List[str]) -> List[ConceptData]:
    """Extraer conceptos usando TF-IDF MEJORADO"""
    
    # MEJORA 1: Ajustar par√°metros
    vectorizer = TfidfVectorizer(
        max_features=300,  # Era 200
        stop_words=self.preprocessor.get_spanish_stopwords(),
        ngram_range=(1, 4),  # Era (1, 3) - ahora detecta frases m√°s largas
        min_df=max(1, len(texts) // 15),  # Era // 10
        max_df=0.85,  # Era 0.9
        sublinear_tf=True,  # NUEVO: Escala logar√≠tmica para TF
        use_idf=True,
        smooth_idf=True  # NUEVO: Suavizado IDF
    )
    
    # MEJORA 2: Agregar an√°lisis sem√°ntico
    tfidf_matrix = vectorizer.fit_transform(texts)
    feature_names = vectorizer.get_feature_names_out()
    
    # MEJORA 3: Calcular similitud sem√°ntica entre conceptos
    similarity_matrix = cosine_similarity(tfidf_matrix.T)
    
    # MEJORA 4: Agrupar conceptos relacionados
    related_groups = self._group_related_concepts(
        feature_names, 
        similarity_matrix
    )
    
    # Crear ConceptData enriquecido
    concepts = []
    mean_scores = tfidf_matrix.mean(axis=0).A1
    
    for i, score in enumerate(mean_scores):
        if score > 0:
            concept = ConceptData(
                concept=feature_names[i],
                score=float(score),
                frequency=int(score * len(texts) * 10),
                related_concepts=related_groups.get(feature_names[i], []),  # NUEVO
                category=self._categorize_concept(feature_names[i])  # NUEVO
            )
            concepts.append(concept)
    
    return sorted(concepts, key=lambda x: x.score, reverse=True)
```

---

## üöÄ Roadmap de Mejoras Futuras

### Corto Plazo (1-2 semanas)

1. **Optimizaci√≥n de Rendimiento**
   - [ ] Implementar cache distribuido con Redis
   - [ ] Agregar procesamiento as√≠ncrono
   - [ ] Optimizar uso de memoria con generadores

2. **Mejoras en Algoritmos Tradicionales**
   - [ ] Implementar BM25 para ranking
   - [ ] Agregar TextRank para res√∫menes
   - [ ] Incluir PMI para colocaciones

3. **Mejoras en Visualizaciones**
   - [ ] Agregar gr√°ficos 3D con Plotly
   - [ ] Implementar dashboards interactivos
   - [ ] Crear exportaci√≥n a diferentes formatos

### Mediano Plazo (1 mes)

1. **Integraci√≥n de IA Avanzada**
   - [ ] Implementar BERT para NER
   - [ ] Agregar BERTopic para temas
   - [ ] Incluir modelos de embeddings sem√°nticos

2. **An√°lisis Avanzados**
   - [ ] An√°lisis de coherencia de documentos
   - [ ] Detecci√≥n de argumentos
   - [ ] An√°lisis de estructura ret√≥rica

3. **Mejoras en UX**
   - [ ] Wizard de configuraci√≥n
   - [ ] Recomendaciones autom√°ticas
   - [ ] Exportaci√≥n de reportes profesionales

### Largo Plazo (3 meses)

1. **Machine Learning Personalizado**
   - [ ] Entrenamiento de modelos espec√≠ficos del dominio
   - [ ] Transfer learning para espa√±ol
   - [ ] Fine-tuning de modelos LLM

2. **An√°lisis Multimodal**
   - [ ] Procesamiento de im√°genes en documentos
   - [ ] An√°lisis de tablas y gr√°ficos
   - [ ] OCR integrado

3. **Colaboraci√≥n y Compartici√≥n**
   - [ ] An√°lisis colaborativo
   - [ ] Compartir configuraciones
   - [ ] Plantillas de an√°lisis reutilizables

---

## üìù Conclusiones

El m√≥dulo de an√°lisis cualitativo es extremadamente completo pero tambi√©n complejo. Las mejoras propuestas permitir√°n:

1. **Mejor rendimiento** con cache distribuido y procesamiento as√≠ncrono
2. **An√°lisis m√°s precisos** con algoritmos de ML avanzados
3. **Mayor usabilidad** con mejor organizaci√≥n y documentaci√≥n
4. **Escalabilidad** para manejar grandes vol√∫menes de datos

La arquitectura modular implementada facilita agregar nuevas funcionalidades sin afectar c√≥digo existente.

