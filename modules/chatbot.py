"""
M√≥dulo de Chatbot Inteligente con RAG
Sistema de conversaci√≥n con IA local usando Ollama y contexto RAG

ORGANIZACI√ìN DEL C√ìDIGO POR FUNCIONALIDADES:
================================================================================
1. IMPORTS Y CONFIGURACI√ìN GLOBAL (L√≠nea 50)
2. CONSTANTES Y CONFIGURACI√ìN (L√≠nea 80)
3. FUNCIONES DE CONTEXTO RAG (L√≠nea 696)
4. FUNCIONES DE RENDERIZADO (L√≠nea 33)
   - render() ‚Üí Funci√≥n principal
   - render_history_sidebar() ‚Üí Sidebar de historial
   - render_message_export_buttons() ‚Üí Botones de exportaci√≥n
   - render_conversation_export_controls() ‚Üí Controles de exportaci√≥n
================================================================================

√çNDICE DE NAVEGACI√ìN R√ÅPIDA:
================================================================================
Para encontrar y modificar cualquier funcionalidad, busca estas funciones:

üöÄ FUNCI√ìN PRINCIPAL:
   - render() (L√≠nea 33) ‚Üí Funci√≥n principal del chatbot
   
üìö INTEGRACI√ìN RAG:
   - get_rag_context() (L√≠nea 696) ‚Üí Obtener contexto RAG
   
üìÅ GESTI√ìN DE HISTORIAL:
   - render_history_sidebar() (L√≠nea 493) ‚Üí Sidebar de historial
   - Guardar conversaci√≥n
   - Cargar conversaci√≥n
   - Eliminar conversaci√≥n
   
üì§ EXPORTACI√ìN:
   - render_message_export_buttons() (L√≠nea 559) ‚Üí Exportar mensaje individual
   - render_conversation_export_controls() (L√≠nea 626) ‚Üí Exportar conversaci√≥n completa
   - Exportar a DOCX
   - Exportar a PDF
   - Copiar al portapapeles

üí¨ PROCESAMIENTO DE CHAT:
   - Generaci√≥n de respuesta con IA
   - Construcci√≥n de prompts
   - Manejo de contexto RAG
   - Sistema de progreso
   
üé® INTERFAZ DE USUARIO:
   - Configuraci√≥n del chat (modelo, RAG, tokens, debug)
   - Visualizaci√≥n de mensajes
   - M√©tricas de conversaci√≥n
   - Informaci√≥n de debug

FUNCIONALIDADES CLAVE:
================================================================================
‚úÖ Sistema RAG: Contexto completo de documentos
‚úÖ Modo Debug: Trazabilidad y estad√≠sticas
‚úÖ Historial: Guardar/cargar conversaciones
‚úÖ Exportaci√≥n: DOCX, PDF, portapapeles
‚úÖ M√©tricas: Estad√≠sticas de conversaci√≥n
‚úÖ Cancelaci√≥n: Detener procesamiento en curso
‚úÖ Configuraci√≥n: Modelo, tokens, RAG, debug

MEJORAS IMPLEMENTADAS (√öltima actualizaci√≥n):
================================================================================
‚úÖ ORGANIZACI√ìN:
   - C√≥digo organizado en secciones funcionales
   - √çndice de navegaci√≥n completo
   - Separadores claros para cada funcionalidad
   - Comentarios descriptivos de modificaci√≥n

‚úÖ SISTEMA DE PLANTILLAS:
   - Plantillas reutilizables de prompts
   - Detecci√≥n autom√°tica del tipo de consulta
   - Selecci√≥n manual de tipo de respuesta
   - 6 tipos de plantillas especializadas:
     ‚Ä¢ General: Respuesta balanceada
     ‚Ä¢ An√°lisis: An√°lisis detallado y estructurado
     ‚Ä¢ Resumen: Resumen conciso y completo
     ‚Ä¢ Explicaci√≥n: Lenguaje simple y claro
     ‚Ä¢ Comparaci√≥n: Similitudes y diferencias
     ‚Ä¢ Extracci√≥n: Datos espec√≠ficos

‚úÖ SUGERENCIAS INTELIGENTES:
   - 6 sugerencias de preguntas comunes
   - Un clic para usar sugerencia
   - Mostradas solo al inicio de conversaci√≥n
   - Dise√±o en 3 columnas para mejor UX

================================================================================
"""

import streamlit as st
import time
from datetime import datetime
from config.settings import config
from utils.logger import setup_logger
from utils.error_handler import ErrorHandler
from utils.ollama_client import OllamaClient
from utils.rag_processor import rag_processor
from utils.chat_history import chat_history_manager
from utils.traceability import TraceabilityManager

# Importar exportadores con manejo de errores
try:
    from utils.chat_history import chat_exporter
    EXPORT_AVAILABLE = True
except ImportError:
    EXPORT_AVAILABLE = False

# Importar pyperclip con manejo de errores
try:
    import pyperclip
    CLIPBOARD_AVAILABLE = True
except ImportError:
    CLIPBOARD_AVAILABLE = False

logger = setup_logger()
error_handler = ErrorHandler()

# =============================================================================
# SISTEMA DE PLANTILLAS DE PROMPTS
# =============================================================================
# Esta secci√≥n define plantillas reutilizables de prompts para diferentes escenarios.
# MODIFICAR AQU√ç PARA:
# - Agregar nuevos tipos de prompts
# - Modificar prompts existentes
# - Personalizar el comportamiento de la IA

class PromptTemplates:
    """Plantillas de prompts para diferentes modos de conversaci√≥n"""
    
    # Plantilla principal (RAG con contexto completo)
    DEFAULT_RAG = """<think>
El usuario me ha proporcionado documentos completos y me est√° haciendo una pregunta. Necesito:

1. Analizar cuidadosamente TODA la informaci√≥n disponible en los documentos
2. Identificar qu√© partes son relevantes para la pregunta espec√≠fica
3. Razonar sobre las conexiones y relaciones entre diferentes partes del contenido
4. Proporcionar una respuesta completa y bien fundamentada
5. Citar las fuentes espec√≠ficas cuando sea apropiado

Voy a revisar todo el contexto disponible y pensar profundamente sobre la mejor respuesta.
</think>

INSTRUCCIONES PARA DEEPSEEK:
- Tienes acceso a TODO el contenido de los documentos del usuario
- Analiza profundamente toda la informaci√≥n disponible
- Razona sobre las conexiones y relaciones entre diferentes partes del contenido
- Proporciona respuestas completas y bien fundamentadas
- SOLO usa informaci√≥n que est√© en los documentos proporcionados
- Si la informaci√≥n no est√° en los documentos, ind√≠calo claramente
- Cita las fuentes espec√≠ficas cuando sea relevante

CONTEXTO COMPLETO DE TODOS LOS DOCUMENTOS:
{context}

PREGUNTA DEL USUARIO:
{query}

RESPUESTA (basada en an√°lisis profundo de todos los documentos):"""

    # Plantilla para an√°lisis detallado
    ANALYTICAL = """<think>
Necesito realizar un an√°lisis profundo y estructurado de la pregunta del usuario, considerando todo el contexto disponible.
</think>

Eres un analista experto. Analiza profundamente el siguiente contexto y proporciona una respuesta detallada y bien estructurada.

INSTRUCCIONES:
- Proporciona un an√°lisis detallado y estructurado
- Usa secciones y puntos cuando sea apropiado
- Incluye ejemplos espec√≠ficos del contexto
- Razona sobre implicaciones y relaciones
- Cita evidencia espec√≠fica de los documentos

CONTEXTO DE DOCUMENTOS:
{context}

PREGUNTA A ANALIZAR:
{query}

AN√ÅLISIS DETALLADO:"""

    # Plantilla para resumen
    SUMMARY = """<think>
El usuario quiere un resumen. Debo identificar los puntos m√°s importantes y presentarlos de forma concisa pero completa.
</think>

Proporciona un resumen claro y conciso del siguiente contenido.

INSTRUCCIONES:
- Identifica los puntos principales
- S√© conciso pero completo
- Usa vi√±etas o listas cuando sea apropiado
- Mant√©n la informaci√≥n m√°s relevante

CONTENIDO A RESUMIR:
{context}

PREGUNTA DEL USUARIO:
{query}

RESUMEN:"""

    # Plantilla para explicaci√≥n simple
    EXPLAIN_SIMPLE = """<think>
El usuario necesita una explicaci√≥n clara y f√°cil de entender. Debo simplificar conceptos complejos sin perder precisi√≥n.
</think>

Explica el siguiente tema de forma clara y f√°cil de entender, como si le hablaras a alguien que no es experto en el tema.

INSTRUCCIONES:
- Usa lenguaje simple y claro
- Evita jerga t√©cnica innecesaria
- Usa analog√≠as o ejemplos cuando sea √∫til
- Estructura la explicaci√≥n de forma l√≥gica

CONTEXTO:
{context}

PREGUNTA:
{query}

EXPLICACI√ìN CLARA:"""

    # Plantilla para comparaci√≥n
    COMPARE = """<think>
El usuario quiere comparar informaci√≥n. Debo identificar similitudes y diferencias de forma estructurada.
</think>

Compara y contrasta la informaci√≥n relevante del contexto para responder la pregunta.

INSTRUCCIONES:
- Identifica similitudes y diferencias clave
- Organiza la comparaci√≥n de forma clara
- Proporciona ejemplos espec√≠ficos
- Resume conclusiones principales

CONTEXTO:
{context}

PREGUNTA SOBRE COMPARACI√ìN:
{query}

COMPARACI√ìN DETALLADA:"""

    # Plantilla para extracci√≥n de datos
    EXTRACT_DATA = """<think>
El usuario busca informaci√≥n espec√≠fica o datos concretos. Debo extraer la informaci√≥n relevante de forma precisa.
</think>

Extrae la informaci√≥n o datos espec√≠ficos solicitados del siguiente contexto.

INSTRUCCIONES:
- S√© preciso y espec√≠fico
- Cita las fuentes exactas
- Si los datos no est√°n disponibles, ind√≠calo claramente
- Organiza la informaci√≥n de forma estructurada

CONTEXTO:
{context}

INFORMACI√ìN SOLICITADA:
{query}

DATOS EXTRA√çDOS:"""

    # Plantilla para no RAG (sin documentos)
    NO_DOCUMENTS = """<think>
El usuario est√° preguntando algo pero no hay documentos cargados en el sistema. Necesito explicar esto claramente y guiar al usuario sobre c√≥mo cargar documentos.
</think>

No hay documentos cargados en el sistema RAG.

Para obtener respuestas basadas en tus documentos:
1. Ve a la p√°gina "Procesamiento de Documentos RAG"
2. Carga tus documentos (PDF, DOCX, TXT, etc.)
3. Proc√©salos haciendo clic en "Procesar Documentos"
4. Regresa aqu√≠ para hacer preguntas sobre su contenido

Tu pregunta: {query}

Respuesta: No puedo responder esta pregunta porque no hay documentos cargados para analizar. Por favor, carga y procesa tus documentos primero."""

    # Plantilla para RAG deshabilitado
    RAG_DISABLED = """<think>
El sistema RAG est√° deshabilitado, pero el usuario est√° haciendo una pregunta. Debo informar que para obtener respuestas basadas en documentos espec√≠ficos, necesita habilitar el RAG.
</think>

NOTA: El sistema RAG est√° deshabilitado. Para obtener respuestas basadas en documentos espec√≠ficos, habilita el RAG en la configuraci√≥n.

Pregunta: {query}

Respuesta: Para responder bas√°ndome en tus documentos espec√≠ficos, necesitas habilitar el sistema RAG y cargar tus documentos."""

    @staticmethod
    def get_prompt(template_type: str, query: str, context: str = None) -> str:
        """
        Obtener prompt formateado seg√∫n el tipo de plantilla.
        
        Args:
            template_type: Tipo de plantilla ('default', 'analytical', 'summary', etc.)
            query: Pregunta del usuario
            context: Contexto RAG (opcional)
            
        Returns:
            Prompt completo formateado
        """
        templates = {
            'default': PromptTemplates.DEFAULT_RAG,
            'analytical': PromptTemplates.ANALYTICAL,
            'summary': PromptTemplates.SUMMARY,
            'explain': PromptTemplates.EXPLAIN_SIMPLE,
            'compare': PromptTemplates.COMPARE,
            'extract': PromptTemplates.EXTRACT_DATA,
            'no_docs': PromptTemplates.NO_DOCUMENTS,
            'no_rag': PromptTemplates.RAG_DISABLED
        }
        
        template = templates.get(template_type, PromptTemplates.DEFAULT_RAG)
        
        return template.format(query=query, context=context or "Sin contexto disponible")
    
    @staticmethod
    def detect_query_type(query: str) -> str:
        """
        Detectar autom√°ticamente el tipo de consulta para seleccionar la plantilla apropiada.
        
        Args:
            query: Pregunta del usuario
            
        Returns:
            Tipo de plantilla recomendada
        """
        query_lower = query.lower()
        
        # Palabras clave para cada tipo
        if any(word in query_lower for word in ['resume', 'resumen', 'resumir', 'principales puntos']):
            return 'summary'
        
        elif any(word in query_lower for word in ['explica', 'explicar', 'qu√© es', 'que es', 'c√≥mo funciona', 'como funciona']):
            return 'explain'
        
        elif any(word in query_lower for word in ['compara', 'comparar', 'diferencia', 'diferencias', 'vs', 'versus']):
            return 'compare'
        
        elif any(word in query_lower for word in ['extrae', 'extraer', 'datos', 'informaci√≥n sobre', 'dame', 'muestra', 'lista']):
            return 'extract'
        
        elif any(word in query_lower for word in ['analiza', 'analizar', 'an√°lisis', 'profundiza', 'detallado']):
            return 'analytical'
        
        else:
            return 'default'

# =============================================================================
# FUNCI√ìN PRINCIPAL DE RENDERIZADO
# =============================================================================
# Esta es la funci√≥n principal que renderiza toda la interfaz del chatbot.
# MODIFICAR AQU√ç PARA:
# - Cambiar el flujo principal de la interfaz
# - Ajustar la l√≥gica de inicializaci√≥n
# - Modificar el orden de renderizado de componentes

def render():
    """Renderizar la pesta√±a del chatbot"""
    # Inicializar el historial de mensajes si no existe (DEBE SER LO PRIMERO)
    if "messages" not in st.session_state:
        st.session_state.messages = []
    
    st.header("üí¨ Chat Inteligente")
    
    # Inicializar cliente Ollama
    ollama_client = OllamaClient()
    
    # Verificar disponibilidad de Ollama
    if not ollama_client.is_available():
        st.error("‚ùå Ollama no est√° disponible. Aseg√∫rate de que est√© ejecut√°ndose.")
        st.info("Para iniciar Ollama, ejecuta: `ollama serve` en tu terminal")
        return
    
    # Configuraci√≥n r√°pida del chat (compacta)
    with st.expander("üéõÔ∏è Configuraci√≥n R√°pida del Chat", expanded=False):
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            # Obtener modelos disponibles con cach√© en session_state
            if 'available_models' not in st.session_state or st.session_state.get('models_last_refresh', 0) < time.time() - 300:
                # Actualizar cach√© cada 5 minutos
                available_models = ollama_client.get_available_models()
                st.session_state.available_models = available_models
                st.session_state.models_last_refresh = time.time()
            else:
                available_models = st.session_state.available_models
            
            model_names = [model['name'] for model in available_models]
            
            if not model_names:
                st.error("No hay modelos disponibles")
                # Bot√≥n para refrescar modelos
                if st.button("üîÑ Refrescar modelos", key="refresh_models_chatbot"):
                    st.session_state.available_models = ollama_client.get_available_models(force_refresh=True)
                    st.rerun()
                return
            
            # Usar modelo configurado o por defecto
            current_model = st.session_state.get('selected_llm_model', config.DEFAULT_LLM_MODEL)
            if current_model not in model_names and model_names:
                current_model = model_names[0]
            
            selected_model = st.selectbox(
                "ü§ñ Modelo LLM",
                model_names,
                index=model_names.index(current_model) if current_model in model_names else 0,
                help="Modelo de lenguaje para el chat",
                key="chat_model_selector"
            )
            
            # El modelo se actualiza autom√°ticamente por el widget
        
        with col2:
            use_rag = st.toggle(
                "üìö Usar RAG", 
                value=st.session_state.get('rag_enabled', True), 
                help="Usar documentos cargados como contexto",
                key="chat_rag_toggle"
            )
            
            # El toggle se actualiza autom√°ticamente en session_state
            
        with col3:
            max_tokens = st.slider(
                "üéØ M√°ximo tokens", 
                100, config.MAX_RESPONSE_TOKENS, 
                st.session_state.get('chat_max_tokens', config.MAX_RESPONSE_TOKENS),
                help="Longitud m√°xima de la respuesta",
                key="chat_tokens_slider"
            )
            
            # El slider se actualiza autom√°ticamente en session_state
        
        with col4:
            # NUEVO: Selector de tipo de prompt
            prompt_type = st.selectbox(
                "üéØ Tipo de Respuesta:",
                options=['auto', 'default', 'analytical', 'summary', 'explain', 'compare', 'extract'],
                format_func=lambda x: {
                    'auto': 'ü§ñ Autom√°tico (Detecta el tipo)',
                    'default': 'üí¨ General',
                    'analytical': 'üîç An√°lisis Detallado',
                    'summary': 'üìù Resumen',
                    'explain': 'üí° Explicaci√≥n Simple',
                    'compare': '‚öñÔ∏è Comparaci√≥n',
                    'extract': 'üìä Extracci√≥n de Datos'
                }[x],
                help="Selecciona el tipo de respuesta que deseas o deja en 'Autom√°tico' para detecci√≥n inteligente",
                key="chat_prompt_type"
            )
        
        with col5:
            debug_mode = st.toggle(
                "üîç Debug",
                value=st.session_state.get('debug_mode', config.ENABLE_DEBUG_MODE),
                help="Mostrar informaci√≥n detallada de procesamiento",
                key="chat_debug_toggle"
            )
            
            # El toggle se actualiza autom√°ticamente en session_state
    
    # √Årea principal del chat
    chat_container = st.container()
    
    # Mostrar historial de chat con mejor dise√±o
    with chat_container:
        if not st.session_state.messages:
            st.markdown("""
            <div style="text-align: center; padding: 2rem; background: #2c3e50; border-radius: 10px; margin: 1rem 0; color: #ecf0f1;">
                <h3 style="color: #00FF99;">üëã ¬°Hola! Soy CogniChat</h3>
                <p style="color: #bdc3c7;">Preg√∫ntame cualquier cosa. Si tienes documentos cargados, puedo usarlos como contexto.</p>
            </div>
            """, unsafe_allow_html=True)
        
        for i, message in enumerate(st.session_state.messages):
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                if "timestamp" in message:
                    st.caption(f"üïí {message['timestamp']}")
                
                # Botones de exportaci√≥n para mensajes del asistente
                if message["role"] == "assistant":
                    render_message_export_buttons(message, i)
                
                # Mostrar contexto usado si es un mensaje del asistente
                if message["role"] == "assistant" and "context_used" in message and message["context_used"]:
                    with st.expander("üìö Contexto utilizado", expanded=False):
                        st.markdown(f"```\n{message['context_used'][:500]}...\n```")
    
    # =============================================================================
    # PROCESAMIENTO DE MENSAJES DEL USUARIO (SOLO SUGERENCIAS)
    # =============================================================================
    
    # Verificar si hay prompt sugerido para procesar (se maneja al final)
    prompt = None
    if 'suggested_prompt' in st.session_state:
        prompt = st.session_state.suggested_prompt
        del st.session_state.suggested_prompt
    
    if prompt:
        # Agregar mensaje del usuario
        timestamp = datetime.now().strftime("%H:%M:%S")
        st.session_state.messages.append({
            "role": "user", 
            "content": prompt,
            "timestamp": timestamp
        })
        
        # Mostrar mensaje del usuario
        with st.chat_message("user"):
            st.markdown(prompt)
            st.caption(f"üïí {timestamp}")
        
        # Generar respuesta con interfaz mejorada
        with st.chat_message("assistant"):
            # Crear contenedor para el progreso
            progress_container = st.empty()
            response_container = st.empty()
            
            # Inicializar variables de control
            if 'processing_cancelled' not in st.session_state:
                st.session_state.processing_cancelled = False
            
            try:
                # Paso 1: Inicializaci√≥n
                with progress_container.container():
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        progress_bar = st.progress(0)
                        status_text = st.empty()
                        status_text.text("üöÄ Iniciando procesamiento...")
                    with col2:
                        if st.button("‚ùå Cancelar", key="cancel_processing", type="secondary"):
                            st.session_state.processing_cancelled = True
                            st.warning("‚ö†Ô∏è Procesamiento cancelado por el usuario")
                            st.stop()
                
                progress_bar.progress(10)
                time.sleep(0.5)
                
                # Verificar cancelaci√≥n
                if st.session_state.processing_cancelled:
                    st.session_state.processing_cancelled = False
                    st.stop()
                
                # Obtener configuraci√≥n de debug
                debug_mode = st.session_state.get('debug_mode', False)
                
                # Paso 2: Procesamiento RAG
                status_text.text("üîç Analizando documentos...")
                progress_bar.progress(25)
                
                context = None
                relevant_chunks = []
                query_id = None
                
                if use_rag:
                    if debug_mode:
                        # Modo debug: usar trazabilidad
                        context, relevant_chunks, query_id = get_rag_context(prompt, enable_tracing=True)
                    else:
                        # Modo normal: usar contexto completo
                        context = get_rag_context(prompt, enable_tracing=False)
                    
                    progress_bar.progress(50)
                    
                    if context:
                        if debug_mode:
                            status_text.text(f"‚úÖ Contexto recuperado - {len(relevant_chunks)} chunks encontrados")
                        else:
                            status_text.text(f"‚úÖ Contexto completo cargado - {len(context):,} caracteres")
                    else:
                        status_text.text("‚ö†Ô∏è No hay documentos procesados disponibles")
                else:
                    status_text.text("‚ÑπÔ∏è Modo sin RAG - Respuesta general")
                    progress_bar.progress(50)
                
                # Verificar cancelaci√≥n
                if st.session_state.processing_cancelled:
                    st.session_state.processing_cancelled = False
                    st.stop()
                
                # Paso 3: Preparaci√≥n del prompt
                status_text.text("üìù Preparando consulta para la IA...")
                progress_bar.progress(70)
                
                # NUEVO: Seleccionar tipo de consulta (autom√°tico o manual)
                selected_prompt_type = st.session_state.get('chat_prompt_type', 'auto')
                
                if selected_prompt_type == 'auto':
                    # Detecci√≥n autom√°tica
                    query_type = PromptTemplates.detect_query_type(prompt)
                else:
                    # Usar tipo seleccionado manualmente
                    query_type = selected_prompt_type
                
                # Mostrar tipo detectado/seleccionado en modo debug
                if debug_mode:
                    type_names = {
                        'default': 'General',
                        'summary': 'Resumen',
                        'explain': 'Explicaci√≥n',
                        'compare': 'Comparaci√≥n',
                        'extract': 'Extracci√≥n',
                        'analytical': 'An√°lisis'
                    }
                    detected_type = type_names.get(query_type, 'General')
                    mode_text = "detectado" if selected_prompt_type == 'auto' else "seleccionado"
                    status_text.text(f"üìù Tipo {mode_text}: {detected_type} | Preparando consulta...")
                
                # Construir el prompt usando el sistema de plantillas
                if use_rag:
                    if context:
                        # Usar plantilla seg√∫n el tipo detectado
                        full_prompt = PromptTemplates.get_prompt(query_type, prompt, context)
                    else:
                        # Sin documentos cargados
                        full_prompt = PromptTemplates.get_prompt('no_docs', prompt)
                else:
                    # RAG deshabilitado
                    full_prompt = PromptTemplates.get_prompt('no_rag', prompt)
                
                # Verificar cancelaci√≥n antes de generar
                if st.session_state.processing_cancelled:
                    st.session_state.processing_cancelled = False
                    st.stop()
                
                # Paso 4: Generaci√≥n de respuesta
                status_text.text("ü§ñ Generando respuesta con IA...")
                progress_bar.progress(85)
                
                # Generar respuesta con configuraci√≥n de tokens
                response = ollama_client.generate_response(
                    model=selected_model,
                    prompt=full_prompt,
                    context=None,  # El contexto ya est√° incluido en el prompt
                    max_tokens=max_tokens
                )
                
                # Verificar cancelaci√≥n despu√©s de generar
                if st.session_state.processing_cancelled:
                    st.session_state.processing_cancelled = False
                    st.stop()
                
                # Paso 5: Finalizaci√≥n
                status_text.text("‚úÖ Respuesta generada exitosamente")
                progress_bar.progress(100)
                time.sleep(0.5)
                
                # Limpiar el contenedor de progreso y mostrar respuesta
                progress_container.empty()
                
                if response:
                    with response_container.container():
                        st.markdown(response)
                        response_timestamp = datetime.now().strftime("%H:%M:%S")
                        st.caption(f"üïí {response_timestamp}")
                        
                        # Mostrar informaci√≥n de contexto si est√° disponible
                        if use_rag and context:
                            if debug_mode:
                                st.success(f"üß† Contexto recuperado con trazabilidad (Query ID: {query_id})")
                                st.info(f"üìä {len(relevant_chunks)} chunks relevantes encontrados ({len(context):,} caracteres)")
                            else:
                                st.success("üß† Contexto COMPLETO de todos los documentos utilizado")
                                st.info(f"üìä Analizados {len(context):,} caracteres de contenido")
                        
                        # Mostrar informaci√≥n de debug si est√° habilitado
                        if debug_mode and use_rag and context:
                            with st.expander("üîç Informaci√≥n de Debug", expanded=False):
                                col1, col2 = st.columns(2)
                                
                                with col1:
                                    st.subheader("üìä Estad√≠sticas")
                                    st.write(f"**Query ID:** {query_id}")
                                    st.write(f"**Chunks recuperados:** {len(relevant_chunks)}")
                                    st.write(f"**Caracteres de contexto:** {len(context):,}")
                                    st.write(f"**Tokens m√°ximos:** {max_tokens}")
                                    st.write(f"**Modelo usado:** {selected_model}")
                                
                                with col2:
                                    st.subheader("üìö Documentos fuente")
                                    if relevant_chunks:
                                        sources = set()
                                        for chunk_data in relevant_chunks:
                                            # relevant_chunks es una lista de tuplas (DocumentChunk, float)
                                            if isinstance(chunk_data, tuple) and len(chunk_data) >= 2:
                                                chunk, score = chunk_data
                                                if hasattr(chunk, 'source_file'):
                                                    sources.add(chunk.source_file)
                                            elif hasattr(chunk_data, 'source_file'):
                                                sources.add(chunk_data.source_file)
                                        for source in sources:
                                            st.write(f"‚Ä¢ {source}")
                                    else:
                                        st.write("No hay chunks espec√≠ficos (contexto completo)")
                                
                                if relevant_chunks:
                                    st.subheader("üîç Chunks recuperados")
                                    for i, chunk_data in enumerate(relevant_chunks[:5]):  # Mostrar solo los primeros 5
                                        # Extraer chunk y score de la tupla
                                        if isinstance(chunk_data, tuple) and len(chunk_data) >= 2:
                                            chunk, score = chunk_data
                                            content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                                            source = chunk.source_file if hasattr(chunk, 'source_file') else 'N/A'
                                            score_display = f"{score:.3f}" if isinstance(score, float) else str(score)
                                        else:
                                            content = str(chunk_data)
                                            source = 'N/A'
                                            score_display = 'N/A'
                                        
                                        with st.expander(f"Chunk {i+1} (Score: {score_display})"):
                                            st.write(f"**Fuente:** {source}")
                                            st.write(f"**Contenido:** {content[:300]}...")
                        
                        # Registrar en el historial con trazabilidad si est√° habilitado
                        message_data = {
                            "role": "assistant",
                            "content": response,
                            "timestamp": response_timestamp,
                            "context_used": context if context else None
                        }
                        
                        # Agregar informaci√≥n de debug al historial
                        if debug_mode and query_id:
                            message_data.update({
                                "query_id": query_id,
                                "chunks_count": len(relevant_chunks),
                                "model_used": selected_model,
                                "max_tokens": max_tokens
                            })
                        
                        st.session_state.messages.append(message_data)
                        
                        # Registrar en el sistema de historial si est√° habilitado
                        if config.ENABLE_HISTORY_TRACKING:
                            try:
                                traceability_manager = TraceabilityManager()
                                
                                # Preparar datos para el historial
                                sources = []
                                if relevant_chunks:
                                    for chunk_data in relevant_chunks:
                                        # relevant_chunks es una lista de tuplas (DocumentChunk, float)
                                        if isinstance(chunk_data, tuple) and len(chunk_data) >= 2:
                                            chunk, score = chunk_data
                                            if hasattr(chunk, 'source_file'):
                                                sources.append(chunk.source_file)
                                        elif hasattr(chunk_data, 'source_file'):
                                            sources.append(chunk_data.source_file)
                                    sources = list(set(sources))  # Eliminar duplicados
                                
                                traceability_manager.save_query_history(
                                    query=prompt,
                                    response=response,
                                    query_id=query_id,
                                    sources=sources,
                                    chunks_count=len(relevant_chunks),
                                    model_used=selected_model,
                                    max_tokens=max_tokens,
                                    rag_enabled=use_rag
                                )
                            except Exception as e:
                                logger.warning(f"Error al guardar historial: {e}")
                else:
                    with response_container.container():
                        st.error("‚ùå No se pudo generar una respuesta")
                        
            except Exception as e:
                # Limpiar contenedores en caso de error
                progress_container.empty()
                with response_container.container():
                    error_handler.handle_error(e, "Error al generar respuesta del chat")
                    st.error("‚ùå Ha ocurrido un error al generar la respuesta")
            
            finally:
                # Limpiar estado de cancelaci√≥n
                if 'processing_cancelled' in st.session_state:
                    st.session_state.processing_cancelled = False
    
    # Panel de control del chat
    if st.session_state.messages:
        st.divider()
        
        # M√©tricas de la conversaci√≥n
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            user_messages = len([m for m in st.session_state.messages if m["role"] == "user"])
            st.metric("üí¨ Mensajes enviados", user_messages)
        
        with col2:
            assistant_messages = len([m for m in st.session_state.messages if m["role"] == "assistant"])
            st.metric("ü§ñ Respuestas recibidas", assistant_messages)
        
        with col3:
            rag_responses = len([m for m in st.session_state.messages if m["role"] == "assistant" and m.get("context_used")])
            st.metric("üìö Respuestas con RAG", rag_responses)
        
        with col4:
            if st.button("üßπ Limpiar Chat", type="secondary"):
                st.session_state.messages = []
                st.rerun()
        
        # Controles de exportaci√≥n de conversaci√≥n completa
        st.divider()
        render_conversation_export_controls()
    
    # =============================================================================
    # SUGERENCIAS Y CHAT INPUT - SIEMPRE AL FINAL
    # =============================================================================
    
    # Sugerencias inteligentes de preguntas (solo si no hay mensajes)
    if not st.session_state.messages and use_rag:
        st.markdown("### üí° Sugerencias de Preguntas:")
        st.caption("Haz clic en una sugerencia o escribe tu propia pregunta")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("üìä ¬øCu√°les son los puntos principales?", key="sug_1", use_container_width=True):
                st.session_state.suggested_prompt = "¬øCu√°les son los puntos principales del documento?"
                st.rerun()
            
            if st.button("üîç Explica los conceptos clave", key="sug_2", use_container_width=True):
                st.session_state.suggested_prompt = "Expl√≠came los conceptos clave del documento de forma clara"
                st.rerun()
        
        with col2:
            if st.button("üìù Resume en 3 p√°rrafos", key="sug_3", use_container_width=True):
                st.session_state.suggested_prompt = "Resume el contenido en 3 p√°rrafos"
                st.rerun()
            
            if st.button("üéØ Conclusiones importantes", key="sug_4", use_container_width=True):
                st.session_state.suggested_prompt = "¬øQu√© conclusiones importantes hay en el documento?"
                st.rerun()
        
        with col3:
            if st.button("üí° Dame insights interesantes", key="sug_5", use_container_width=True):
                st.session_state.suggested_prompt = "Dame los insights m√°s interesantes del documento"
                st.rerun()
            
            if st.button("üìà Analiza en detalle", key="sug_6", use_container_width=True):
                st.session_state.suggested_prompt = "Analiza el documento en detalle y proporciona un an√°lisis completo"
                st.rerun()
    
    # Renderizar sidebar de historial
    render_history_sidebar()
    
    # =============================================================================
    # CHAT INPUT - SIEMPRE AL FINAL ABSOLUTO
    # =============================================================================
    
    # Forzar que el chat input aparezca al final usando un contenedor especial
    st.markdown("---")  # Separador visual
    
    # Crear un contenedor para el chat input al final
    with st.container():
        st.markdown("")  # Espacio en blanco para empujar hacia abajo
        st.markdown("")  # M√°s espacio
        
        # Chat input al final absoluto
        final_chat_input = st.chat_input("üí≠ Escribe tu mensaje aqu√≠...")
        
        # Procesar input del chat final si existe
        if final_chat_input:
            # Agregar mensaje del usuario
            timestamp = datetime.now().strftime("%H:%M:%S")
            st.session_state.messages.append({
                "role": "user", 
                "content": final_chat_input,
                "timestamp": timestamp
            })
            
            # Mostrar mensaje del usuario
            with st.chat_message("user"):
                st.markdown(final_chat_input)
                st.caption(f"üïí {timestamp}")
            
            # Generar respuesta con interfaz mejorada
            with st.chat_message("assistant"):
                # Crear contenedor para el progreso
                progress_container = st.empty()
                response_container = st.empty()
                
                # Inicializar variables de control
                if 'processing_cancelled' not in st.session_state:
                    st.session_state.processing_cancelled = False
                
                try:
                    # Paso 1: Inicializaci√≥n
                    with progress_container.container():
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                            status_text.text("üöÄ Iniciando procesamiento...")
                        with col2:
                            if st.button("‚ùå Cancelar", key="cancel_processing_final", type="secondary"):
                                st.session_state.processing_cancelled = True
                                st.warning("‚ö†Ô∏è Procesamiento cancelado por el usuario")
                                st.stop()
                    
                    progress_bar.progress(10)
                    time.sleep(0.5)
                    
                    # Verificar cancelaci√≥n
                    if st.session_state.processing_cancelled:
                        st.session_state.processing_cancelled = False
                        st.stop()
                    
                    # Obtener configuraci√≥n de debug
                    debug_mode = st.session_state.get('debug_mode', False)
                    
                    # Paso 2: Procesamiento RAG
                    status_text.text("üîç Analizando documentos...")
                    progress_bar.progress(25)
                    
                    context = None
                    relevant_chunks = []
                    query_id = None
                    
                    if use_rag:
                        if debug_mode:
                            # Modo debug: usar trazabilidad
                            context, relevant_chunks, query_id = get_rag_context(final_chat_input, enable_tracing=True)
                        else:
                            # Modo normal: usar contexto completo
                            context = get_rag_context(final_chat_input, enable_tracing=False)
                        
                        progress_bar.progress(50)
                        
                        if context:
                            if debug_mode:
                                status_text.text(f"‚úÖ Contexto recuperado - {len(relevant_chunks)} chunks encontrados")
                            else:
                                status_text.text(f"‚úÖ Contexto completo cargado - {len(context):,} caracteres")
                        else:
                            status_text.text("‚ö†Ô∏è No hay documentos procesados disponibles")
                    else:
                        status_text.text("‚ÑπÔ∏è Modo sin RAG - Respuesta general")
                        progress_bar.progress(50)
                    
                    # Verificar cancelaci√≥n
                    if st.session_state.processing_cancelled:
                        st.session_state.processing_cancelled = False
                        st.stop()
                    
                    # Paso 3: Preparaci√≥n del prompt
                    status_text.text("üìù Preparando consulta para la IA...")
                    progress_bar.progress(70)
                    
                    # Seleccionar tipo de consulta (autom√°tico o manual)
                    selected_prompt_type = st.session_state.get('chat_prompt_type', 'auto')
                    
                    if selected_prompt_type == 'auto':
                        # Detecci√≥n autom√°tica
                        query_type = PromptTemplates.detect_query_type(final_chat_input)
                    else:
                        # Usar tipo seleccionado manualmente
                        query_type = selected_prompt_type
                    
                    # Mostrar tipo detectado/seleccionado en modo debug
                    if debug_mode:
                        type_names = {
                            'default': 'General',
                            'summary': 'Resumen',
                            'explain': 'Explicaci√≥n',
                            'compare': 'Comparaci√≥n',
                            'extract': 'Extracci√≥n',
                            'analytical': 'An√°lisis'
                        }
                        detected_type = type_names.get(query_type, 'General')
                        mode_text = "detectado" if selected_prompt_type == 'auto' else "seleccionado"
                        status_text.text(f"üìù Tipo {mode_text}: {detected_type} | Preparando consulta...")
                    
                    # Construir el prompt usando el sistema de plantillas
                    if use_rag:
                        if context:
                            # Usar plantilla seg√∫n el tipo detectado
                            full_prompt = PromptTemplates.get_prompt(query_type, final_chat_input, context)
                        else:
                            # Sin documentos cargados
                            full_prompt = PromptTemplates.get_prompt('no_docs', final_chat_input)
                    else:
                        # RAG deshabilitado
                        full_prompt = PromptTemplates.get_prompt('no_rag', final_chat_input)
                    
                    # Verificar cancelaci√≥n antes de generar
                    if st.session_state.processing_cancelled:
                        st.session_state.processing_cancelled = False
                        st.stop()
                    
                    # Paso 4: Generaci√≥n de respuesta
                    status_text.text("ü§ñ Generando respuesta con IA...")
                    progress_bar.progress(85)
                    
                    # Generar respuesta con configuraci√≥n de tokens
                    response = ollama_client.generate_response(
                        model=selected_model,
                        prompt=full_prompt,
                        context=None,  # El contexto ya est√° incluido en el prompt
                        max_tokens=max_tokens
                    )
                    
                    # Verificar cancelaci√≥n despu√©s de generar
                    if st.session_state.processing_cancelled:
                        st.session_state.processing_cancelled = False
                        st.stop()
                    
                    # Paso 5: Finalizaci√≥n
                    status_text.text("‚úÖ Respuesta generada exitosamente")
                    progress_bar.progress(100)
                    time.sleep(0.5)
                    
                    # Limpiar el contenedor de progreso y mostrar respuesta
                    progress_container.empty()
                    
                    if response:
                        with response_container.container():
                            st.markdown(response)
                            response_timestamp = datetime.now().strftime("%H:%M:%S")
                            st.caption(f"üïí {response_timestamp}")
                            
                            # Mostrar informaci√≥n de contexto si est√° disponible
                            if use_rag and context:
                                if debug_mode:
                                    st.success(f"üß† Contexto recuperado con trazabilidad (Query ID: {query_id})")
                                    st.info(f"üìä {len(relevant_chunks)} chunks relevantes encontrados ({len(context):,} caracteres)")
                                else:
                                    st.success("üß† Contexto COMPLETO de todos los documentos utilizado")
                                    st.info(f"üìä Analizados {len(context):,} caracteres de contenido")
                            
                            # Mostrar informaci√≥n de debug si est√° habilitado
                            if debug_mode and use_rag and context:
                                with st.expander("üîç Informaci√≥n de Debug", expanded=False):
                                    col1, col2 = st.columns(2)
                                    
                                    with col1:
                                        st.subheader("üìä Estad√≠sticas")
                                        st.write(f"**Query ID:** {query_id}")
                                        st.write(f"**Chunks recuperados:** {len(relevant_chunks)}")
                                        st.write(f"**Caracteres de contexto:** {len(context):,}")
                                        st.write(f"**Tokens m√°ximos:** {max_tokens}")
                                        st.write(f"**Modelo usado:** {selected_model}")
                                    
                                    with col2:
                                        st.subheader("üìö Documentos fuente")
                                        if relevant_chunks:
                                            sources = set()
                                            for chunk_data in relevant_chunks:
                                                # relevant_chunks es una lista de tuplas (DocumentChunk, float)
                                                if isinstance(chunk_data, tuple) and len(chunk_data) >= 2:
                                                    chunk, score = chunk_data
                                                    if hasattr(chunk, 'source_file'):
                                                        sources.add(chunk.source_file)
                                                elif hasattr(chunk_data, 'source_file'):
                                                    sources.add(chunk_data.source_file)
                                            for source in sources:
                                                st.write(f"‚Ä¢ {source}")
                                        else:
                                            st.write("No hay chunks espec√≠ficos (contexto completo)")
                                    
                                    if relevant_chunks:
                                        st.subheader("üîç Chunks recuperados")
                                        for i, chunk_data in enumerate(relevant_chunks[:5]):  # Mostrar solo los primeros 5
                                            # Extraer chunk y score de la tupla
                                            if isinstance(chunk_data, tuple) and len(chunk_data) >= 2:
                                                chunk, score = chunk_data
                                                content = chunk.content if hasattr(chunk, 'content') else str(chunk)
                                                source = chunk.source_file if hasattr(chunk, 'source_file') else 'N/A'
                                                score_display = f"{score:.3f}" if isinstance(score, float) else str(score)
                                            else:
                                                content = str(chunk_data)
                                                source = 'N/A'
                                                score_display = 'N/A'
                                            
                                            with st.expander(f"Chunk {i+1} (Score: {score_display})"):
                                                st.write(f"**Fuente:** {source}")
                                                st.write(f"**Contenido:** {content[:300]}...")
                            
                            # Registrar en el historial con trazabilidad si est√° habilitado
                            message_data = {
                                "role": "assistant",
                                "content": response,
                                "timestamp": response_timestamp,
                                "context_used": context if context else None
                            }
                            
                            # Agregar informaci√≥n de debug al historial
                            if debug_mode and query_id:
                                message_data.update({
                                    "query_id": query_id,
                                    "chunks_count": len(relevant_chunks),
                                    "model_used": selected_model,
                                    "max_tokens": max_tokens
                                })
                            
                            st.session_state.messages.append(message_data)
                            
                            # Registrar en el sistema de historial si est√° habilitado
                            if config.ENABLE_HISTORY_TRACKING:
                                try:
                                    traceability_manager = TraceabilityManager()
                                    
                                    # Preparar datos para el historial
                                    sources = []
                                    if relevant_chunks:
                                        for chunk_data in relevant_chunks:
                                            # relevant_chunks es una lista de tuplas (DocumentChunk, float)
                                            if isinstance(chunk_data, tuple) and len(chunk_data) >= 2:
                                                chunk, score = chunk_data
                                                if hasattr(chunk, 'source_file'):
                                                    sources.append(chunk.source_file)
                                            elif hasattr(chunk_data, 'source_file'):
                                                sources.append(chunk_data.source_file)
                                        sources = list(set(sources))  # Eliminar duplicados
                                    
                                    traceability_manager.save_query_history(
                                        query=final_chat_input,
                                        response=response,
                                        query_id=query_id,
                                        sources=sources,
                                        chunks_count=len(relevant_chunks),
                                        model_used=selected_model,
                                        max_tokens=max_tokens,
                                        rag_enabled=use_rag
                                    )
                                except Exception as e:
                                    logger.warning(f"Error al guardar historial: {e}")
                    else:
                        with response_container.container():
                            st.error("‚ùå No se pudo generar una respuesta")
                            
                except Exception as e:
                    # Limpiar contenedores en caso de error
                    progress_container.empty()
                    with response_container.container():
                        error_handler.handle_error(e, "Error al generar respuesta del chat")
                        st.error("‚ùå Ha ocurrido un error al generar la respuesta")
                
                finally:
                    # Limpiar estado de cancelaci√≥n
                    if 'processing_cancelled' in st.session_state:
                        st.session_state.processing_cancelled = False
            
            # Forzar rerun para mostrar la nueva respuesta
            st.rerun()

# =============================================================================
# GESTI√ìN DE HISTORIAL
# =============================================================================
# Esta secci√≥n maneja el guardado, carga y eliminaci√≥n de conversaciones.
# MODIFICAR AQU√ç PARA:
# - Cambiar formato de almacenamiento
# - Modificar interfaz del sidebar
# - Ajustar gesti√≥n de conversaciones guardadas

def render_history_sidebar():
    """Renderizar sidebar para gesti√≥n de historial"""
    with st.sidebar:
        st.header("üìö Historial de Conversaciones")
        
        # Guardar conversaci√≥n actual
        if st.session_state.messages:
            st.subheader("üíæ Guardar Conversaci√≥n")
            
            conversation_name = st.text_input(
                "Nombre de la conversaci√≥n",
                placeholder="Ej: An√°lisis de datos 2024",
                key="conversation_name_input"
            )
            
            if st.button("üíæ Guardar Conversaci√≥n Actual", type="primary"):
                try:
                    filename = chat_history_manager.save_conversation(
                        st.session_state.messages,
                        conversation_name if conversation_name else None
                    )
                    st.success(f"‚úÖ Conversaci√≥n guardada: {filename}")
                    # Usar st.rerun() en lugar de modificar directamente el session_state
                    st.rerun()
                except Exception as e:
                    st.error(f"‚ùå Error al guardar: {str(e)}")
        
        st.divider()
        
        # Cargar conversaciones guardadas
        st.subheader("üìÇ Conversaciones Guardadas")
        
        conversations = chat_history_manager.list_conversations()
        
        if conversations:
            for conv in conversations:
                with st.expander(f"üìÑ {conv['name']}", expanded=False):
                    st.write(f"**Fecha:** {conv.get('created_at', 'N/A')[:19]}")
                    st.write(f"**Mensajes:** {conv.get('total_messages', 0)}")
                    st.write(f"**Tama√±o:** {conv.get('file_size', 0)} bytes")
                    
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        if st.button(f"üì• Cargar", key=f"load_{conv['filename']}"):
                            try:
                                conversation_data = chat_history_manager.load_conversation(conv['filename'])
                                if conversation_data:
                                    st.session_state.messages = conversation_data['messages']
                                    st.success(f"‚úÖ Conversaci√≥n cargada: {conv['name']}")
                                    st.rerun()
                                else:
                                    st.error("‚ùå Error al cargar la conversaci√≥n")
                            except Exception as e:
                                st.error(f"‚ùå Error: {str(e)}")
                    
                    with col2:
                        if st.button(f"üóëÔ∏è Eliminar", key=f"delete_{conv['filename']}"):
                            if chat_history_manager.delete_conversation(conv['filename']):
                                st.success(f"‚úÖ Conversaci√≥n eliminada")
                                st.rerun()
                            else:
                                st.error("‚ùå Error al eliminar")
        else:
            st.info("No hay conversaciones guardadas")

# =============================================================================
# EXPORTACI√ìN Y GUARDADO
# =============================================================================
# Esta secci√≥n maneja la exportaci√≥n de mensajes y conversaciones.
# MODIFICAR AQU√ç PARA:
# - Agregar nuevos formatos de exportaci√≥n
# - Modificar contenido de los archivos exportados
# - Ajustar dise√±o de botones de exportaci√≥n

def render_message_export_buttons(message: dict, message_index: int):
    """Renderizar botones de exportaci√≥n para un mensaje individual"""
    col1, col2 = st.columns([1, 1])
    
    with col1:
        if st.button("üìÑ DOCX", key=f"docx_{message_index}", help="Exportar a Word"):
            try:
                buffer = chat_exporter.export_message_to_docx(message)
                if buffer:
                    timestamp = message.get('timestamp', 'mensaje').replace(':', '-')
                    filename = f"cognichat_mensaje_{timestamp}.docx"
                    
                    st.download_button(
                        label="‚¨áÔ∏è Descargar DOCX",
                        data=buffer.getvalue(),
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        key=f"download_docx_{message_index}"
                    )
                else:
                    st.error("‚ùå Error al generar DOCX")
            except ImportError:
                st.error("‚ùå python-docx no est√° instalado")
            except Exception as e:
                st.error(f"‚ùå Error: {str(e)}")
    
    with col2:
        if st.button("üìã Copiar", key=f"copy_{message_index}", help="Copiar al portapapeles"):
            try:
                if CLIPBOARD_AVAILABLE:
                    text = chat_exporter.get_message_text_for_clipboard(message)
                    pyperclip.copy(text)
                    st.success("‚úÖ Copiado al portapapeles")
                else:
                    # Fallback: mostrar texto para copiar manualmente
                    text = chat_exporter.get_message_text_for_clipboard(message)
                    st.text_area(
                        "Copiar manualmente:",
                        value=text,
                        height=100,
                        key=f"manual_copy_{message_index}"
                    )
            except Exception as e:
                st.error(f"‚ùå Error: {str(e)}")

def render_conversation_export_controls():
    """Renderizar controles de exportaci√≥n de conversaci√≥n completa"""
    st.subheader("üì§ Exportar Conversaci√≥n Completa")
    
    if not st.session_state.messages:
        st.info("No hay mensajes para exportar")
        return
    
    col1, col2, col3 = st.columns([2, 2, 1])
    
    with col1:
        if st.button("üìÑ Exportar a DOCX", type="secondary"):
            try:
                buffer = chat_exporter.export_conversation_to_docx(st.session_state.messages)
                if buffer:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    filename = f"cognichat_conversacion_{timestamp}.docx"
                    
                    st.download_button(
                        label="‚¨áÔ∏è Descargar Conversaci√≥n DOCX",
                        data=buffer.getvalue(),
                        file_name=filename,
                        mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
                        key="download_conversation_docx"
                    )
                else:
                    st.error("‚ùå Error al generar DOCX")
            except ImportError:
                st.error("‚ùå python-docx no est√° instalado")
            except Exception as e:
                st.error(f"‚ùå Error: {str(e)}")
    
    with col2:
        if st.button("üìã Copiar Todo", type="secondary"):
            try:
                if CLIPBOARD_AVAILABLE:
                    # Crear texto completo de la conversaci√≥n
                    conversation_text = []
                    conversation_text.append("=" * 60)
                    conversation_text.append("CONVERSACI√ìN COMPLETA DE COGNICHAT")
                    conversation_text.append(f"Exportado el: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                    conversation_text.append("=" * 60)
                    conversation_text.append("")
                    
                    for i, message in enumerate(st.session_state.messages, 1):
                        role_text = "Usuario" if message['role'] == 'user' else "Asistente"
                        timestamp = message.get('timestamp', 'N/A')
                        
                        conversation_text.append(f"--- MENSAJE {i} - {role_text} ({timestamp}) ---")
                        conversation_text.append(message['content'])
                        conversation_text.append("")
                    
                    conversation_text.append("=" * 60)
                    
                    full_text = "\n".join(conversation_text)
                    pyperclip.copy(full_text)
                    st.success("‚úÖ Conversaci√≥n copiada al portapapeles")
                else:
                    st.error("‚ùå pyperclip no est√° disponible")
            except Exception as e:
                st.error(f"‚ùå Error: {str(e)}")
    
    with col3:
        # Estad√≠sticas de la conversaci√≥n
        total_chars = sum(len(m['content']) for m in st.session_state.messages)
        estimated_tokens = total_chars // 4  # Estimaci√≥n aproximada
        
        st.metric("üìä Caracteres totales", f"{total_chars:,}")
        st.metric("üéØ Tokens estimados", f"{estimated_tokens:,}")

# =============================================================================
# GESTI√ìN DE CONTEXTO RAG
# =============================================================================
# Esta secci√≥n maneja la obtenci√≥n de contexto RAG para las consultas.
# MODIFICAR AQU√ç PARA:
# - Cambiar estrategia de recuperaci√≥n de contexto
# - Ajustar par√°metros de b√∫squeda
# - Modificar formato de contexto

def get_rag_context(query: str, enable_tracing: bool = False):
    """
    Obtiene el contexto RAG para una consulta espec√≠fica.
    
    Args:
        query: La consulta del usuario
        enable_tracing: Si habilitar el trazado de chunks
        
    Returns:
        tuple: (contexto, chunks_relevantes, query_id) si enable_tracing es True
               str: contexto si enable_tracing es False
    """
    try:
        
        if enable_tracing:
            # Usar la funci√≥n optimizada con trazabilidad
            context, relevant_chunks, query_id = rag_processor.get_context_for_query(
                query, enable_tracing=True
            )
            return context, relevant_chunks, query_id
        else:
            # Usar el contexto completo como antes
            context = rag_processor.get_full_context_for_comprehensive_analysis()
            
            if context:
                logger.info(f"Contexto COMPLETO generado para DeepSeek: {query[:50]}... ({len(context)} caracteres)")
                return context
            else:
                logger.info("No se encontraron documentos procesados")
                return None
        
    except Exception as e:
        logger.error(f"Error al obtener contexto completo: {e}")
        return None if not enable_tracing else (None, [], None)