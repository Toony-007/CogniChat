"""
Extractor Inteligente de Conceptos Clave
Sistema dise√±ado para identificar y analizar conceptos principales en documentos de investigaci√≥n

Este m√≥dulo NO copia y pega informaci√≥n, sino que:
1. Analiza el contenido completo
2. Identifica patrones y frecuencias
3. Sintetiza conceptos clave
4. Proporciona contexto y fundamentaci√≥n
"""

from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass, field
from collections import Counter, defaultdict
import re
from datetime import datetime

# Importaciones para NLP
try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    import numpy as np
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False

try:
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize
    NLTK_AVAILABLE = True
except ImportError:
    NLTK_AVAILABLE = False

from ..core.citation_manager import CitationManager, Citation
from ..core.config import AnalysisConfig
from utils.ollama_client import ollama_client


@dataclass
class ExtractedConcept:
    """
    Representa un concepto extra√≠do del an√°lisis
    
    Cada concepto incluye:
    - El t√©rmino o frase identificada
    - M√©tricas de relevancia
    - Citas a las fuentes originales
    - Contexto de aparici√≥n
    """
    
    concept: str
    """El concepto identificado (palabra o frase)"""
    
    frequency: int
    """N√∫mero de veces que aparece en los documentos"""
    
    relevance_score: float
    """Score de relevancia (0.0 a 1.0) calculado por TF-IDF"""
    
    sources: List[str] = field(default_factory=list)
    """Lista de fuentes donde aparece el concepto"""
    
    citations: List[Citation] = field(default_factory=list)
    """Citas espec√≠ficas donde aparece"""
    
    context_examples: List[str] = field(default_factory=list)
    """Ejemplos de contexto donde aparece el concepto"""
    
    related_concepts: List[str] = field(default_factory=list)
    """Conceptos relacionados que co-ocurren"""
    
    category: Optional[str] = None
    """Categor√≠a del concepto (si se ha clasificado)"""
    
    extraction_method: str = "tfidf"
    """M√©todo utilizado para extraer el concepto"""
    
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    """Momento de extracci√≥n"""
    
    def to_dict(self) -> Dict[str, Any]:
        """Convertir a diccionario"""
        return {
            'concept': self.concept,
            'frequency': self.frequency,
            'relevance_score': self.relevance_score,
            'sources': self.sources,
            'num_citations': len(self.citations),
            'context_examples': self.context_examples,
            'related_concepts': self.related_concepts,
            'category': self.category,
            'extraction_method': self.extraction_method,
            'timestamp': self.timestamp
        }
    
    def get_first_citation(self) -> Optional[Citation]:
        """Obtener la primera cita (m√°s relevante)"""
        return self.citations[0] if self.citations else None
    
    def get_source_distribution(self) -> Dict[str, int]:
        """Obtener distribuci√≥n del concepto por fuente"""
        distribution = Counter(self.sources)
        return dict(distribution)
    
    def __repr__(self) -> str:
        return f"ExtractedConcept('{self.concept}', freq={self.frequency}, score={self.relevance_score:.3f})"


class ConceptExtractor:
    """
    Extractor Inteligente de Conceptos Clave
    
    Este sistema analiza documentos para identificar los conceptos m√°s importantes,
    proporcionando fundamentaci√≥n completa con citas a las fuentes originales.
    
    Caracter√≠sticas principales:
    1. Procesamiento inteligente con TF-IDF
    2. Detecci√≥n de n-gramas (frases completas)
    3. Sistema de citaci√≥n integrado
    4. An√°lisis de co-ocurrencia
    5. Contexto de cada concepto
    
    Ejemplo de uso:
        extractor = ConceptExtractor(config)
        concepts = extractor.extract_concepts(chunks)
        
        # Ver conceptos con sus fuentes
        for concept in concepts[:10]:
            print(f"{concept.concept}: {concept.frequency} ocurrencias")
            print(f"  Fuentes: {', '.join(concept.sources)}")
            if concept.citations:
                citation = concept.get_first_citation()
                print(f"  Cita: {citation.format_citation()}")
    """
    
    def __init__(self, config: Optional[AnalysisConfig] = None):
        """
        Inicializar el extractor
        
        Args:
            config: Configuraci√≥n del an√°lisis (usa valores por defecto si no se proporciona)
        """
        self.config = config or AnalysisConfig()
        self.citation_manager = CitationManager()
        self.stopwords = self._load_stopwords()
    
    def _load_stopwords(self) -> set:
        """
        Cargar stopwords en espa√±ol
        
        Returns:
            Conjunto de palabras vac√≠as a ignorar
        """
        if NLTK_AVAILABLE:
            try:
                return set(stopwords.words('spanish'))
            except LookupError:
                # Descargar stopwords si no est√°n disponibles
                try:
                    nltk.download('stopwords', quiet=True)
                    nltk.download('punkt', quiet=True)
                    return set(stopwords.words('spanish'))
                except:
                    pass
        
        # Stopwords b√°sicas en espa√±ol si NLTK no est√° disponible
        return {
            'el', 'la', 'de', 'que', 'y', 'a', 'en', 'un', 'ser', 'se', 'no', 'haber',
            'por', 'con', 'su', 'para', 'como', 'estar', 'tener', 'le', 'lo', 'todo',
            'pero', 'm√°s', 'hacer', 'o', 'poder', 'decir', 'este', 'ir', 'otro', 'ese',
            'la', 'si', 'me', 'ya', 'ver', 'porque', 'dar', 'cuando', '√©l', 'muy',
            'sin', 'vez', 'mucho', 'saber', 'qu√©', 'sobre', 'mi', 'alguno', 'mismo',
            'yo', 'tambi√©n', 'hasta', 'a√±o', 'dos', 'querer', 'entre', 'as√≠', 'primero',
            'desde', 'grande', 'eso', 'ni', 'nos', 'llegar', 'pasar', 'tiempo', 'ella',
            's√≠', 'd√≠a', 'uno', 'bien', 'poco', 'deber', 'entonces', 'poner', 'cosa',
            'tanto', 'hombre', 'parecer', 'nuestro', 'tan', 'donde', 'ahora', 'parte',
            'despu√©s', 'vida', 'quedar', 'siempre', 'creer', 'hablar', 'llevar', 'dejar',
            'nada', 'cada', 'seguir', 'menos', 'nuevo', 'encontrar', 'algo', 'solo',
            'decir', 'mundo', 'casa', 'usar', 'salir', 'volver', 'tomar', 'conocer',
            'durante', '√∫ltimo', 'llamar', 'empezar', 'menos', 'dios', 'hecho', 'casi',
            'momento', 'trav√©s', 'ser', 'estar', 'haber', 'hacer', 'poder', 'tener'
        }
    
    def _preprocess_text(self, text: str) -> str:
        """
        Preprocesar texto para an√°lisis
        
        Args:
            text: Texto original
            
        Returns:
            Texto preprocesado
        """
        # Convertir a min√∫sculas
        text = text.lower()
        
        # Eliminar URLs
        text = re.sub(r'http\S+|www\S+', '', text)
        
        # Eliminar emails
        text = re.sub(r'\S+@\S+', '', text)
        
        # Eliminar n√∫meros standalone (pero mantener en palabras)
        text = re.sub(r'\b\d+\b', '', text)
        
        # Normalizar espacios
        text = re.sub(r'\s+', ' ', text)
        
        return text.strip()
    
    def _extract_with_tfidf(
        self,
        chunks: List[Dict[str, Any]]
    ) -> Tuple[List[ExtractedConcept], List[str]]:
        """
        Extraer conceptos usando TF-IDF (Term Frequency-Inverse Document Frequency)
        
        TF-IDF identifica t√©rminos que son:
        - Frecuentes en un documento espec√≠fico (TF)
        - Raros en el corpus general (IDF)
        
        Esto nos permite encontrar conceptos que son importantes y distintivos.
        
        Args:
            chunks: Lista de chunks de documentos
            
        Returns:
            Tupla de (conceptos extra√≠dos, textos procesados)
        """
        if not SKLEARN_AVAILABLE:
            raise ImportError("scikit-learn es requerido para extracci√≥n con TF-IDF")
        
        # Preparar textos
        texts = []
        for chunk in chunks:
            content = chunk.get('content', '')
            processed = self._preprocess_text(content)
            texts.append(processed)
        
        if not texts:
            return [], []
        
        # Configurar vectorizador TF-IDF
        ngram_min, ngram_max = self.config.ngram_range if self.config.use_ngrams else (1, 1)
        
        vectorizer = TfidfVectorizer(
            max_features=self.config.max_concepts * 2,  # Extraer m√°s para luego filtrar
            stop_words=list(self.stopwords),
            ngram_range=(ngram_min, ngram_max),
            min_df=self.config.min_concept_frequency,
            max_df=0.85,  # Ignorar t√©rminos que aparecen en m√°s del 85% de docs
            token_pattern=r'(?u)\b[a-z√°√©√≠√≥√∫√±√º]+\b'  # Solo palabras en espa√±ol
        )
        
        try:
            # Calcular TF-IDF
            tfidf_matrix = vectorizer.fit_transform(texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # Calcular scores m√°ximos para cada t√©rmino (m√°s diferenciados que el promedio)
            max_scores = tfidf_matrix.max(axis=0).toarray().flatten()
            
            # Crear conceptos
            concepts = []
            term_locations = defaultdict(list)  # Para rastrear d√≥nde aparece cada t√©rmino
            
            for idx, (term, score) in enumerate(zip(feature_names, max_scores)):
                if score > 0:
                    # Encontrar en qu√© chunks aparece
                    chunk_indices = tfidf_matrix[:, idx].nonzero()[0]
                    
                    # Calcular frecuencia real
                    frequency = sum(
                        chunks[chunk_idx]['content'].lower().count(term)
                        for chunk_idx in chunk_indices
                    )
                    
                    # Recopilar fuentes
                    sources = list(set(
                        chunks[chunk_idx]['metadata'].get('source_file', 'unknown')
                        for chunk_idx in chunk_indices
                    ))
                    
                    # Crear citas
                    citations = []
                    context_examples = []
                    
                    for chunk_idx in chunk_indices[:5]:  # M√°ximo 5 citas por concepto
                        chunk = chunks[chunk_idx]
                        content = chunk['content']
                        source = chunk['metadata'].get('source_file', 'unknown')
                        
                        # Encontrar posici√≥n del t√©rmino
                        term_pos = content.lower().find(term)
                        if term_pos != -1:
                            # Extraer contexto
                            context_start = max(0, term_pos - self.config.citation_context_chars)
                            context_end = min(len(content), term_pos + len(term) + self.config.citation_context_chars)
                            
                            context_before = content[context_start:term_pos]
                            term_exact = content[term_pos:term_pos + len(term)]
                            context_after = content[term_pos + len(term):context_end]
                            
                            # Crear cita
                            if self.config.enable_citations:
                                citation = self.citation_manager.add_citation(
                                    source_file=source,
                                    chunk_id=chunk_idx,
                                    content=term_exact,
                                    context_before=context_before,
                                    context_after=context_after,
                                    page_number=chunk['metadata'].get('page_number'),
                                    relevance_score=float(score)
                                )
                                citations.append(citation)
                            
                            # Agregar ejemplo de contexto
                            context_example = f"...{context_before}[{term_exact}]{context_after}..."
                            context_examples.append(context_example)
                    
                    # Calcular score de relevancia mejorado
                    # Combinar TF-IDF con frecuencia y distribuci√≥n
                    frequency_score = min(1.0, frequency / 10.0)  # Normalizar frecuencia
                    distribution_score = len(sources) / len(set(chunk['metadata'].get('source_file', 'unknown') for chunk in chunks))
                    combined_score = (score * 0.6 + frequency_score * 0.3 + distribution_score * 0.1)
                    
                    # Crear concepto
                    concept = ExtractedConcept(
                        concept=term,
                        frequency=frequency,
                        relevance_score=combined_score,
                        sources=sources,
                        citations=citations,
                        context_examples=context_examples[:3],  # M√°ximo 3 ejemplos
                        extraction_method='tfidf'
                    )
                    
                    concepts.append(concept)
                    term_locations[term] = [int(i) for i in chunk_indices]
            
            # Ordenar por relevancia
            concepts.sort(key=lambda c: c.relevance_score, reverse=True)
            
            # Limitar a max_concepts
            concepts = concepts[:self.config.max_concepts]
            
            # Identificar conceptos relacionados (co-ocurrencia)
            self._identify_related_concepts(concepts, term_locations)
            
            return concepts, texts
            
        except Exception as e:
            raise Exception(f"Error en extracci√≥n TF-IDF: {str(e)}")
    
    def _identify_related_concepts(
        self,
        concepts: List[ExtractedConcept],
        term_locations: Dict[str, List[int]]
    ):
        """
        Identificar conceptos relacionados bas√°ndose en co-ocurrencia
        
        Args:
            concepts: Lista de conceptos extra√≠dos
            term_locations: Diccionario de t√©rmino -> lista de chunk indices
        """
        for concept in concepts:
            term = concept.concept
            locations = set(term_locations.get(term, []))
            
            if not locations:
                continue
            
            # Encontrar conceptos que co-ocurren
            related = []
            for other_concept in concepts:
                if other_concept.concept == term:
                    continue
                
                other_locations = set(term_locations.get(other_concept.concept, []))
                
                # Calcular co-ocurrencia
                intersection = locations & other_locations
                if len(intersection) >= 2:  # Co-ocurren en al menos 2 chunks
                    jaccard = len(intersection) / len(locations | other_locations)
                    if jaccard > 0.3:  # Umbral de similitud
                        related.append(other_concept.concept)
            
            # Guardar los 5 m√°s relacionados
            concept.related_concepts = related[:5]
    
    def _refine_concepts_with_llm(
        self,
        raw_concepts: List[ExtractedConcept],
        document_context: str
    ) -> List[ExtractedConcept]:
        """
        Refinar conceptos usando modelo LLM (DeepSeek R1)
        
        Este m√©todo toma los conceptos candidatos extra√≠dos por TF-IDF
        y los env√≠a al LLM para:
        1. Filtrar conceptos irrelevantes
        2. Consolidar conceptos relacionados
        3. Generar nombres m√°s precisos
        4. A√±adir explicaciones sem√°nticas
        
        Args:
            raw_concepts: Conceptos candidatos extra√≠dos por TF-IDF
            document_context: Contexto del documento para el LLM
            
        Returns:
            Lista de conceptos refinados y mejorados
        """
        if not self.config.enable_llm_refinement:
            return raw_concepts[:self.config.max_final_concepts]
        
        try:
            # Filtrar conceptos candidatos m√°s prometedores
            # Priorizar conceptos con mayor relevancia y frecuencia
            filtered_concepts = [
                c for c in raw_concepts 
                if len(c.concept) > 3 and  # M√°s de 3 caracteres
                c.relevance_score > 0.1 and  # Relevancia m√≠nima
                c.frequency > 1 and  # Frecuencia m√≠nima
                not c.concept.lower() in ['texto', 'documento', 'anexo', 'p√°gina', 'cap√≠tulo']  # Filtrar gen√©ricos
            ]
            
            # Tomar los mejores candidatos (m√°ximo 15 para an√°lisis profundo)
            concept_list = [c.concept for c in filtered_concepts[:15]]
            
            if not concept_list:
                print("‚ö†Ô∏è No se encontraron conceptos candidatos de calidad suficiente")
                return raw_concepts[:self.config.max_final_concepts]
            
            # Crear prompt para el LLM
            prompt = self._create_refinement_prompt(concept_list, document_context)
            
            # Llamar al LLM sin l√≠mites de tokens para evitar JSON truncado
            response = ollama_client.generate_response(
                model=self.config.llm_model,
                prompt=prompt
                # Sin max_tokens para permitir respuestas completas
            )
            
            # Parsear respuesta del LLM
            refined_concepts = self._parse_llm_response(response, raw_concepts)
            
            # Validar calidad de conceptos refinados con criterios m√°s flexibles
            quality_concepts = []
            for concept in refined_concepts:
                # Criterios m√°s flexibles y realistas
                concept_words = concept.concept.split()
                is_valid = (
                    len(concept_words) >= 1 and  # Al menos 1 palabra (m√°s flexible)
                    len(concept.concept) > 3 and  # M√°s de 3 caracteres (m√°s flexible)
                    not concept.concept.lower() in ['texto', 'documento', 'anexo', 'p√°gina', 'cap√≠tulo', 'secci√≥n', 'parte'] and  # Filtrar gen√©ricos
                    not concept.concept.lower().strip() in ['a', 'e', 'i', 'o', 'u', 'y', 'de', 'la', 'el', 'en', 'un', 'es', 'se', 'no', 'si']  # Filtrar palabras muy cortas
                )
                
                if is_valid:
                    quality_concepts.append(concept)
                    print(f"‚úÖ Concepto v√°lido: '{concept.concept}' (palabras: {len(concept_words)}, chars: {len(concept.concept)})")
                else:
                    print(f"‚ùå Concepto rechazado: '{concept.concept}' (palabras: {len(concept_words)}, chars: {len(concept.concept)})")
            
            if not quality_concepts:
                print("‚ö†Ô∏è Los conceptos refinados no cumplen criterios de calidad")
                print("üìä Conceptos recibidos del LLM:")
                for i, concept in enumerate(refined_concepts, 1):
                    print(f"  {i}. '{concept.concept}' (palabras: {len(concept.concept.split())}, chars: {len(concept.concept)})")
                
                # Fallback m√°s inteligente: usar conceptos originales pero mejorados
                fallback_concepts = []
                for c in raw_concepts:
                    if len(c.concept) > 3 and not c.concept.lower() in ['texto', 'documento', 'anexo', 'p√°gina']:
                        fallback_concepts.append(c)
                
                print(f"üîÑ Usando {len(fallback_concepts)} conceptos originales como fallback")
                return fallback_concepts[:self.config.max_final_concepts]
            
            print(f"‚úÖ {len(quality_concepts)} conceptos pasaron los criterios de calidad")
            return quality_concepts[:self.config.max_final_concepts]
            
        except Exception as e:
            print(f"Error en refinamiento LLM: {e}")
            # Fallback: devolver conceptos originales limitados
            return raw_concepts[:self.config.max_final_concepts]
    
    def _get_category_options(self) -> str:
        """
        Obtener opciones de categor√≠as para el prompt
        
        Returns:
            String con las opciones de categor√≠as disponibles
        """
        if self.config.use_custom_categories and self.config.custom_categories:
            # Usar categor√≠as personalizadas del usuario
            return "|".join(self.config.custom_categories.keys())
        else:
            # Usar categor√≠as por defecto
            return "metodologia|teoria|resultado|proceso|herramienta|fenomeno_social"
    
    def _create_refinement_prompt(self, concept_list: List[str], document_context: str) -> str:
        """
        Crear prompt optimizado para DeepSeek R1 que genere conceptos acad√©micos profundos
        
        Args:
            concept_list: Lista de conceptos candidatos
            document_context: Contexto del documento
            
        Returns:
            Prompt formateado para el LLM
        """
        context_preview = document_context[:2000] + "..." if len(document_context) > 2000 else document_context
        
        # Usar prompt especializado seg√∫n si hay categor√≠as personalizadas
        if self.config.use_custom_categories and self.config.custom_categories:
            return self._create_custom_categories_prompt(concept_list, context_preview)
        else:
            return self._create_general_analysis_prompt(concept_list, context_preview)
    
    def _create_custom_categories_prompt(self, concept_list: List[str], context_preview: str) -> str:
        """
        Crear prompt especializado para an√°lisis con categor√≠as personalizadas del usuario
        
        Args:
            concept_list: Lista de conceptos candidatos
            context_preview: Contexto del documento
            
        Returns:
            Prompt especializado para categor√≠as personalizadas
        """
        categories_section = ""
        for category, definition in self.config.custom_categories.items():
            categories_section += f"- **{category}**: {definition}\n"
        
        return f"""Eres un experto en an√°lisis cualitativo de documentos acad√©micos. Tu tarea es identificar conceptos clave que se ajusten EXACTAMENTE a las categor√≠as espec√≠ficas definidas por el usuario.

IMPORTANTE: Responde √öNICAMENTE en espa√±ol. Todos los conceptos, explicaciones y categor√≠as deben estar en espa√±ol.

CONTEXTO DEL DOCUMENTO:
{context_preview}

T√âRMINOS CANDIDATOS EXTRA√çDOS:
{', '.join(concept_list)}

CATEGOR√çAS ESPEC√çFICAS REQUERIDAS POR EL USUARIO:
{categories_section}

INSTRUCCIONES CR√çTICAS:
1. SOLO genera conceptos que encajen PERFECTAMENTE en las categor√≠as definidas arriba
2. Si un concepto no encaja en ninguna categor√≠a, NO lo incluyas
3. Cada concepto DEBE ser clasificado en una de las categor√≠as del usuario
4. Prioriza conceptos que sean espec√≠ficos y relevantes para cada categor√≠a
5. Genera conceptos que sean √∫tiles para la investigaci√≥n del usuario
6. Cada concepto debe tener una explicaci√≥n √öNICA y espec√≠fica
7. NO uses plantillas gen√©ricas - cada explicaci√≥n debe ser √∫nica

FORMATO DE RESPUESTA (JSON estricto):
{{
    "conceptos_refinados": [
        {{
            "concepto": "nombre_del_concepto_espec√≠fico_en_espa√±ol",
            "relevancia": "alta|media|baja",
            "categoria": "{'|'.join(self.config.custom_categories.keys())}",
            "explicacion": "Explicaci√≥n detallada y √öNICA de por qu√© este concepto espec√≠fico es importante para esta categor√≠a particular y qu√© fen√≥meno captura.",
            "conceptos_relacionados": ["concepto_relacionado1", "concepto_relacionado2"]
        }}
    ]
}}

CRITERIOS DE CALIDAD PARA CATEGOR√çAS PERSONALIZADAS:
- Los conceptos deben ser espec√≠ficos a las categor√≠as definidas
- Deben ser √∫tiles para la investigaci√≥n del usuario
- Deben tener explicaciones √∫nicas y espec√≠ficas
- NO deben ser conceptos gen√©ricos o irrelevantes
- Cada concepto debe aportar valor espec√≠fico a su categor√≠a
- Los conceptos deben tener AL MENOS 2 palabras y m√°s de 5 caracteres
- NO uses palabras sueltas como "a", "e", "i", "o", "u", "de", "la", "el"
- NO uses t√©rminos gen√©ricos como "texto", "documento", "p√°gina"

EJEMPLOS DE CONCEPTOS V√ÅLIDOS:
‚úÖ "An√°lisis cualitativo de datos" (espec√≠fico, √∫til)
‚úÖ "Metodolog√≠a de investigaci√≥n" (claro, acad√©mico)
‚úÖ "Teor√≠a del aprendizaje" (espec√≠fico, te√≥rico)

EJEMPLOS DE CONCEPTOS INV√ÅLIDOS:
‚ùå "texto" (gen√©rico)
‚ùå "an√°lisis" (muy general)
‚ùå "a" (palabra suelta)
‚ùå "documento" (gen√©rico)

REGLAS CR√çTICAS:
- SOLO incluye conceptos que encajen en las categor√≠as del usuario
- Cada explicaci√≥n debe ser completamente diferente
- Las explicaciones deben ser espec√≠ficas al concepto y su categor√≠a
- NO uses frases gen√©ricas o plantillas repetidas
- Si no encuentras conceptos que encajen, devuelve menos conceptos pero de mayor calidad

IMPORTANTE:
- Responde SOLO con JSON v√°lido
- M√°ximo {self.config.max_final_concepts} conceptos
- Cada concepto debe ser clasificado en una de las categor√≠as del usuario
- TODO debe estar en espa√±ol
- Si no encuentras conceptos relevantes para las categor√≠as, es mejor devolver menos conceptos pero de mayor calidad"""

    def _create_general_analysis_prompt(self, concept_list: List[str], context_preview: str) -> str:
        """
        Crear prompt para an√°lisis general sin categor√≠as espec√≠ficas
        
        Args:
            concept_list: Lista de conceptos candidatos
            context_preview: Contexto del documento
            
        Returns:
            Prompt para an√°lisis general
        """
        return f"""Eres un experto en an√°lisis cualitativo de documentos acad√©micos. Tu tarea es identificar y desarrollar CONCEPTOS CLAVE profundos y significativos que emergen del documento.

IMPORTANTE: Responde √öNICAMENTE en espa√±ol. Todos los conceptos, explicaciones y categor√≠as deben estar en espa√±ol.

CONTEXTO DEL DOCUMENTO:
{context_preview}

T√âRMINOS CANDIDATOS EXTRA√çDOS:
{', '.join(concept_list)}

DEFINICI√ìN DE CONCEPTO CLAVE:
Un concepto clave en investigaci√≥n cualitativa es una idea o categor√≠a fundamental que emerge de los datos para explicar y dar sentido al fen√≥meno estudiado. Los conceptos clave permiten:
- Identificar patrones o temas recurrentes
- Capturar motivaciones, experiencias, creencias y emociones
- Ofrecer comprensi√≥n profunda y matizada de fen√≥menos complejos
- Desarrollar teor√≠as o marcos conceptuales

EJEMPLOS DE CONCEPTOS CLAVE ACAD√âMICOS EN ESPA√ëOL:
- "Resiliencia comunitaria" (no "comunidad")
- "Identidad profesional en transici√≥n" (no "trabajo")
- "Alfabetizaci√≥n digital generacional" (no "tecnolog√≠a")
- "Redes de apoyo vecinal" (no "vecinos")
- "Memoria colectiva del trauma" (no "recuerdos")

INSTRUCCIONES ESPEC√çFICAS:
1. ANALIZA el contexto completo del documento para identificar fen√≥menos complejos
2. DESARROLLA conceptos que capturen la esencia de procesos, relaciones o fen√≥menos
3. EVITA palabras simples o gen√©ricas - busca conceptos que expliquen "c√≥mo" y "por qu√©"
4. COMBINA t√©rminos relacionados en conceptos m√°s ricos y significativos
5. PRIORIZA conceptos que revelen patrones, procesos o relaciones sociales
6. ELIMINA completamente letras sueltas, palabras gen√©ricas y t√©rminos irrelevantes
7. CADA CONCEPTO DEBE TENER UNA EXPLICACI√ìN √öNICA Y ESPEC√çFICA
8. NO REPITAS explicaciones entre conceptos diferentes

FORMATO DE RESPUESTA (JSON estricto):
{{
    "conceptos_refinados": [
        {{
            "concepto": "nombre_del_concepto_acad√©mico_profundo_en_espa√±ol",
            "relevancia": "alta|media|baja",
            "categoria": "metodologia|teoria|resultado|proceso|herramienta|fenomeno_social",
            "explicacion": "Explicaci√≥n detallada y √öNICA de por qu√© este concepto espec√≠fico es importante y qu√© fen√≥meno particular captura. Debe ser diferente a cualquier otra explicaci√≥n.",
            "conceptos_relacionados": ["concepto_relacionado1", "concepto_relacionado2"]
        }}
    ]
}}

CRITERIOS DE CALIDAD:
- Los conceptos deben ser sustantivos y descriptivos (2-4 palabras)
- Deben capturar procesos, relaciones o fen√≥menos complejos
- Deben ser √∫tiles para explicar el fen√≥meno estudiado
- Deben ser espec√≠ficos al contexto del documento
- NO deben ser palabras simples o gen√©ricas
- CADA explicaci√≥n debe ser √öNICA y espec√≠fica al concepto
- Los conceptos deben tener AL MENOS 2 palabras y m√°s de 5 caracteres
- NO uses palabras sueltas como "a", "e", "i", "o", "u", "de", "la", "el"
- NO uses t√©rminos gen√©ricos como "texto", "documento", "p√°gina"

EJEMPLOS DE CONCEPTOS V√ÅLIDOS:
‚úÖ "Resiliencia comunitaria" (espec√≠fico, acad√©mico)
‚úÖ "Identidad profesional en transici√≥n" (complejo, significativo)
‚úÖ "Alfabetizaci√≥n digital generacional" (espec√≠fico, moderno)

EJEMPLOS DE CONCEPTOS INV√ÅLIDOS:
‚ùå "comunidad" (muy general)
‚ùå "trabajo" (gen√©rico)
‚ùå "texto" (irrelevante)
‚ùå "a" (palabra suelta)

REGLAS CR√çTICAS:
- Cada concepto debe tener una explicaci√≥n completamente diferente
- Las explicaciones deben ser espec√≠ficas al concepto, no gen√©ricas
- Si dos conceptos son similares, sus explicaciones deben destacar las diferencias
- NO uses plantillas o frases gen√©ricas repetidas

IMPORTANTE:
- Responde SOLO con JSON v√°lido
- M√°ximo {self.config.max_final_concepts} conceptos
- Cada concepto debe ser una idea compleja y significativa
- Si no encuentras conceptos profundos, es mejor devolver menos conceptos pero de mayor calidad
- TODO debe estar en espa√±ol"""
    
    def _parse_llm_response(self, response: str, original_concepts: List[ExtractedConcept]) -> List[ExtractedConcept]:
        """
        Parsear respuesta del LLM y crear conceptos refinados
        
        Args:
            response: Respuesta del LLM
            original_concepts: Conceptos originales para preservar metadatos
            
        Returns:
            Lista de conceptos refinados
        """
        import json
        import re
        
        try:
            # Limpiar caracteres de control inv√°lidos
            cleaned_response = self._clean_json_response(response)
            
            # Buscar JSON en la respuesta
            json_start = cleaned_response.find('{')
            json_end = cleaned_response.rfind('}') + 1
            
            if json_start == -1 or json_end == 0:
                raise ValueError("No se encontr√≥ JSON v√°lido en la respuesta")
            
            json_str = cleaned_response[json_start:json_end]
            
            # Verificar si el JSON est√° completo
            if not self._is_json_complete(json_str):
                print("‚ö†Ô∏è JSON parece estar truncado, intentando reparar...")
                json_str = self._repair_truncated_json(json_str)
            
            # Intentar parsear JSON con manejo de errores mejorado
            try:
                data = json.loads(json_str)
            except json.JSONDecodeError as e:
                print(f"‚ö†Ô∏è Error JSON inicial: {e}")
                print(f"üìÑ JSON problem√°tico: {json_str[:500]}...")
                
                # Si falla, intentar limpiar m√°s agresivamente
                json_str = self._aggressive_json_clean(json_str)
                print(f"üîß JSON despu√©s de limpieza: {json_str[:500]}...")
                
                try:
                    data = json.loads(json_str)
                    print("‚úÖ JSON parseado exitosamente despu√©s de limpieza")
                except json.JSONDecodeError as e2:
                    print(f"‚ùå Error JSON persistente: {e2}")
                    print(f"üìÑ JSON final: {json_str}")
                    
                    # √öltimo intento: reparaci√≥n manual b√°sica
                    json_str = self._manual_json_repair(json_str)
                    try:
                        data = json.loads(json_str)
                        print("‚úÖ JSON reparado manualmente")
                    except json.JSONDecodeError as e3:
                        print(f"‚ùå Error JSON final: {e3}")
                        raise ValueError(f"No se pudo parsear JSON despu√©s de m√∫ltiples intentos: {e3}")
            
            refined_concepts = []
            
            for item in data.get('conceptos_refinados', []):
                # Buscar concepto original m√°s similar para preservar metadatos
                original_concept = self._find_most_similar_original(
                    item['concepto'], original_concepts
                )
                
                # Calcular score de relevancia mejorado para conceptos refinados
                base_score = original_concept.relevance_score if original_concept else 0.5
                
                # Ajustar score basado en relevancia del LLM
                relevancia_llm = item.get('relevancia', 'media')
                if relevancia_llm == 'alta':
                    llm_multiplier = 1.2
                elif relevancia_llm == 'media':
                    llm_multiplier = 1.0
                else:  # baja
                    llm_multiplier = 0.8
                
                # Ajustar por longitud del concepto (conceptos m√°s largos suelen ser m√°s espec√≠ficos)
                length_bonus = min(0.1, len(item['concepto'].split()) * 0.02)
                
                final_score = min(1.0, (base_score * llm_multiplier) + length_bonus)
                
                # Crear concepto refinado
                refined_concept = ExtractedConcept(
                    concept=item['concepto'],
                    frequency=original_concept.frequency if original_concept else 1,
                    relevance_score=final_score,
                    sources=original_concept.sources if original_concept else [],
                    citations=original_concept.citations if original_concept else [],
                    context_examples=original_concept.context_examples if original_concept else [],
                    related_concepts=item.get('conceptos_relacionados', []),
                    category=item.get('categoria'),
                    extraction_method='llm_refined',
                    timestamp=datetime.now().isoformat()
                )
                
                # A√±adir explicaci√≥n si est√° disponible
                if self.config.include_concept_explanations and 'explicacion' in item:
                    explanation_text = f"Explicaci√≥n: {item['explicacion']}"
                    refined_concept.context_examples.append(explanation_text)
                    print(f"‚úÖ Explicaci√≥n agregada para '{item['concepto']}': {item['explicacion'][:100]}...")
                else:
                    print(f"‚ö†Ô∏è No se agreg√≥ explicaci√≥n para '{item['concepto']}' - include_explanations: {self.config.include_concept_explanations}, tiene_explicacion: {'explicacion' in item}")
                
                print(f"üìä Concepto refinado creado: '{refined_concept.concept}' - M√©todo: {refined_concept.extraction_method}, Categor√≠a: {refined_concept.category}")
                refined_concepts.append(refined_concept)
            
            return refined_concepts
            
        except (json.JSONDecodeError, KeyError, ValueError) as e:
            print(f"Error parseando respuesta LLM: {e}")
            print(f"Respuesta original: {response[:500]}...")
            # Fallback: devolver conceptos originales
            return original_concepts[:self.config.max_final_concepts]
    
    def _clean_json_response(self, response: str) -> str:
        """
        Limpiar caracteres de control inv√°lidos de la respuesta del LLM
        
        Args:
            response: Respuesta original del LLM
            
        Returns:
            Respuesta limpia sin caracteres de control inv√°lidos
        """
        import re
        
        # Remover caracteres de control excepto \n, \r, \t
        cleaned = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', response)
        
        # Normalizar saltos de l√≠nea
        cleaned = cleaned.replace('\r\n', '\n').replace('\r', '\n')
        
        # Remover espacios m√∫ltiples
        cleaned = re.sub(r' +', ' ', cleaned)
        
        return cleaned
    
    def _aggressive_json_clean(self, json_str: str) -> str:
        """
        Limpieza agresiva de JSON para manejar casos problem√°ticos
        
        Args:
            json_str: String JSON a limpiar
            
        Returns:
            JSON limpio y v√°lido
        """
        import re
        
        # Paso 1: Escapar caracteres problem√°ticos en strings
        def escape_string(match):
            string_content = match.group(1)
            # Escapar caracteres problem√°ticos
            string_content = string_content.replace('\\', '\\\\')
            string_content = string_content.replace('"', '\\"')
            string_content = string_content.replace('\n', '\\n')
            string_content = string_content.replace('\t', '\\t')
            string_content = string_content.replace('\r', '\\r')
            return f'"{string_content}"'
        
        # Aplicar escape a strings
        json_str = re.sub(r'"([^"]*)"', escape_string, json_str)
        
        # Paso 2: Corregir comas faltantes antes de llaves de cierre
        # Buscar patrones como: "valor" } o "valor" ] y agregar coma
        json_str = re.sub(r'"\s*}\s*', '",\n}', json_str)
        json_str = re.sub(r'"\s*]\s*', '",\n]', json_str)
        
        # Paso 3: Corregir comas faltantes despu√©s de valores
        # Buscar patrones como: "valor" "siguiente" y agregar coma
        json_str = re.sub(r'"\s*"', '",\n"', json_str)
        
        # Paso 4: Limpiar comas m√∫ltiples
        json_str = re.sub(r',\s*,', ',', json_str)
        
        # Paso 5: Corregir comas antes de llaves de cierre (casos espec√≠ficos)
        json_str = re.sub(r',\s*}', '}', json_str)
        json_str = re.sub(r',\s*]', ']', json_str)
        
        # Paso 6: Remover caracteres de control restantes
        json_str = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', json_str)
        
        return json_str
    
    def _is_json_complete(self, json_str: str) -> bool:
        """
        Verificar si el JSON est√° completo (no truncado)
        
        Args:
            json_str: String JSON a verificar
            
        Returns:
            True si el JSON parece completo, False si est√° truncado
        """
        # Contar llaves de apertura y cierre
        open_braces = json_str.count('{')
        close_braces = json_str.count('}')
        
        # Contar corchetes de apertura y cierre
        open_brackets = json_str.count('[')
        close_brackets = json_str.count(']')
        
        # Verificar que las llaves est√©n balanceadas
        braces_balanced = open_braces == close_braces
        brackets_balanced = open_brackets == close_brackets
        
        # Verificar que termine con } o ]
        ends_properly = json_str.strip().endswith('}') or json_str.strip().endswith(']')
        
        return braces_balanced and brackets_balanced and ends_properly
    
    def _repair_truncated_json(self, json_str: str) -> str:
        """
        Reparar JSON truncado agregando elementos faltantes
        
        Args:
            json_str: JSON truncado a reparar
            
        Returns:
            JSON reparado
        """
        import re
        
        print(f"üîß Reparando JSON truncado...")
        print(f"üìÑ JSON original (√∫ltimos 200 chars): ...{json_str[-200:]}")
        
        # Si el JSON est√° incompleto, intentar cerrarlo apropiadamente
        if not json_str.strip().endswith('}'):
            # Buscar el √∫ltimo objeto completo
            # Encontrar el √∫ltimo } completo
            last_complete_brace = json_str.rfind('},')
            if last_complete_brace != -1:
                # Cortar hasta el √∫ltimo objeto completo y cerrar el array y objeto principal
                json_str = json_str[:last_complete_brace + 1] + '\n    ]\n}'
                print(f"‚úÖ JSON reparado cortando en √∫ltimo objeto completo")
            else:
                # Si no hay objetos completos, cerrar lo que hay
                # Buscar el √∫ltimo objeto incompleto y cerrarlo
                last_open_brace = json_str.rfind('{')
                if last_open_brace != -1:
                    # Encontrar el √∫ltimo objeto incompleto
                    incomplete_part = json_str[last_open_brace:]
                    # Cerrar el objeto incompleto
                    if not incomplete_part.strip().endswith('}'):
                        # Agregar comas faltantes y cerrar
                        incomplete_part = incomplete_part.rstrip(',') + '}'
                        json_str = json_str[:last_open_brace] + incomplete_part + '\n    ]\n}'
                        print(f"‚úÖ JSON reparado cerrando objeto incompleto")
                    else:
                        json_str = json_str.rstrip(',') + '\n    ]\n}'
                        print(f"‚úÖ JSON reparado con cierre b√°sico")
                else:
                    json_str = json_str.rstrip(',') + '\n    ]\n}'
                    print(f"‚úÖ JSON reparado con cierre b√°sico")
        
        # Limpiar comas duplicadas
        json_str = re.sub(r',\s*,', ',', json_str)
        
        # Remover comas antes de llaves de cierre
        json_str = re.sub(r',\s*}', '}', json_str)
        json_str = re.sub(r',\s*]', ']', json_str)
        
        print(f"üìÑ JSON reparado (√∫ltimos 200 chars): ...{json_str[-200:]}")
        return json_str
    
    def _manual_json_repair(self, json_str: str) -> str:
        """
        Reparaci√≥n manual b√°sica de JSON para casos extremos
        
        Args:
            json_str: String JSON a reparar
            
        Returns:
            JSON reparado
        """
        import re
        
        print(f"üîß Reparaci√≥n manual de JSON...")
        print(f"üìÑ JSON problem√°tico (√∫ltimos 300 chars): ...{json_str[-300:]}")
        
        # Reparaciones b√°sicas comunes
        # 1. Agregar comas faltantes antes de llaves de cierre
        json_str = re.sub(r'"\s*}\s*', '",\n}', json_str)
        json_str = re.sub(r'"\s*]\s*', '",\n]', json_str)
        
        # 2. Corregir comas faltantes entre elementos del array
        json_str = re.sub(r'}\s*{', '},\n{', json_str)
        
        # 3. Limpiar comas duplicadas
        json_str = re.sub(r',\s*,', ',', json_str)
        
        # 4. Remover comas antes de llaves de cierre
        json_str = re.sub(r',\s*}', '}', json_str)
        json_str = re.sub(r',\s*]', ']', json_str)
        
        # 5. Asegurar que el JSON est√© bien formado
        if not json_str.strip().startswith('{'):
            json_str = '{' + json_str
        if not json_str.strip().endswith('}'):
            json_str = json_str + '}'
        
        # 6. Reparaci√≥n espec√≠fica para el caso del usuario
        # Buscar objetos incompletos y cerrarlos
        lines = json_str.split('\n')
        repaired_lines = []
        in_object = False
        brace_count = 0
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Contar llaves
            brace_count += line.count('{') - line.count('}')
            
            if '{' in line and not in_object:
                in_object = True
                brace_count = 1
            
            if in_object:
                # Si la l√≠nea no termina con } o }, agregar coma si es necesario
                if line.endswith('"') and not line.endswith('",') and not line.endswith('"}') and not line.endswith('"]'):
                    line = line + ','
                
                repaired_lines.append(line)
                
                # Si cerramos el objeto
                if brace_count == 0:
                    in_object = False
        
        json_str = '\n'.join(repaired_lines)
        
        print(f"üìÑ JSON reparado manualmente (√∫ltimos 200 chars): ...{json_str[-200:]}")
        return json_str
    
    def _find_most_similar_original(self, refined_name: str, original_concepts: List[ExtractedConcept]) -> Optional[ExtractedConcept]:
        """
        Encontrar el concepto original m√°s similar al refinado
        
        Args:
            refined_name: Nombre del concepto refinado
            original_concepts: Lista de conceptos originales
            
        Returns:
            Concepto original m√°s similar o None
        """
        if not original_concepts:
            return None
        
        # B√∫squeda simple por similitud de texto
        refined_lower = refined_name.lower()
        
        for original in original_concepts:
            if refined_lower in original.concept.lower() or original.concept.lower() in refined_lower:
                return original
        
        # Si no hay coincidencia, devolver el primero
        return original_concepts[0]
    
    def extract_concepts(
        self,
        chunks: List[Dict[str, Any]],
        method: str = "tfidf"
    ) -> List[ExtractedConcept]:
        """
        Extraer conceptos clave de los documentos usando enfoque h√≠brido
        
        Este es el m√©todo principal que un investigador utilizar√≠a.
        
        Proceso h√≠brido:
        1. Preprocesa el texto (limpieza, normalizaci√≥n)
        2. Aplica algoritmo de extracci√≥n inicial (TF-IDF por defecto)
        3. Identifica fuentes y crea citas
        4. [NUEVO] Refina conceptos con modelo LLM (DeepSeek R1)
        5. Encuentra conceptos relacionados
        6. Ordena por relevancia
        
        Args:
            chunks: Lista de chunks de documentos con estructura:
                    {
                        'content': str,
                        'metadata': {
                            'source_file': str,
                            'page_number': int (opcional)
                        }
                    }
            method: M√©todo de extracci√≥n ('tfidf' por defecto)
            
        Returns:
            Lista de ExtractedConcept ordenados por relevancia
            
        Raises:
            ValueError: Si chunks est√° vac√≠o o mal formado
            ImportError: Si faltan dependencias necesarias
        """
        # Validar entrada
        if not chunks:
            raise ValueError("La lista de chunks no puede estar vac√≠a")
        
        if not all('content' in chunk and 'metadata' in chunk for chunk in chunks):
            raise ValueError("Cada chunk debe tener 'content' y 'metadata'")
        
        # Limpiar citas anteriores
        self.citation_manager.clear()
        
        # FASE 1: Extracci√≥n inicial con TF-IDF
        print("üîç Extrayendo conceptos candidatos con TF-IDF...")
        if method == "tfidf":
            raw_concepts, processed_texts = self._extract_with_tfidf(chunks)
        else:
            raise ValueError(f"M√©todo desconocido: {method}")
        
        # Crear contexto del documento para el LLM
        document_context = " ".join(processed_texts)
        
        # FASE 2: Refinamiento con LLM (si est√° habilitado)
        if self.config.enable_llm_refinement:
            print(f"ü§ñ Refinando conceptos con {self.config.llm_model}...")
            refined_concepts = self._refine_concepts_with_llm(raw_concepts, document_context)
            
            # Identificar conceptos relacionados en los refinados
            term_locations = defaultdict(list)
            for i, concept in enumerate(refined_concepts):
                term_locations[concept.concept] = [i]
            
            self._identify_related_concepts(refined_concepts, term_locations)
            
            print(f"‚úÖ An√°lisis completado: {len(refined_concepts)} conceptos refinados")
            return refined_concepts
        else:
            # Solo TF-IDF, sin refinamiento LLM
            print(f"‚úÖ An√°lisis completado: {len(raw_concepts)} conceptos extra√≠dos")
            return raw_concepts
    
    def get_concept_summary(self, concepts: List[ExtractedConcept]) -> Dict[str, Any]:
        """
        Generar resumen estad√≠stico de los conceptos extra√≠dos
        
        Args:
            concepts: Lista de conceptos extra√≠dos
            
        Returns:
            Diccionario con estad√≠sticas
        """
        if not concepts:
            return {
                'total_concepts': 0,
                'total_frequency': 0,
                'avg_relevance': 0.0,
                'unique_sources': 0,
                'total_citations': 0
            }
        
        total_freq = sum(c.frequency for c in concepts)
        avg_relevance = sum(c.relevance_score for c in concepts) / len(concepts)
        all_sources = set()
        total_citations = 0
        
        for concept in concepts:
            all_sources.update(concept.sources)
            total_citations += len(concept.citations)
        
        return {
            'total_concepts': len(concepts),
            'total_frequency': total_freq,
            'avg_relevance': avg_relevance,
            'unique_sources': len(all_sources),
            'total_citations': total_citations,
            'top_concept': concepts[0].concept if concepts else None,
            'citation_stats': self.citation_manager.get_statistics()
        }
    
    def export_concepts(
        self,
        concepts: List[ExtractedConcept],
        include_citations: bool = True
    ) -> List[Dict[str, Any]]:
        """
        Exportar conceptos a formato serializable
        
        Args:
            concepts: Lista de conceptos
            include_citations: Si incluir informaci√≥n de citas
            
        Returns:
            Lista de diccionarios con conceptos
        """
        exported = []
        
        for concept in concepts:
            data = concept.to_dict()
            
            if include_citations and concept.citations:
                data['citations'] = [c.to_dict() for c in concept.citations]
            
            exported.append(data)
        
        return exported
    
    def __repr__(self) -> str:
        return f"ConceptExtractor(citations={len(self.citation_manager)}, config={self.config})"

