"""PÃ¡gina de procesamiento de documentos para el sistema RAG"""

import streamlit as st
import os
import json
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime
import time

# Importaciones locales
from config.settings import AppConfig
from utils.logger import setup_logger
from utils.rag_processor import rag_processor
from utils.ollama_client import ollama_client, get_available_or_default_models, get_default_models
from modules.document_upload import get_valid_uploaded_files

# Inicializar configuraciÃ³n y logger
config = AppConfig()
logger = setup_logger("DocumentProcessor")

def show_document_stats():
    """Mostrar estadÃ­sticas de documentos procesados"""
    stats = rag_processor.get_document_stats()
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Documentos", stats.get('total_documents', 0))
    
    with col2:
        st.metric("Chunks", stats.get('total_chunks', 0))
    
    with col3:
        st.metric("Embeddings", stats.get('total_embeddings', 0))
    
    with col4:
        st.metric("TamaÃ±o Cache", f"{stats.get('cache_size_mb', 0):.1f} MB")

def upload_documents():
    """Interfaz para subir documentos"""
    st.subheader("ğŸ“¤ Subir Documentos")
    
    uploaded_files = st.file_uploader(
        "Selecciona documentos para procesar",
        type=['pdf', 'docx', 'xlsx', 'csv', 'txt', 'md', 'html', 'json'],
        accept_multiple_files=True,
        help="Formatos soportados: PDF, DOCX, XLSX, CSV, TXT, MD, HTML, JSON"
    )
    
    if uploaded_files:
        st.write(f"ğŸ“ {len(uploaded_files)} archivo(s) seleccionado(s)")
        
        if st.button("ğŸš€ Procesar Documentos", type="primary"):
            process_uploaded_files(uploaded_files)

def process_uploaded_files(uploaded_files):
    """Procesar archivos subidos"""
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    total_files = len(uploaded_files)
    processed_files = []
    failed_files = []
    
    for i, uploaded_file in enumerate(uploaded_files):
        try:
            # Actualizar progreso
            progress = (i + 1) / total_files
            progress_bar.progress(progress)
            status_text.text(f"Procesando: {uploaded_file.name}")
            
            # Guardar archivo temporalmente
            file_path = config.UPLOADS_DIR / uploaded_file.name
            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            
            # Procesar con RAG (pasar Path object, no string)
            chunks = rag_processor.process_document(file_path)
            
            if chunks:  # Si se procesaron chunks exitosamente
                # Generar embeddings
                chunks_with_embeddings = rag_processor.generate_embeddings(chunks)
                processed_files.append(uploaded_file.name)
                logger.info(f"Documento procesado exitosamente: {uploaded_file.name} ({len(chunks)} chunks)")
            else:
                failed_files.append(uploaded_file.name)
                logger.error(f"Error al procesar documento: {uploaded_file.name}")
            
            time.sleep(0.1)  # PequeÃ±a pausa para mostrar progreso
            
        except Exception as e:
            failed_files.append(uploaded_file.name)
            logger.error(f"Error al procesar {uploaded_file.name}: {e}")
    
    # Mostrar resultados
    progress_bar.progress(1.0)
    status_text.text("âœ… Procesamiento completado")
    
    if processed_files:
        st.success(f"âœ… {len(processed_files)} documento(s) procesado(s) exitosamente:")
        for file_name in processed_files:
            st.write(f"  â€¢ {file_name}")
    
    if failed_files:
        st.error(f"âŒ {len(failed_files)} documento(s) fallaron:")
        for file_name in failed_files:
            st.write(f"  â€¢ {file_name}")
    
    # Actualizar estadÃ­sticas
    st.rerun()

def show_uploaded_documents():
    """Mostrar documentos subidos con informaciÃ³n detallada"""
    st.subheader("ğŸ“‹ Documentos Procesados")
    
    # Obtener lista de documentos usando funciÃ³n estandarizada
    uploaded_files = get_valid_uploaded_files()
    
    if not uploaded_files:
        st.info("ğŸ“ No hay documentos procesados aÃºn.")
        st.markdown("ğŸ’¡ **Sugerencia:** Usa la pestaÃ±a 'GestiÃ³n de Documentos' para subir archivos.")
        return
    
    # Mostrar estadÃ­sticas generales
    stats = rag_processor.get_document_stats()
    
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ğŸ“„ Total Documentos", len(uploaded_files))
    with col2:
        st.metric("ğŸ“ Total Chunks", stats.get('total_chunks', 0))
    with col3:
        st.metric("ğŸ§  Total Embeddings", stats.get('total_embeddings', 0))
    
    st.markdown("---")
    
    # Mostrar documentos en tabla con mÃ¡s informaciÃ³n
    documents_data = []
    for file_path in uploaded_files:
        if file_path.is_file():
            stat = file_path.stat()
            
            # Verificar si estÃ¡ en cache
            file_hash = rag_processor._get_file_hash(file_path)
            cache_key = f"{file_path.name}_{file_hash}"
            chunks_count = 0
            if hasattr(rag_processor, 'chunks_cache') and cache_key in rag_processor.chunks_cache:
                chunks_count = len(rag_processor.chunks_cache[cache_key])
            
            documents_data.append({
                "ğŸ“„ Nombre": file_path.name,
                "ğŸ“Š TamaÃ±o": f"{stat.st_size / 1024:.1f} KB",
                "ğŸ§© Chunks": chunks_count,
                "ğŸ“… Modificado": time.strftime("%Y-%m-%d %H:%M", time.localtime(stat.st_mtime)),
                "âœ… Estado": "Procesado" if chunks_count > 0 else "Pendiente"
            })
    
    if documents_data:
        # Mostrar tabla
        st.dataframe(documents_data, use_container_width=True, hide_index=True)
        
        # SecciÃ³n para eliminar documentos individuales
        st.markdown("---")
        st.subheader("ğŸ—‘ï¸ Eliminar Documentos")
        
        selected_files = st.multiselect(
            "Selecciona documentos para eliminar:",
            options=[f.name for f in uploaded_files],
            help="Selecciona uno o mÃ¡s documentos para eliminar"
        )
        
        if selected_files:
            if st.button("ğŸ—‘ï¸ Eliminar Seleccionados", type="secondary"):
                delete_selected_documents(uploaded_files, selected_files)

def manage_documents():
    """Gestionar documentos existentes"""
    st.subheader("ğŸ“‹ Documentos Procesados")
    
    # Obtener lista de documentos usando funciÃ³n estandarizada
    uploaded_files = get_valid_uploaded_files()
    
    if not uploaded_files:
        st.info("No hay documentos procesados aÃºn.")
        return
    
    # Mostrar documentos en tabla con mÃ¡s informaciÃ³n
    documents_data = []
    for file_path in uploaded_files:
        if file_path.is_file():
            stat = file_path.stat()
            
            # Verificar si estÃ¡ en cache
            file_hash = rag_processor._get_file_hash(file_path)
            cache_key = f"{file_path.name}_{file_hash}"
            chunks_count = 0
            if cache_key in rag_processor.chunks_cache:
                chunks_count = len(rag_processor.chunks_cache[cache_key])
            
            documents_data.append({
                "ğŸ“„ Nombre": file_path.name,
                "ğŸ“Š TamaÃ±o": f"{stat.st_size / 1024:.1f} KB",
                "ğŸ§© Chunks": chunks_count,
                "ğŸ“… Modificado": time.strftime("%Y-%m-%d %H:%M", time.localtime(stat.st_mtime)),
                "ğŸ”— Ruta": str(file_path)
            })
    
    if documents_data:
        # Mostrar tabla
        df = st.dataframe(documents_data, use_container_width=True, hide_index=True)
        
        # Opciones de gestiÃ³n
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            if st.button("ğŸ—‘ï¸ Limpiar Cache", help="Limpiar cache de embeddings"):
                rag_processor.clear_cache()
                st.success("Cache limpiado exitosamente")
                st.rerun()
        
        with col2:
            if st.button("ğŸ”„ Reprocesar Todo", help="Reprocesar todos los documentos"):
                reprocess_all_documents(uploaded_files)
        
        with col3:
            if st.button("ğŸ“Š Actualizar Stats", help="Actualizar estadÃ­sticas"):
                st.rerun()
        
        with col4:
            if st.button("ğŸ’¾ Guardar Cache", help="Forzar guardado del cache"):
                rag_processor._save_cache()
                st.success("Cache guardado")
        
        # SecciÃ³n para eliminar documentos individuales
        st.markdown("---")
        st.subheader("ğŸ—‘ï¸ Eliminar Documentos")
        
        selected_files = st.multiselect(
            "Selecciona documentos para eliminar:",
            options=[f.name for f in uploaded_files],
            help="Selecciona uno o mÃ¡s documentos para eliminar"
        )
        
        if selected_files:
            if st.button("ğŸ—‘ï¸ Eliminar Seleccionados", type="secondary"):
                delete_selected_documents(uploaded_files, selected_files)

def delete_selected_documents(uploaded_files, selected_files):
    """Eliminar documentos seleccionados"""
    try:
        deleted_count = 0
        failed_deletions = []
        
        for file_path in uploaded_files:
            if file_path.name in selected_files:
                try:
                    # Eliminar del cache primero
                    file_hash = rag_processor._get_file_hash(file_path)
                    cache_key = f"{file_path.name}_{file_hash}"
                    
                    if cache_key in rag_processor.chunks_cache:
                        del rag_processor.chunks_cache[cache_key]
                    
                    # Eliminar embeddings relacionados
                    embeddings_to_remove = [key for key in rag_processor.embeddings_cache.keys() 
                                          if key.startswith(cache_key.split('_')[0])]
                    for key in embeddings_to_remove:
                        del rag_processor.embeddings_cache[key]
                    
                    # Eliminar archivo fÃ­sico
                    file_path.unlink()
                    deleted_count += 1
                    logger.info(f"Documento eliminado: {file_path.name}")
                    
                except Exception as e:
                    failed_deletions.append(file_path.name)
                    logger.error(f"Error al eliminar {file_path.name}: {e}")
        
        # Guardar cache actualizado
        rag_processor._save_cache()
        
        # Mostrar resultados
        if deleted_count > 0:
            st.success(f"âœ… {deleted_count} documento(s) eliminado(s) exitosamente")
        
        if failed_deletions:
            st.error(f"âŒ Error al eliminar: {', '.join(failed_deletions)}")
        
        st.rerun()
        
    except Exception as e:
        st.error(f"Error general al eliminar documentos: {e}")
        logger.error(f"Error en delete_selected_documents: {e}")

def reprocess_all_documents(file_paths):
    """Reprocesar todos los documentos"""
    if not file_paths:
        return
    
    with st.spinner("Reprocesando documentos..."):
        # Limpiar cache primero
        rag_processor.clear_cache()
        
        success_count = 0
        for file_path in file_paths:
            if file_path.is_file():
                try:
                    # Procesar documento (pasar Path object directamente)
                    chunks = rag_processor.process_document(file_path)
                    if chunks:
                        # Generar embeddings
                        chunks_with_embeddings = rag_processor.generate_embeddings(chunks)
                        success_count += 1
                        logger.info(f"Documento reprocesado: {file_path.name} ({len(chunks)} chunks)")
                except Exception as e:
                    logger.error(f"Error reprocesando {file_path}: {e}")
        
        st.success(f"âœ… {success_count}/{len(file_paths)} documentos reprocesados")
        st.rerun()

def test_rag_search():
    """Probar bÃºsqueda RAG"""
    st.subheader("ğŸ” Probar BÃºsqueda RAG")
    
    # ConfiguraciÃ³n de bÃºsqueda
    col1, col2 = st.columns(2)
    
    with col1:
        # Obtener modelos disponibles o usar predefinidos
        try:
            available_models = get_available_or_default_models(ollama_client)
            
            if not available_models:
                st.warning("No hay modelos disponibles")
                return
                
            model = st.selectbox(
                "Modelo para respuesta:",
                available_models,
                index=0 if available_models else None
            )
        except Exception as e:
            st.error(f"Error al obtener modelos: {e}")
            return
    
    with col2:
        similarity_threshold = st.slider(
            "Umbral de similitud:",
            min_value=0.0,
            max_value=1.0,
            value=config.SIMILARITY_THRESHOLD,
            step=0.05,
            help="MÃ­nima similitud requerida para considerar un chunk relevante"
        )
    
    with col2:
        max_results = st.slider(
            "MÃ¡ximo de resultados:",
            min_value=1,
            max_value=10,
            value=config.MAX_RETRIEVAL_DOCS,
            help="NÃºmero mÃ¡ximo de chunks a retornar"
        )
    
    query = st.text_input(
        "Consulta de prueba:",
        placeholder="Escribe una pregunta sobre tus documentos..."
    )
    
    if query and st.button("ğŸ” Buscar", key="search_documents"):
        with st.spinner("Buscando en documentos..."):
            try:
                # Obtener todos los chunks procesados usando funciÃ³n estandarizada
                all_chunks = []
                uploaded_files = get_valid_uploaded_files()
                
                for file_path in uploaded_files:
                        chunks = rag_processor.process_document(file_path)
                        chunks_with_embeddings = rag_processor.generate_embeddings(chunks)
                        all_chunks.extend(chunks_with_embeddings)
                
                if not all_chunks:
                    st.warning("âš ï¸ No hay documentos procesados para buscar")
                    return
                
                # Realizar bÃºsqueda con parÃ¡metros personalizados
                relevant_chunks = rag_processor.similarity_search(
                    query, all_chunks, top_k=max_results, threshold=similarity_threshold
                )
                
                if relevant_chunks:
                    st.success(f"âœ… Se encontraron {len(relevant_chunks)} chunks relevantes:")
                    
                    # Mostrar resultados detallados
                    for i, (chunk, similarity) in enumerate(relevant_chunks, 1):
                        with st.expander(f"ğŸ“„ Resultado {i} - {chunk.source_file} (Similitud: {similarity:.3f})"):
                            st.write(f"**Archivo:** {chunk.source_file}")
                            st.write(f"**Chunk:** {chunk.chunk_index + 1} de {chunk.metadata.get('total_chunks', 'N/A')}")
                            st.write(f"**Similitud:** {similarity:.3f}")
                            st.text_area("Contenido:", chunk.content, height=150, key=f"chunk_{i}")
                    
                    # Mostrar contexto completo
                    st.markdown("---")
                    st.subheader("ğŸ“‹ Contexto Completo")
                    context_parts = []
                    for chunk, similarity in relevant_chunks:
                        context_parts.append(f"[Fuente: {chunk.source_file}] {chunk.content}")
                    
                    full_context = "\n\n".join(context_parts)
                    st.text_area("Contexto que se enviarÃ­a a la IA:", full_context, height=300)
                    
                else:
                    st.warning(f"âš ï¸ No se encontraron chunks con similitud >= {similarity_threshold}")
                    st.info("ğŸ’¡ Intenta reducir el umbral de similitud o usar tÃ©rminos diferentes")
                
            except Exception as e:
                st.error(f"âŒ Error en la bÃºsqueda: {e}")
                logger.error(f"Error en test_rag_search: {e}")
    
    # InformaciÃ³n adicional
    if st.checkbox("â„¹ï¸ Mostrar informaciÃ³n del sistema"):
        st.markdown("---")
        st.subheader("ğŸ”§ InformaciÃ³n del Sistema RAG")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**ConfiguraciÃ³n actual:**")
            st.write(f"â€¢ Modelo de embeddings: `{config.DEFAULT_EMBEDDING_MODEL}`")
            st.write(f"â€¢ TamaÃ±o de chunk: `{config.CHUNK_SIZE}` caracteres")
            st.write(f"â€¢ Overlap: `{config.CHUNK_OVERLAP}` caracteres")
        
        with col2:
            st.write("**EstadÃ­sticas:**")
            stats = rag_processor.get_document_stats()
            st.write(f"â€¢ Documentos: `{stats.get('total_documents', 0)}`")
            st.write(f"â€¢ Chunks: `{stats.get('total_chunks', 0)}`")
            st.write(f"â€¢ Embeddings: `{stats.get('total_embeddings', 0)}`")

def clear_rag_cache():
    """Limpiar completamente el cache RAG"""
    st.subheader("ğŸ—‘ï¸ Limpiar Cache RAG")
    
    st.warning("âš ï¸ Esta acciÃ³n eliminarÃ¡ todos los chunks y embeddings procesados.")
    st.info("ğŸ’¡ Los archivos originales se mantendrÃ¡n, pero deberÃ¡s reprocesarlos.")
    
    if st.button("ğŸ—‘ï¸ Limpiar Cache Completo", type="secondary"):
        try:
            # Limpiar cache en memoria
            rag_processor.cache = {
                "chunks": [],
                "embeddings": [],
                "metadata": {
                    "last_updated": datetime.now().isoformat(),
                    "total_documents": 0,
                    "total_chunks": 0,
                    "total_embeddings": 0
                }
            }
            
            # Guardar cache vacÃ­o
            rag_processor.save_cache()
            
            st.success("âœ… Cache RAG limpiado completamente")
            st.info("ğŸ’¡ Puedes reprocesar tus documentos usando el botÃ³n 'Reprocesar Todos'")
            
            # Actualizar estadÃ­sticas
            st.rerun()
            
        except Exception as e:
            st.error(f"âŒ Error al limpiar cache: {e}")
            logger.error(f"Error en clear_rag_cache: {e}")

def export_rag_data():
    """Exportar datos RAG para respaldo"""
    st.subheader("ğŸ“¤ Exportar Datos RAG")
    
    try:
        stats = rag_processor.get_document_stats()
        
        if stats.get('total_chunks', 0) == 0:
            st.warning("âš ï¸ No hay datos RAG para exportar")
            return
        
        # Preparar datos para exportaciÃ³n
        export_data = {
            "metadata": {
                "export_date": datetime.now().isoformat(),
                "total_documents": stats.get('total_documents', 0),
                "total_chunks": stats.get('total_chunks', 0),
                "total_embeddings": stats.get('total_embeddings', 0),
                "config": {
                    "chunk_size": config.CHUNK_SIZE,
                    "chunk_overlap": config.CHUNK_OVERLAP,
                    "embedding_model": config.DEFAULT_EMBEDDING_MODEL
                }
            },
            "chunks": []
        }
        
        # Agregar chunks (sin embeddings para reducir tamaÃ±o)
        for chunk in rag_processor.cache.get("chunks", []):
            chunk_data = {
                "content": chunk.content,
                "source_file": chunk.source_file,
                "chunk_index": chunk.chunk_index,
                "metadata": chunk.metadata
            }
            export_data["chunks"].append(chunk_data)
        
        # Convertir a JSON
        json_data = json.dumps(export_data, indent=2, ensure_ascii=False)
        
        # BotÃ³n de descarga
        st.download_button(
            label="ğŸ“¥ Descargar Respaldo RAG",
            data=json_data,
            file_name=f"rag_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
            mime="application/json"
        )
        
        st.success(f"âœ… Respaldo preparado ({len(json_data)} caracteres)")
        
    except Exception as e:
        st.error(f"âŒ Error al exportar datos: {e}")
        logger.error(f"Error en export_rag_data: {e}")

def advanced_rag_settings():
    """Configuraciones avanzadas del sistema RAG"""
    st.subheader("âš™ï¸ Configuraciones Avanzadas")
    
    with st.expander("ğŸ”§ ParÃ¡metros de Procesamiento"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**ConfiguraciÃ³n actual:**")
            st.code(f"""
CHUNK_SIZE = {config.CHUNK_SIZE}
CHUNK_OVERLAP = {config.CHUNK_OVERLAP}
SIMILARITY_THRESHOLD = {config.SIMILARITY_THRESHOLD}
MAX_RETRIEVAL_DOCS = {config.MAX_RETRIEVAL_DOCS}
            """)
        
        with col2:
            st.write("**Modelos disponibles:**")
            try:
                if ollama_client.is_available():
                    models = ollama_client.get_available_models()
                    if models:
                        for model in models:
                            st.write(f"â€¢ {model['name']}")
                    else:
                        st.warning("No se encontraron modelos instalados")
                        st.write("**Modelos recomendados para instalar:**")
                        default_models = get_default_models()
                        for model in default_models["chat"][:3]:
                            st.write(f"â€¢ {model} (Chat)")
                        for model in default_models["embeddings"][:2]:
                            st.write(f"â€¢ {model} (Embeddings)")
                else:
                    st.error("Ollama no estÃ¡ disponible")
                    st.write("**Modelos predefinidos recomendados:**")
                    default_models = get_default_models()
                    for model in default_models["chat"][:3]:
                        st.write(f"â€¢ {model} (Chat)")
                    for model in default_models["embeddings"][:2]:
                        st.write(f"â€¢ {model} (Embeddings)")
            except Exception as e:
                st.error(f"Error al obtener modelos: {e}")
                st.write("**Modelos predefinidos recomendados:**")
                default_models = get_default_models()
                for model in default_models["chat"][:3]:
                    st.write(f"â€¢ {model} (Chat)")
                for model in default_models["embeddings"][:2]:
                    st.write(f"â€¢ {model} (Embeddings)")
    
    with st.expander("ğŸ“Š EstadÃ­sticas Detalladas"):
        stats = rag_processor.get_document_stats()
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("Documentos", stats.get('total_documents', 0))
            st.metric("Chunks", stats.get('total_chunks', 0))
        
        with col2:
            st.metric("Embeddings", stats.get('total_embeddings', 0))
            st.metric("Cache (MB)", f"{stats.get('cache_size_mb', 0):.2f}")
        
        with col3:
            if stats.get('total_chunks', 0) > 0:
                avg_chunk_size = stats.get('total_characters', 0) / stats.get('total_chunks', 1)
                st.metric("Promedio chars/chunk", f"{avg_chunk_size:.0f}")
            else:
                st.metric("Promedio chars/chunk", "0")
    
    with st.expander("ğŸ” DiagnÃ³stico del Sistema"):
        if st.button("ğŸ” Ejecutar DiagnÃ³stico", key="execute_diagnostic"):
            with st.spinner("Ejecutando diagnÃ³stico..."):
                # Verificar conexiÃ³n Ollama
                try:
                    models = ollama_client.get_available_models()
                    st.success(f"âœ… Ollama conectado ({len(models)} modelos)")
                except Exception as e:
                    st.error(f"âŒ Error Ollama: {e}")
                
                # Verificar directorios
                for dir_name, dir_path in [
                    ("Uploads", config.UPLOADS_DIR),
                    ("Cache", config.CACHE_DIR),
                    ("Processed", config.PROCESSED_DIR)
                ]:
                    if dir_path.exists():
                        files_count = len(list(dir_path.glob("*")))
                        st.success(f"âœ… {dir_name}: {files_count} archivos")
                    else:
                        st.error(f"âŒ {dir_name}: Directorio no existe")
                
                # Verificar cache
                cache_file = config.CACHE_DIR / "rag_cache.json"
                if cache_file.exists():
                    cache_size = cache_file.stat().st_size / 1024 / 1024
                    st.success(f"âœ… Cache: {cache_size:.2f} MB")
                else:
                    st.warning("âš ï¸ Cache: Archivo no existe")

def check_ollama_connection():
    """Verificar conexiÃ³n con Ollama"""
    try:
        return ollama_client.is_available()
    except Exception as e:
        logger.error(f"Error al conectar con Ollama: {e}")
        return False

def main():
    """FunciÃ³n principal de la pÃ¡gina de procesamiento de documentos"""
    st.title("ğŸ“„ Procesamiento de Documentos RAG")
    
    # Verificar conexiÃ³n con Ollama
    if not check_ollama_connection():
        st.error("âŒ No se puede conectar con Ollama. Verifica que estÃ© ejecutÃ¡ndose.")
        st.info("ğŸ’¡ **Para solucionar este problema:**")
        st.code("ollama serve", language="bash")
        st.markdown("Ejecuta este comando en tu terminal y luego recarga la pÃ¡gina.")
        st.stop()
    
    # Crear pestaÃ±as para organizar funcionalidades
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“ GestiÃ³n de Documentos", 
        "ğŸ” BÃºsqueda RAG", 
        "âš™ï¸ ConfiguraciÃ³n", 
        "ğŸ“¤ Respaldo", 
        "ğŸ”§ Avanzado"
    ])
    
    with tab1:
        st.header("ğŸ“ GestiÃ³n de Documentos")
        
        # Subir documentos
        upload_documents()
        
        st.markdown("---")
        
        # Mostrar documentos existentes
        show_uploaded_documents()
        
        st.markdown("---")
        
        # Botones de acciÃ³n
        col1, col2, col3 = st.columns(3)
        
        with col1:
            if st.button("ğŸ”„ Reprocesar Todos", type="primary"):
                # Obtener lista de archivos usando funciÃ³n estandarizada
                uploaded_files = get_valid_uploaded_files()
                reprocess_all_documents(uploaded_files)
        
        with col2:
            if st.button("ğŸ“Š Actualizar EstadÃ­sticas", key="update_stats_processor"):
                st.rerun()
        
        with col3:
            if st.button("ğŸ’¾ Guardar Cache", key="save_cache_processor"):
                try:
                    rag_processor.save_cache()
                    st.success("âœ… Cache guardado")
                except Exception as e:
                    st.error(f"âŒ Error: {e}")
    
    with tab2:
        st.header("ğŸ” BÃºsqueda y Pruebas RAG")
        test_rag_search()
    
    with tab3:
        st.header("âš™ï¸ ConfiguraciÃ³n del Sistema")
        
        # Mostrar estadÃ­sticas generales
        stats = rag_processor.get_document_stats()
        
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            st.metric("ğŸ“„ Documentos", stats.get('total_documents', 0))
        
        with col2:
            st.metric("ğŸ“ Chunks", stats.get('total_chunks', 0))
        
        with col3:
            st.metric("ğŸ§  Embeddings", stats.get('total_embeddings', 0))
        
        with col4:
            st.metric("ğŸ’¾ Cache (MB)", f"{stats.get('cache_size_mb', 0):.2f}")
        
        st.markdown("---")
        
        # Limpiar cache
        clear_rag_cache()
    
    with tab4:
        st.header("ğŸ“¤ Respaldo y ExportaciÃ³n")
        export_rag_data()
    
    with tab5:
        st.header("ğŸ”§ ConfiguraciÃ³n Avanzada")
        advanced_rag_settings()

if __name__ == "__main__":
    main()