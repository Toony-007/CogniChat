"""
Script ejecutor de tests automatizados para CogniChat
Ejecuta todas las pruebas y genera reportes de cobertura
"""

import pytest
import sys
import os
from pathlib import Path
import subprocess
import json
from datetime import datetime

def run_all_tests():
    """Ejecuta todas las pruebas automatizadas"""
    
    # Directorio de tests
    tests_dir = Path(__file__).parent
    
    # Lista de archivos de test
    test_files = [
        "test_validators.py",
        "test_chatbot.py", 
        "test_document_processor.py",
        "test_qualitative_analysis.py",
        "test_document_upload.py",
        "test_alerts.py",
        "test_settings.py",
        "test_utils.py"
    ]
    
    print("ğŸš€ Iniciando ejecuciÃ³n de tests automatizados para CogniChat")
    print("=" * 60)
    
    results = {}
    total_tests = 0
    total_passed = 0
    total_failed = 0
    
    for test_file in test_files:
        test_path = tests_dir / test_file
        
        if not test_path.exists():
            print(f"âš ï¸  Archivo de test no encontrado: {test_file}")
            continue
        
        print(f"\nğŸ“‹ Ejecutando: {test_file}")
        print("-" * 40)
        
        try:
            # Ejecutar pytest para el archivo especÃ­fico
            result = pytest.main([
                str(test_path),
                "-v",
                "--tb=short",
                "--no-header"
            ])
            
            if result == 0:
                print(f"âœ… {test_file}: TODOS LOS TESTS PASARON")
                status = "PASSED"
            else:
                print(f"âŒ {test_file}: ALGUNOS TESTS FALLARON")
                status = "FAILED"
            
            results[test_file] = {
                "status": status,
                "exit_code": result
            }
            
        except Exception as e:
            print(f"ğŸ’¥ Error ejecutando {test_file}: {str(e)}")
            results[test_file] = {
                "status": "ERROR",
                "error": str(e)
            }
    
    # Generar reporte final
    print("\n" + "=" * 60)
    print("ğŸ“Š REPORTE FINAL DE TESTS")
    print("=" * 60)
    
    for test_file, result in results.items():
        status_icon = "âœ…" if result["status"] == "PASSED" else "âŒ" if result["status"] == "FAILED" else "ğŸ’¥"
        print(f"{status_icon} {test_file}: {result['status']}")
    
    # EstadÃ­sticas
    passed_count = sum(1 for r in results.values() if r["status"] == "PASSED")
    failed_count = sum(1 for r in results.values() if r["status"] == "FAILED")
    error_count = sum(1 for r in results.values() if r["status"] == "ERROR")
    
    print(f"\nğŸ“ˆ ESTADÃSTICAS:")
    print(f"   âœ… Tests exitosos: {passed_count}")
    print(f"   âŒ Tests fallidos: {failed_count}")
    print(f"   ğŸ’¥ Tests con error: {error_count}")
    print(f"   ğŸ“Š Total archivos: {len(results)}")
    
    # Calcular porcentaje de Ã©xito
    if len(results) > 0:
        success_rate = (passed_count / len(results)) * 100
        print(f"   ğŸ¯ Tasa de Ã©xito: {success_rate:.1f}%")
    
    return results

def run_specific_module_tests(module_name):
    """Ejecuta tests para un mÃ³dulo especÃ­fico"""
    
    test_file = f"test_{module_name}.py"
    test_path = Path(__file__).parent / test_file
    
    if not test_path.exists():
        print(f"âŒ Archivo de test no encontrado: {test_file}")
        return False
    
    print(f"ğŸ¯ Ejecutando tests para mÃ³dulo: {module_name}")
    print("-" * 40)
    
    try:
        result = pytest.main([
            str(test_path),
            "-v",
            "--tb=long"
        ])
        
        if result == 0:
            print(f"âœ… Todos los tests de {module_name} pasaron")
            return True
        else:
            print(f"âŒ Algunos tests de {module_name} fallaron")
            return False
            
    except Exception as e:
        print(f"ğŸ’¥ Error ejecutando tests de {module_name}: {str(e)}")
        return False

def run_coverage_report():
    """Genera reporte de cobertura de cÃ³digo"""
    
    print("ğŸ“Š Generando reporte de cobertura...")
    
    try:
        # Ejecutar pytest con coverage
        result = subprocess.run([
            sys.executable, "-m", "pytest",
            "--cov=modules",
            "--cov=utils", 
            "--cov-report=html",
            "--cov-report=term-missing",
            "tests/"
        ], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("âœ… Reporte de cobertura generado exitosamente")
            print("ğŸ“ Reporte HTML disponible en: htmlcov/index.html")
        else:
            print("âŒ Error generando reporte de cobertura")
            print(result.stderr)
            
    except Exception as e:
        print(f"ğŸ’¥ Error ejecutando coverage: {str(e)}")

def generate_test_report(results):
    """Genera reporte detallado en JSON"""
    
    report = {
        "timestamp": datetime.now().isoformat(),
        "summary": {
            "total_files": len(results),
            "passed": sum(1 for r in results.values() if r["status"] == "PASSED"),
            "failed": sum(1 for r in results.values() if r["status"] == "FAILED"),
            "errors": sum(1 for r in results.values() if r["status"] == "ERROR")
        },
        "details": results
    }
    
    # Guardar reporte
    report_path = Path(__file__).parent / "test_report.json"
    
    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ Reporte detallado guardado en: {report_path}")
        
    except Exception as e:
        print(f"âš ï¸  Error guardando reporte: {str(e)}")

def main():
    """FunciÃ³n principal del ejecutor de tests"""
    
    if len(sys.argv) > 1:
        # Ejecutar tests para mÃ³dulo especÃ­fico
        module_name = sys.argv[1]
        run_specific_module_tests(module_name)
    else:
        # Ejecutar todos los tests
        results = run_all_tests()
        
        # Generar reporte detallado
        generate_test_report(results)
        
        # Preguntar si generar reporte de cobertura
        print("\nğŸ¤” Â¿Desea generar reporte de cobertura? (y/n): ", end="")
        try:
            response = input().lower().strip()
            if response in ['y', 'yes', 's', 'si', 'sÃ­']:
                run_coverage_report()
        except KeyboardInterrupt:
            print("\nğŸ‘‹ EjecuciÃ³n cancelada por el usuario")

if __name__ == "__main__":
    main()